{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main project/home work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/'\n",
    "out = './out/'\n",
    "\n",
    "# Bold print for Jupyter Notebook\n",
    "b1 = '\\033[1m'\n",
    "b0 = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some matplotlib and seaborn parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axistitlesize = 20\n",
    "axisticksize = 17\n",
    "axislabelsize = 26\n",
    "axislegendsize = 23\n",
    "axistextsize = 20\n",
    "axiscbarfontsize = 15\n",
    "\n",
    "# Set axtick dimensions\n",
    "major_size = 6\n",
    "major_width = 1.2\n",
    "minor_size = 3\n",
    "minor_width = 1\n",
    "mpl.rcParams['xtick.major.size'] = major_size\n",
    "mpl.rcParams['xtick.major.width'] = major_width\n",
    "mpl.rcParams['xtick.minor.size'] = minor_size\n",
    "mpl.rcParams['xtick.minor.width'] = minor_width\n",
    "mpl.rcParams['ytick.major.size'] = major_size\n",
    "mpl.rcParams['ytick.major.width'] = major_width\n",
    "mpl.rcParams['ytick.minor.size'] = minor_size\n",
    "mpl.rcParams['ytick.minor.width'] = minor_width\n",
    "\n",
    "mpl.rcParams.update({'figure.autolayout': False})\n",
    "\n",
    "# Seaborn style settings\n",
    "sns.set_style({'axes.axisbelow': True,\n",
    "               'axes.edgecolor': '.8',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': True,\n",
    "               'axes.labelcolor': '.15',\n",
    "               'axes.spines.bottom': True,\n",
    "               'axes.spines.left': True,\n",
    "               'axes.spines.right': True,\n",
    "               'axes.spines.top': True,\n",
    "               'figure.facecolor': 'white',\n",
    "               'font.family': ['sans-serif'],\n",
    "               'font.sans-serif': ['Arial',\n",
    "                'DejaVu Sans',\n",
    "                'Liberation Sans',\n",
    "                'Bitstream Vera Sans',\n",
    "                'sans-serif'],\n",
    "               'grid.color': '.8',\n",
    "               'grid.linestyle': '--',\n",
    "               'image.cmap': 'rocket',\n",
    "               'lines.solid_capstyle': 'round',\n",
    "               'patch.edgecolor': 'w',\n",
    "               'patch.force_edgecolor': True,\n",
    "               'text.color': '.15',\n",
    "               'xtick.bottom': True,\n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'in',\n",
    "               'xtick.top': True,\n",
    "               'ytick.color': '.15',\n",
    "               'ytick.direction': 'in',\n",
    "               'ytick.left': True,\n",
    "               'ytick.right': True})\n",
    "\n",
    "# Colorpalettes, colormaps, etc.\n",
    "sns.set_palette(palette='rocket')\n",
    "rocket_cmap = sns.color_palette('rocket', as_cmap=True)\n",
    "\n",
    "target_colors = np.array([cm.magma(0.5), cm.magma(0.75), cm.magma(0.93)])\n",
    "feature_colors = np.array([rocket_cmap(0.17), cm.magma(0.45), cm.magma(0.60), cm.magma(0.75)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./a. Load and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ENCODING: utf-8 can't handle some continuation bytes, so I choose `ISO-8859-1` for this task\n",
    "# NA_VALUES: NaN values are stored as 'nan' and 'NaN' values in the dataset\n",
    "# TRUE/FALSE VALUES: There are binary entries marked with the 'ja'/'nein' German words\n",
    "df = pd.read_csv(data + 'cars.csv', encoding='ISO-8859-1',\n",
    "                 na_values=['nan', 'NaN'], true_values=['ja'], false_values=['nein'])\n",
    "# The columns called 'name', and 'postalCode' can be easily dropped,\n",
    "# since they're only ID-like columns and doesn't necessary contan any useful information\n",
    "df = df.drop(labels=['name', 'postalCode'], axis=1)\n",
    "# The columns `dateCrawled`, `dateCreated` and `lastSeen` are\n",
    "# automatically generated values for this specific dataset,\n",
    "# which can be dropped in normal circumstances.\n",
    "# However let's keep the `dateCrawled` and `lastSeen` features\n",
    "# to combine them into one continuous feature.\n",
    "df = df.drop(labels=['dateCreated'], axis=1)\n",
    "d_1 = np.array([datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in df['dateCrawled'].values])\n",
    "d_2 = np.array([datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in df['lastSeen'].values])\n",
    "d_d = d_2 - d_1\n",
    "df['soldMin'] = np.array([d.seconds/60 for d in d_d])\n",
    "df = df.drop(labels=['dateCrawled', 'lastSeen'], axis=1)\n",
    "\n",
    "# The column `nrOfPictures` contains exclusively the value `0`. It can be dropped.\n",
    "df = df.drop(labels=['nrOfPictures'], axis=1)\n",
    "# The columns 'seller' and 'offerType' are incomprehensibly biased and contains\n",
    "# almost entirely a single value. They can be dropped with an ease\n",
    "df = df.drop(labels=['seller', 'offerType'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unique(df, label):\n",
    "    return np.unique(df[label][~df.isna()[label]],\n",
    "                     return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the unique values by features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('price :\\n', return_unique(df, label='price'), '\\n')          # Continuous, but not boring, contains mock values\n",
    "#print('seller :\\n', return_unique(df, label='seller'), '\\n')        # Incomprehensibly biased, should be dropped\n",
    "#print('offerType :\\n', return_unique(df, label='offerType'), '\\n')  # Incomprehensibly biased, should be dropped\n",
    "print('abtest :\\n', return_unique(df, label='abtest'), '\\n')         # This is not an informative value, but still interesting\n",
    "print('vehicleType :\\n', return_unique(df, label='vehicleType'), '\\n')\n",
    "print('yearOfRegistration :\\n', return_unique(df, label='yearOfRegistration'), '\\n')\n",
    "print('gearbox :\\n', return_unique(df, label='gearbox'), '\\n')\n",
    "#print('powerPS :\\n', return_unique(df, label='powerPS'), '\\n')      # Continuous, also boring\n",
    "print('model :\\n', return_unique(df, label='model'), '\\n')\n",
    "print('kilometer :\\n', return_unique(df, label='kilometer'), '\\n')\n",
    "print('monthOfRegistration :\\n', return_unique(df, label='monthOfRegistration'), '\\n')\n",
    "print('fuelType :\\n', return_unique(df, label='fuelType'), '\\n')\n",
    "print('brand :\\n', return_unique(df, label='brand'), '\\n')\n",
    "print('notRepairedDamage :\\n', return_unique(df, label='notRepairedDamage'), '\\n')\n",
    "#print('nrOfPictures :\\n', return_unique(df, label='nrOfPictures'), '\\n') # Contains only 0 values, can be dropped\n",
    "#print('soldMin :\\n', return_unique(df, label='soldMin'), '\\n')      # Continuous, also boring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical and continuous column names\n",
    "columns = np.array(df.columns)\n",
    "columns_cat = np.array(['abtest', 'yearOfRegistration', 'gearbox', 'model', 'kilometer', 'monthOfRegistration','fuelType', 'brand', 'notRepairedDamage'])\n",
    "columns_con = np.array(['price', 'powerPS', 'soldMin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./b. Explore and handle NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to analyze missing entries easier\n",
    "nan_mask = df.isna()\n",
    "nan_count = nan_mask.sum()\n",
    "print('Count of missing values:\\n' +\n",
    "      '========================')\n",
    "print(tabulate([[c, nan_count[c]] for c in nan_count.index], headers=['Feature', 'Count of NaNs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing all values on a figure is sadly can't be conveniently done, because the `cars.csv` file contains too much rows compared to its number of columns. This table should replace that classic figure here.\n",
    "\n",
    "Since all those columns that miss values are categorical features, we have to either drop them totally, or fill them with appropriately sampled values to keep the NaN-less distribution of features unchanged. Let's examine first, how many rows should we drop if we drop all with at least one NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the old dataframe for reusability\n",
    "df_n = df.copy()\n",
    "df_n = df_n.dropna(axis=0, how='any')\n",
    "\n",
    "# Fraction of dropped rows\n",
    "print('Dropped {:.2f}% of all original rows.'.format((1 - len(df_n) / len(df)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, I'll drop them..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./c. Inspect distribution of continous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = {}\n",
    "bins = {}\n",
    "width = {}\n",
    "center = {}\n",
    "\n",
    "for c in columns_con:\n",
    "    hist[c], bins[c] = np.histogram(df_n[c], bins=10, density=True)\n",
    "    width[c] = 0.8 * (bins[c][1] - bins[c][0])\n",
    "    center[c] = (bins[c][:-1] + bins[c][1:]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = 1\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*8, nrows*6),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "# Loop through axes/features\n",
    "for col_i, ax in enumerate(axes.reshape(-1)):\n",
    "    ax.set_yscale('log')\n",
    "    col = columns_con[col_i]\n",
    "\n",
    "    ax.bar(center[col], hist[col] / hist[col].sum(), width=width[col],\n",
    "           color=cm.magma(0.93), alpha=0.7,\n",
    "           ec='black', lw=0.5, align='center')\n",
    "    ax.set_title(col, fontsize=axistitlesize-5, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize,\n",
    "                   labelrotation=42, colors='white')\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=axisticksize-4,\n",
    "                   labelrotation=42, colors='white')\n",
    "\n",
    "#plt.suptitle('Fig. 2. Distribution of non-binary labels in the original dataset.',\n",
    "#             color='white', fontsize=axistitlesize+5, y=-0.04)\n",
    "\n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)\n",
    "plt.savefig(out + 'fig_1_dist_pre.png',\n",
    "            format='png', dpi=200,\n",
    "            facecolor='black', edgecolor='black',\n",
    "            bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_n.head())\n",
    "display(df_n.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./d. Handle placeholder/faulty/mock values\n",
    "\n",
    "Some columns contains obviously fake values, inputted for whatever reason into the dataset. These values should be filtered out using some meaningful/sane measure. Here I'll list all rules I formulated and used to filter fake values in the dataset.\n",
    "\n",
    "#### 1. `price`\n",
    "On Fig 2. we can see, that mock values here are really huge, probably values like `999999999` or similar placeholder values widely used in digital enviroments. They're are separates from real values and  on the far right side We can easly filter these values by deleting them over some threshold. Let us choose this threshold to be over $3$ million.\n",
    "\n",
    "#### 2. `yearOfRegistration`\n",
    "Fake values here are all over the spectrum. Feature values are ranging from `1000` to `9999`. Values before `1900` and after `2020` can be obviously dropped.\n",
    "\n",
    "#### 3. `powerPS`\n",
    "Representing the engine's power output in horsepower. A Bugatti Veyron's engine is approximately capped at $1200$ HP, while a main battle tank's engine can be output up to $2000$ HP. All values above eg. $1500$ can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the old dataframe for reusability\n",
    "df_f = df_n.copy()\n",
    "\n",
    "# Drop rows with mock `price` values\n",
    "df_f = df_f[df_f['price'] < 3_000_000]\n",
    "\n",
    "# Drop rows with mock `yearOfRegistration` values\n",
    "df_f = df_f[df_f['yearOfRegistration'] >= 1900][df_f['yearOfRegistration'] <= 2020]\n",
    "\n",
    "# Drop rows with mock `powerPS` values\n",
    "df_f = df_f[df_f['powerPS'] <= 1500]\n",
    "\n",
    "# Convert binary labels to 0,1 digits\n",
    "# Collect the datalabels\n",
    "df_f['abtest'] = df_f['abtest'].map({'control' : 0, 'test' : 1})\n",
    "df_f['gearbox'] = df_f['gearbox'].map({'automatik' : 0, 'manuell' : 1})\n",
    "df_f['notRepairedDamage'] = df_f['notRepairedDamage'].map({False : 0, True : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = {}\n",
    "bins = {}\n",
    "width = {}\n",
    "center = {}\n",
    "\n",
    "for c in columns_con:\n",
    "    hist[c], bins[c] = np.histogram(df_f[c], bins=10, density=True)\n",
    "    width[c] = 0.8 * (bins[c][1] - bins[c][0])\n",
    "    center[c] = (bins[c][:-1] + bins[c][1:]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = 1\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*8, nrows*6),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "# Loop through axes/features\n",
    "for col_i, ax in enumerate(axes.reshape(-1)):\n",
    "    ax.set_yscale('log')\n",
    "    col = columns_con[col_i]\n",
    "\n",
    "    ax.bar(center[col], hist[col] / hist[col].sum(), width=width[col],\n",
    "           color=cm.magma(0.45), alpha=0.7,\n",
    "           ec='black', lw=0.5, align='center')\n",
    "    ax.set_title(col, fontsize=axistitlesize-5, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize,\n",
    "                   labelrotation=42, colors='white')\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=axisticksize-4,\n",
    "                   labelrotation=42, colors='white')\n",
    "\n",
    "#plt.suptitle('Fig. 3. Distribution of continous labels in the modified dataset after filtering\\n' +\n",
    "#             'out all placeholder/faulty/mock feature values. Note, that only `price` and `powerPS`\\n' +\n",
    "#             'were altered during the filtering.',\n",
    "#             color='white', fontsize=axistitlesize+5, y=-0.04)\n",
    "\n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)\n",
    "plt.savefig(out + 'fig_2_dist_after.png',\n",
    "            format='png', dpi=200,\n",
    "            facecolor='black', edgecolor='black',\n",
    "            bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check unique values in the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abtest :\\n', return_unique(df_f, label='abtest'), '\\n')\n",
    "print('vehicleType :\\n', return_unique(df_f, label='vehicleType'), '\\n')\n",
    "print('yearOfRegistration :\\n', return_unique(df_f, label='yearOfRegistration'), '\\n')\n",
    "print('gearbox :\\n', return_unique(df_f, label='gearbox'), '\\n')\n",
    "print('model :\\n', return_unique(df_f, label='model'), '\\n')\n",
    "print('kilometer :\\n', return_unique(df_f, label='kilometer'), '\\n')\n",
    "print('monthOfRegistration :\\n', return_unique(df_f, label='monthOfRegistration'), '\\n')\n",
    "print('fuelType :\\n', return_unique(df_f, label='fuelType'), '\\n')\n",
    "print('brand :\\n', return_unique(df_f, label='brand'), '\\n')\n",
    "print('notRepairedDamage :\\n', return_unique(df_f, label='notRepairedDamage'), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./e. Explore the feature space of continuous(?) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(10*ncols, 10*nrows),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(wspace=0.25)\n",
    "sr = 2\n",
    "\n",
    "space = list(itertools.combinations(columns_con, 2))\n",
    "for idx, ax in enumerate(axes.reshape(-1)):\n",
    "    if idx < 2 :\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlim(1e-01, 1e06)\n",
    "    \n",
    "    i, j = space[idx]                         # Select features for axis\n",
    "    k = set(columns_con) - set(space[idx])    # Select color for scatterpoints\n",
    "    y = df_f[k].values.reshape(-1)\n",
    "    c = cm.magma((y/y.max())/2 + 0.5)\n",
    "    \n",
    "    ax.scatter(df_f[i], df_f[j],\n",
    "               c=c, alpha=0.6, s=sr**2)\n",
    "    ax.set_xlabel(i, fontsize=axislabelsize, fontweight='bold',\n",
    "                  color='white')\n",
    "    ax.set_ylabel(j, fontsize=axislabelsize, fontweight='bold',\n",
    "                  color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize,\n",
    "                   colors='white')\n",
    "\n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)\n",
    "plt.savefig(out + 'fig_3_feat_space_pre.png',\n",
    "            format='png', dpi=200,\n",
    "            facecolor='black', edgecolor='black',\n",
    "            bbox_inches='tight')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above indicates us that a lot of entries still contain mock values, like zeros for `powerPS` and ones for `price`. These entries can be also obviously dropped from the dataset. Also the `soldMin` feature seems to be completely random, or at least independent of `price` and `powerPS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop mock values from `price` and `powerPS`\n",
    "df_f = df_f[(df_f['price'] != 1)]\n",
    "df_f = df_f[(df_f['powerPS'] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(10*ncols, 10*nrows),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(wspace=0.25)\n",
    "sr = 2\n",
    "\n",
    "space = list(itertools.combinations(columns_con, 2))\n",
    "for idx, ax in enumerate(axes.reshape(-1)):\n",
    "    if idx < 2 :\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlim(1e-01, 1e06)\n",
    "    \n",
    "    i, j = space[idx]                         # Select features for axis\n",
    "    k = set(columns_con) - set(space[idx])    # Select color for scatterpoints\n",
    "    y = df_f[k].values.reshape(-1)\n",
    "    c = cm.magma((y/y.max())/2 + 0.5)\n",
    "    \n",
    "    ax.scatter(df_f[i], df_f[j],\n",
    "               c=c, alpha=0.6, s=sr**2)\n",
    "    ax.set_xlabel(i, fontsize=axislabelsize, fontweight='bold',\n",
    "                  color='white')\n",
    "    ax.set_ylabel(j, fontsize=axislabelsize, fontweight='bold',\n",
    "                  color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize,\n",
    "                   colors='white')\n",
    "\n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)\n",
    "plt.savefig(out + 'fig_4_feat_space_after.png',\n",
    "            format='png', dpi=200,\n",
    "            facecolor='black', edgecolor='black',\n",
    "            bbox_inches='tight')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Show something interesting...\n",
    "\n",
    "There are a number of things can be looked into using this dataset. Here I'll brainstorm my ideas and try to explore as many things as possible.\n",
    "\n",
    "#### 2./idea_1. Binary labels from continuous features\n",
    "Can I predict the binary labels (`abtest`, `gearbox`, `notRepairedDamage`) from the continous features (`price`, `powerPS` and `soldMin`)? Basicly I have no idea what type of connection is there between them, so I'll try various linear models in this case.\n",
    "\n",
    "#### 2./idea_2. Doing something with `brand`\n",
    "Is the prediction of `brand` possible somehow?? There are so few features... Is it possible to predict `brand` using again the continous features? Maybe again some linear model?\n",
    "\n",
    "#### 2./idea_3. Should I implement some Bayesian model with `pyro.ai` or `TF Probability`?\n",
    "Maybe next time. But probably would be VERY useful here working with a bunch of categorical/categorizable features. Creating a simple Bayesian graph would be probably the best here to explore the CPT between these features and predict eg. the binned `price` feature given all other conditions. The hard part itself lies in the creation of the graph and I have zero idea which nodes should I connect, not even speaking about the direction of edges.\n",
    "\n",
    "#### 2./idea_4. Predicting `price`\n",
    "Speaking about `price` prediction I should definitely try to build a model for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data exploration visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top brands\n",
    "brand_unique, brand_count = np.unique(df_f['brand'], return_counts=True)\n",
    "brand_count, brand_unique = zip(*sorted(zip(brand_count, brand_unique)))\n",
    "\n",
    "top_n = 9\n",
    "top_brands = brand_unique[::-1][:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(8*ncols, 8*nrows),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(wspace=0.25, hspace=0.3)\n",
    "sr = 2\n",
    "\n",
    "for i, ax in enumerate(axes.reshape(-1)):\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim(1e-01, 1e06)\n",
    "    ax.set_ylim(-20, 1400)\n",
    "    \n",
    "    t = top_brands[i]\n",
    "    x = df_f[df_f['brand'] == t]['price']\n",
    "    y = df_f[df_f['brand'] == t]['powerPS']\n",
    "\n",
    "    #y = df_f[k].values.reshape(-1)\n",
    "    #c = cm.magma((y/y.max())/2 + 0.5)\n",
    "    \n",
    "    ax.set_title('Car brand : `{}`'.format(t), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "    ax.scatter(x, y,\n",
    "               color=cm.magma(0.93), alpha=0.4, s=sr**2)\n",
    "    ax.set_xlabel('price', fontsize=axislabelsize, fontweight='bold',\n",
    "                  color='white')\n",
    "    ax.set_ylabel('powerPS', fontsize=axislabelsize, fontweight='bold',\n",
    "                  color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize,\n",
    "                   colors='white', labelrotation=42)\n",
    "\n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)\n",
    "plt.savefig(out + 'fig_5_car_brands.png',\n",
    "            format='png', dpi=200,\n",
    "            facecolor='black', edgecolor='black',\n",
    "            bbox_inches='tight')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test idea_1: Binary labels from continous features\n",
    "\n",
    "Scaling the data is necessary here beacuse of the high-magnitude feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X):\n",
    "    \"\"\"\n",
    "    Normalize the data to have zero mean and unit variance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray or array-like in shape of (N, M)\n",
    "        The unscaled dataset.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray in shape of (N, M)\n",
    "        The already scaled dataset with zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    scaler = StandardScaler()\n",
    "    # Compute the mean and standard dev. and scale the dataset `X`\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data + label sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_f[['price', 'powerPS', 'soldMin']]\n",
    "X = scale_data(np.array(X))\n",
    "\n",
    "# Collect the datalabels\n",
    "y_1 = df_f['abtest']\n",
    "y_2 = df_f['gearbox']\n",
    "y_3 = df_f['notRepairedDamage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet, RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_linear(X, y, binarize=True, model=LinearRegression(), print_stat=True):\n",
    "    \n",
    "    # Split the data into train and test data\n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define and fit model\n",
    "    model_train = model.fit(X_train, y_train)\n",
    "    model_full = model.fit(X, y)\n",
    "    # Compute predictions\n",
    "    pred_train = model_train.predict(X_train)\n",
    "    pred_test = model_train.predict(X_test)\n",
    "    pred_full = model_full.predict(X)\n",
    "    \n",
    "    if binarize:\n",
    "        # Binarize predictions\n",
    "        pred_train[pred_train < model_train.intercept_] = 0\n",
    "        pred_train[pred_train > model_train.intercept_] = 1\n",
    "        pred_test[pred_test < model_train.intercept_] = 0\n",
    "        pred_test[pred_test > model_train.intercept_] = 1\n",
    "        pred_full[pred_full < model_full.intercept_] = 0\n",
    "        pred_full[pred_full > model_full.intercept_] = 1\n",
    "    \n",
    "    if print_stat:\n",
    "        print(b1 + 'TRAIN DATASET FIT' + b0 + '\\n' +\n",
    "                   '-----------------')\n",
    "        #print(b1 + 'Coefficient of determination (TRAIN):' + b0, r_sq_train)\n",
    "        print(b1 + 'Intercept (TRAIN):' + b0, model_train.intercept_)\n",
    "        print(b1 + 'Slope coefficients (TRAIN):' + b0)\n",
    "        for col, mc in zip(columns_con, model_train.coef_):\n",
    "            print('    {0} : {1}'.format(col, mc))\n",
    "\n",
    "        print()\n",
    "        print(b1 + 'FULL DATASET FIT' + b0 + '\\n' +\n",
    "                   '----------------')\n",
    "        #print(b1 + 'Coefficient of determination (FULL):' + b0, r_sq_full)\n",
    "        print(b1 + 'Intercept (FULL):' + b0, model_full.intercept_)\n",
    "        print(b1 + 'Slope coefficients (FULL):' + b0)\n",
    "        for col, mc in zip(columns_con, model_full.coef_):\n",
    "            print('    {0} : {1}'.format(col, mc))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test,\\\n",
    "           pred_train, pred_test, pred_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1./a. Linear regression on `abtest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_1, model=LinearRegression(), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 7. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the LinearRegression to predict `abtest` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_6_linear_abtest',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1./b. Linear regression on `gearbox`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_2, model=LinearRegression(), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 8. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the LinearRegression to predict `gearbox` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_7_linear_gearbox',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1./c. Linear regression on `notRepairedDamage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_3, model=LinearRegression(), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 9. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the LinearRegression to predict\\n' +\n",
    "#       '`notRepairedDamage` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_8_linear_notrepaired',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2./a. Logistic regression on `abtest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_1, binarize=False, model=LogisticRegression(), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 10. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the LogisticRegression to predict `abtest` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_9_logistic_abtest',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2./b. Logistic regression on `gearbox`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_2, binarize=False, model=LogisticRegression(), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 11. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the LogisticRegression to predict `gearbox` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_10_logistic_gearbox',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2./c. Logistic regression on `notRepairedDamage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_3, binarize=False, model=LogisticRegression(), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 12. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the LogisticRegression to predict\\n' +\n",
    "#       '`notRepairedDamage` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_11_logistic_notrepaired',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3./a. Ridge classifier on `abtest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_1, binarize=False, model=RidgeClassifier(), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 13. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the RidgeClass. to predict `abtest` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_12_ridge_abtest',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3./b. Ridge classifier on `gearbox`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_2, binarize=False, model=RidgeClassifier(), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 14. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the RidgeClass. to predict `gearbox` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_13_ridge_gearbox',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3./c. Ridge classifier on  `notRepairedDamage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_3, binarize=False, model=RidgeClassifier(), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 15. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the RidgeClass. to predict\\n' +\n",
    "#       '`notRepairedDamage` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_14_ridge_notrepaired',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3./a. ElasticNet on `abtest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_1, binarize=True, model=ElasticNet(alpha=0.0001, max_iter=1e07), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 15. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the ElasticNet to predict `abtest` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_15_elastic_abtest',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3./b. ElasticNet on `gearbox`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_2, binarize=True, model=ElasticNet(alpha=0.00001, max_iter=1e07), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 16. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the ElasticNet to predict `gearbox` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_16_elastic_gearbox',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3./c. ElasticNet on  `notRepairedDamage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y_3, binarize=True, model=ElasticNet(alpha=0.00001, max_iter=1e07), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=pred_test)\n",
    "conf_mat = confusion_matrix(y_test, pred_test, labels=[i for i in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title=('Fig. 17. Confusion matrix of the predictions on the\\n' +\n",
    "#       'train set of the ElasticNet to predict\\n' +\n",
    "#       '`notRepairedDamage` values.\\n' +\n",
    "#       'Accuracy of model is {0:.3f}%'.format(accuracy))\n",
    "plot_confusion_matrix(conf_mat, y_test, labels=[i for i in range(0,2)],\n",
    "                      title='Accuracy of model is {0:.3f}%'.format(accuracy),\n",
    "                      save=True, save_filename='fig_17_elastic_notrepaired',\n",
    "                      figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test idea_4: Predict price of cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(X, m, b):\n",
    "    \n",
    "    Y = m * X + b\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using other continous variables to predict price\n",
    "X = df_f[['powerPS', 'soldMin']]\n",
    "X = scale_data(np.array(X))\n",
    "# Add gearbox values\n",
    "g = df_f['gearbox'].values\n",
    "X = np.c_[X,g]\n",
    "\n",
    "# Collect the datalabels\n",
    "y = df_f['price'] / 1e02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y, binarize=False, model=ElasticNet(), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*10, nrows*10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "sc = 6\n",
    "\n",
    "# Pred and groundtruth y values\n",
    "y_pred = [pred_train, pred_test, pred_full]\n",
    "y_grnd = [y_train, y_test, y]\n",
    "# Titles for subplots\n",
    "titles = ['Train dataset', 'Test dataset', 'Full dataset']\n",
    "# X values for the m=1, b=0 line\n",
    "#x_line = np.linspace(0,1,80)\n",
    "for i in range(ncols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    #ax.set_xlim(0,1)\n",
    "    #ax.set_ylim(0,1)\n",
    "    \n",
    "    ax.scatter(y_grnd[i], y_pred[i],\n",
    "               color=cm.magma(0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "    #ax.plot(x_line, line(x_line, m=1, b=0),\n",
    "    #        color=cm.magma(0.93), lw=3, ls='--', alpha=0.7)\n",
    "    ax.set_title(titles[i] + '\\nAccuracy : {0:.3f} %'.format(r2_score(y_grnd[i], y_pred[i]) * 100),\n",
    "                 fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "    ax.set_xlabel('$y_{\\mathrm{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('$y_{\\mathrm{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "    \n",
    "#fig.suptitle('Fig. 16. Accuracy of predictions for different test samples. In the leftmost and middle figures the model was trained\\n' +\n",
    "#             'only on half of the dataset then the train and test datasets were used respectively for prediction. In the\\n' +\n",
    "#             'rightmost figure the full dataset was used for both training and prediction.',\n",
    "#             color='white', fontsize=axistitlesize+5, y=0.00)\n",
    "\n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)\n",
    "plt.savefig(out + 'fig_15_ealstic_price.png',\n",
    "            format='png', dpi=200,\n",
    "            facecolor='black', edgecolor='black',\n",
    "            bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_linear(X, y, binarize=False, model=LinearRegression(), print_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*10, nrows*10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "sc = 6\n",
    "\n",
    "# Pred and groundtruth y values\n",
    "y_pred = [pred_train, pred_test, pred_full]\n",
    "y_grnd = [y_train, y_test, y]\n",
    "# Titles for subplots\n",
    "titles = ['Train dataset', 'Test dataset', 'Full dataset']\n",
    "# X values for the m=1, b=0 line\n",
    "#x_line = np.linspace(0,1,80)\n",
    "for i in range(ncols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    #ax.set_xlim(0,1)\n",
    "    #ax.set_ylim(0,1)\n",
    "    \n",
    "    ax.scatter(y_grnd[i], y_pred[i],\n",
    "               color=cm.magma(0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "    #ax.plot(x_line, line(x_line, m=1, b=0),\n",
    "    #        color=cm.magma(0.93), lw=3, ls='--', alpha=0.7)\n",
    "    ax.set_title(titles[i] + '\\nAccuracy : {0:.3f} %'.format(r2_score(y_grnd[i], y_pred[i]) * 100),\n",
    "                 fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "    ax.set_xlabel('$y_{\\mathrm{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('$y_{\\mathrm{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)\n",
    "plt.savefig(out + 'fig_16_linear_price.png',\n",
    "            format='png', dpi=200,\n",
    "            facecolor='black', edgecolor='black',\n",
    "            bbox_inches='tight')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.c_### Support Vector Machine -- regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_svm(X, y, model=SVR()):\n",
    "    \n",
    "    # Split the data into train and test data\n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define and fit model\n",
    "    model_train = model.fit(X_train, y_train)\n",
    "    model_full = model.fit(X, y)\n",
    "    # Compute predictions\n",
    "    pred_train = model_train.predict(X_train)\n",
    "    pred_test = model_train.predict(X_test)\n",
    "    pred_full = model_full.predict(X)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test,\\\n",
    "           pred_train, pred_test, pred_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,\\\n",
    "    pred_train, pred_test, pred_full = return_svm(X, y, model=SVR())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*10, nrows*10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "sc = 6\n",
    "\n",
    "# Pred and groundtruth y values\n",
    "y_pred = [pred_train, pred_test, pred_full]\n",
    "y_grnd = [y_train, y_test, y]\n",
    "# Titles for subplots\n",
    "titles = ['Train dataset', 'Test dataset', 'Full dataset']\n",
    "# X values for the m=1, b=0 line\n",
    "#x_line = np.linspace(0,1,80)\n",
    "for i in range(ncols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    #ax.set_xlim(0,1)\n",
    "    #ax.set_ylim(0,1)\n",
    "    \n",
    "    ax.scatter(y_grnd[i], y_pred[i],\n",
    "               color=cm.magma(0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "    #ax.plot(x_line, line(x_line, m=1, b=0),\n",
    "    #        color=cm.magma(0.93), lw=3, ls='--', alpha=0.7)\n",
    "    ax.set_title(titles[i] + '\\nAccuracy : {0:.3f} %'.format(r2_score(y_grnd[i], y_pred[i]) * 100),\n",
    "                 fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "    ax.set_xlabel('$y_{\\mathrm{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('$y_{\\mathrm{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "    \n",
    "#fig.suptitle('Fig. 16. Accuracy of predictions for different test samples using an SVM regression model. In the leftmost\\n' +\n",
    "#             'and middle figures the model was trained only on half of the dataset then the train and test datasets were\\n' +\n",
    "#             'used respectively for prediction. In the rightmost figure the full dataset was used for both training\\n' +\n",
    "#             'and prediction.',\n",
    "#             color='white', fontsize=axistitlesize+5, y=0.00)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
