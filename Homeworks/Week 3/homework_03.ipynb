{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Supervised learning introduction, K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/'\n",
    "out = './out/'\n",
    "\n",
    "# Bold print for Jupyter Notebook\n",
    "b1 = '\\033[1m'\n",
    "b0 = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some matplotlib and seaborn parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axistitlesize = 20\n",
    "axisticksize = 17\n",
    "axislabelsize = 26\n",
    "axislegendsize = 23\n",
    "axistextsize = 20\n",
    "axiscbarfontsize = 15\n",
    "\n",
    "# Set axtick dimensions\n",
    "major_size = 6\n",
    "major_width = 1.2\n",
    "minor_size = 3\n",
    "minor_width = 1\n",
    "mpl.rcParams['xtick.major.size'] = major_size\n",
    "mpl.rcParams['xtick.major.width'] = major_width\n",
    "mpl.rcParams['xtick.minor.size'] = minor_size\n",
    "mpl.rcParams['xtick.minor.width'] = minor_width\n",
    "mpl.rcParams['ytick.major.size'] = major_size\n",
    "mpl.rcParams['ytick.major.width'] = major_width\n",
    "mpl.rcParams['ytick.minor.size'] = minor_size\n",
    "mpl.rcParams['ytick.minor.width'] = minor_width\n",
    "\n",
    "mpl.rcParams.update({'figure.autolayout': False})\n",
    "\n",
    "# Seaborn style settings\n",
    "sns.set_style({'axes.axisbelow': True,\n",
    "               'axes.edgecolor': '.8',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': True,\n",
    "               'axes.labelcolor': '.15',\n",
    "               'axes.spines.bottom': True,\n",
    "               'axes.spines.left': True,\n",
    "               'axes.spines.right': True,\n",
    "               'axes.spines.top': True,\n",
    "               'figure.facecolor': 'white',\n",
    "               'font.family': ['sans-serif'],\n",
    "               'font.sans-serif': ['Arial',\n",
    "                'DejaVu Sans',\n",
    "                'Liberation Sans',\n",
    "                'Bitstream Vera Sans',\n",
    "                'sans-serif'],\n",
    "               'grid.color': '.8',\n",
    "               'grid.linestyle': '--',\n",
    "               'image.cmap': 'rocket',\n",
    "               'lines.solid_capstyle': 'round',\n",
    "               'patch.edgecolor': 'w',\n",
    "               'patch.force_edgecolor': True,\n",
    "               'text.color': '.15',\n",
    "               'xtick.bottom': True,\n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'in',\n",
    "               'xtick.top': True,\n",
    "               'ytick.color': '.15',\n",
    "               'ytick.direction': 'in',\n",
    "               'ytick.left': True,\n",
    "               'ytick.right': True})\n",
    "\n",
    "# Colorpalettes, colormaps, etc.\n",
    "sns.set_palette(palette='rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data\n",
    "\n",
    "The provided three files (glass.data, glass.tag, glass.names) contains a small dataset. \"[The study of classification of types of glass was motivated by criminological investigation. At the scene of the crime, the glass left can be used as evidence...if it is correctly identified!](https://archive.ics.uci.edu/ml/datasets/Glass+Identification)\"\n",
    "\n",
    " - read the content of the glass.data file as a pandas dataframe\n",
    " - use the column names found in the glass.names file\n",
    " - if needed, handle the missing values\n",
    " - get rid of the ID column\n",
    " - separate the GlassType from the dataset and store it in a one-hot encoded manner (if we have 3 classes, than instead of 1, 2, 3 we use [1, 0, 0], [0, 1, 0] and [0, 0, 1]. So each dimension represents a class and 1 means the sample belongs to that class, 0 is the opposite)\n",
    "    - be careful, the data contains only 1-2-3-5-6-7, 4 is missing! Convert 5 $\\to$ 4, 6 $\\to$ 5 and 7 $\\to$ 6 and then apply the one-got encoding\n",
    "    - also keep the converted labels, where y is 1-2-3-4-5-6, we will need them later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./a. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the data files in an IDE we see the following structure:\n",
    "\n",
    "![data_files](./img/data_files.png \"Content of datafiles\")\n",
    "\n",
    "1. The `glass.data` file contains exactly $\\mathbf{1}$ index column (indeces from $1$ to $214$) and $\\mathbf{10}$ data columns, with no NaN values whatsover.\n",
    "2. The `glass.names` file contains descriptive information about the content of the `glass.data` file. The meaning for each data columns could be found under the `7. Attribute Information` section, which is the following:\n",
    "    >```\n",
    "    7. Attribute Information:\n",
    "       1. Id number: 1 to 214\n",
    "       2. RI: refractive index\n",
    "       3. Na: Sodium (unit measurement: weight percent in corresponding oxide, as \n",
    "                      are attributes 4-10)\n",
    "       4. Mg: Magnesium\n",
    "       5. Al: Aluminum\n",
    "       6. Si: Silicon\n",
    "       7. K: Potassium\n",
    "       8. Ca: Calcium\n",
    "       9. Ba: Barium\n",
    "      10. Fe: Iron\n",
    "      11. Type of glass: (class attribute)\n",
    "          -- 1 building_windows_float_processed\n",
    "          -- 2 building_windows_non_float_processed\n",
    "          -- 3 vehicle_windows_float_processed\n",
    "          -- 4 vehicle_windows_non_float_processed (none in this database)\n",
    "          -- 5 containers\n",
    "          -- 6 tableware\n",
    "          -- 7 headlamps\n",
    "    ```\n",
    "3. The `glass.tag` file should not be used currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the 10 data column names from the `glass.names` file,\n",
    "# listed under the `7. Attribute Information` section\n",
    "features = ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data + 'glass.data',\n",
    "                 sep=',', names=features, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./b. One-hot encode column `Type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-label types\n",
    "\n",
    "As it was asked, I first revamp the numbering of the column `Type`, by making it consecutive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_numbering(X):\n",
    "    \"\"\"\n",
    "    Changes an input 1D array of non-negative integers in ascending order by changing the values of integers to make them consecutive.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like of shape (N, 1)\n",
    "        Array of `N` non-negative integers in ascending order.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_new : `numpy.array`\n",
    "        The new array with already replaced values.\n",
    "    \"\"\"\n",
    "    assert np.array(X).ndim == 1, \"Input array-like object should be 1 dimensional, while\\\n",
    "                                   input dim. is ({0})!\".format(np.array(X).ndim)\n",
    "    \n",
    "    # Collect numbers in old array\n",
    "    X_nums = set(X)\n",
    "    # Create a new set of consecutive numbers\n",
    "    X_new_nums = set(i+1 for i in range(len(X_nums)))\n",
    "    # Map the new values on the old ones\n",
    "    X_new = pd.Series(X).map({x : x_n for (x, x_n) in zip(X_nums, X_new_nums)}).values\n",
    "    \n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = relabel_numbering(X=df['Type'])\n",
    "# Relabel actual dataset `df`\n",
    "df['Type'] = X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encode relabeled column `Type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep old (already relabeled) column separately but rename the variable it's stored in\n",
    "type_orig = X_new\n",
    "# Create the one-hot encoding using the built-in function from `pandas`\n",
    "type_one_hot = pd.get_dummies(df['Type'], prefix='Type')\n",
    "display(type_one_hot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./c. Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(df, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plots the distribution of feature values in individual columns of a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : `pandas.DataFrame`\n",
    "        Input DataFrame to get feature values from.\n",
    "        \n",
    "    log_scale : bool\n",
    "        Sets whether the Y-axis is displayed on a log-scale or not.\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    ncols = 3\n",
    "    nrows = (len(cols)-1)//ncols + 1\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*10, nrows*6))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            ax = axes[i][j]\n",
    "            if i*ncols + j < len(cols):\n",
    "                feature = cols[i*ncols + j]\n",
    "                # `Type` is a discrete feature and needs to be treated specially\n",
    "                if feature == 'Type':\n",
    "                    XY = df[feature].value_counts(sort=False, normalize=False)\n",
    "                    X = np.array(XY.index)\n",
    "                    Y = np.array(XY.values)\n",
    "                    width = 1.0\n",
    "                # Else simply bin the continuous features and plot on a histogram\n",
    "                else:\n",
    "                    Y, bins = np.histogram(df[feature], bins=10, density=False)\n",
    "                    width = 1.0 * np.diff(bins).min()\n",
    "                    X = (bins[:-1] + bins[1:]) / 2\n",
    "            \n",
    "                ax.bar(X, Y, width=width,\n",
    "                       color='tab:blue', alpha=0.8,\n",
    "                       ec='black', lw=0.5, ls='--')\n",
    "                ax.set_title('Feature [\\'{0}\\']'.format(feature),\n",
    "                             fontsize=axistitlesize, fontweight='bold')\n",
    "                ax.set_xlabel('Feature values', fontsize=axislabelsize, fontweight='bold')\n",
    "                ax.set_ylabel('Counts', fontsize=axislabelsize, fontweight='bold')\n",
    "                ax.tick_params(axis='both', which='major', labelsize=axisticksize)\n",
    "                \n",
    "                if log_scale:\n",
    "                    ax.set_yscale('log')\n",
    "                \n",
    "            else:\n",
    "                ax.axis('off')\n",
    "                \n",
    "    plt.suptitle('Fig. 1. Distribtion of feature values in the Glass Identification Database. For all continuous features\\n' +\n",
    "                 'a bin count of 10 was considered, while simply the value count was plotted in case of the discrete `Type` feature.',\n",
    "                 fontsize=axistitlesize+6, y=0.08)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(df, log_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phase_space(df):\n",
    "    \"\"\"\n",
    "    Plots the phase space of feature values in individual columns of a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : `pandas.DataFrame`\n",
    "        Input DataFrame to get feature values from.\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    nrows = len(cols)\n",
    "    ncols = len(cols)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*8, nrows*8))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            ax = axes[i][j]\n",
    "            if i < j:\n",
    "                X = df[cols[i]]\n",
    "                Y = df[cols[j]]\n",
    "                ax.scatter(X, Y)\n",
    "\n",
    "                ax.set_xlabel('Feature : [\\'{0}\\']'.format(cols[i]),\n",
    "                              fontsize=axislabelsize, fontweight='bold')\n",
    "                ax.set_ylabel('Feature : [\\'{0}\\']'.format(cols[j]),\n",
    "                              fontsize=axislabelsize, fontweight='bold')\n",
    "                ax.tick_params(axis='both', which='major', labelsize=axisticksize)\n",
    "                \n",
    "            else:\n",
    "                ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Fig. 2. Phase space of all available features in the Glass Identification Database. It is interesting to note,\\n' +\n",
    "                 'that there is a clear correlation between the refractive index and calcium content of glasses.',\n",
    "                 fontsize=axistitlesize+46, y=0.17)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_phase_space(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. \\& 3. Implement KNN\n",
    "\n",
    "Implement the K-nearest neighbors regression algorithm using only pure Python3 and numpy! Use L2 distance to find the neighbors. The prediction for each class should be the number of neighbors supporing the given class divided by **k** (for example if **k** is 5 and we have 3 neighbors for class A, 2 for class B and 0 for class C neighbors, then the prediction for class A should be 3/5, for class B 2/5, for class C 0/5). \n",
    "\n",
    "Complete the function!\n",
    "\n",
    "```python\n",
    "def knn_classifier(k, X_train, y_train, X_test):\n",
    "    ...\n",
    "    return predictions\n",
    "```\n",
    " - **k** is the number of neighbors to be considered\n",
    " - **X_train** is the training data points\n",
    " - **X_test** is the test data points\n",
    " - **y_train** is the labels for the training data\n",
    " - assume that **y_test** is one-hot encoded.\n",
    "\n",
    "A valid-syntaxed input looks like:\n",
    "```python\n",
    "k = 2\n",
    "X_train = [[0.9, 0.2, 0.8] , [-1.2, 1.5, 0.7], [5.8, 0.0, 0.9], [6.2, 0.9, 0.9]]\n",
    "y_train = [[0, 1], [0, 1], [1, 0], [0, 1]]\n",
    "X_test  = [[0.8, 0.8, 0.6], [0.5, 0.4, 0.3]]\n",
    "```\n",
    "\n",
    "Here, it means that the training data consists of 4 points, each point is placed in a 3 dimensional space. And there are two possible classes for each point and there are two data points for that predictions is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is detailed on the [Tutorials points website](https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_knn_algorithm_finding_nearest_neighbors.htm), the K-nearest neighbors (KNN) algorithm uses \"feature similarity\" to predict the values of new datapoints which further means that the new data point will be assigned a value based on how closely it matches the points in the training set. We can understand its working with the help of following steps:\n",
    "\n",
    ">1. For implementing any algorithm, we need dataset. So during the first step of KNN, we must load the training as well as test data.\n",
    ">2. Next, we need to choose the value of K i.e. the nearest data points. K can be any integer.\n",
    ">3. For each point in the test data do the following:  \n",
    ">    3.1. Calculate the distance between test data and each row of training data with the help of any of the method namely: Euclidean, Manhattan or Hamming distance. The most commonly used method to calculate distance is Euclidean.  \n",
    ">    3.2. Now, based on the distance value, sort them in ascending order.  \n",
    ">    3.3. Next, it will choose the top K rows from the sorted array.  \n",
    ">    3.4. Now, it will assign a class to the test point based on most frequent class of these rows.\n",
    "\n",
    "There are also a lot of details about the KNN algorithm itself, as well as a full implementations from scratch in Python on [TowardsDataScience](https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761). The list above is a bit reformulated on this page as well, but I won't copy-paste that here. I'll use the list above to implement my KNN classifier. Also there are many useful helper functions on the [Machine Learning Mastery](https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/) website as well.\n",
    "\n",
    "I've written my KNN classifier to accept almost any form of input data, like labels in a one-hot encoded or in a regular 1D array manner. Both method perform completely the same, unlike `sklearn`'s KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln(a, b, n):\n",
    "    \"\"\"\n",
    "    Calculates the n-norm of two input vectors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    a, b : `numpy.array`\n",
    "        The two arrays between the n-norm should be measured\n",
    "    \"\"\"\n",
    "    return np.linalg.norm((a - b), ord=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X):\n",
    "    \"\"\"\n",
    "    Normalize the data to have zero mean and unit variance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray or array-like in shape of (N, M)\n",
    "        The unscaled dataset.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray in shape of (N, M)\n",
    "        The already scaled dataset with zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    scaler = StandardScaler()\n",
    "    # Compute the mean and standard dev. and scale the dataset `X`\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values(X):\n",
    "    \"\"\"\n",
    "    Return unique entries and number of their occurences in an N-dimensional array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray or array-like\n",
    "        Input array to find and count unique values in.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_unique : list of shape (U, N)\n",
    "        Unique entries in the input array. `U` is the number of unique entries in the\n",
    "        input array `X`.\n",
    "        \n",
    "    X_counts : list of shape (U)\n",
    "        The number of occurences of entries in the input array `X`. The indeces in this\n",
    "        list references the entries in the list `X_unique`. `U` is the number of unique\n",
    "        entries in the input array `X`.\n",
    "        \n",
    "    X_indeces : list\n",
    "        The unique elements's indeces of occurence.\n",
    "    \"\"\"\n",
    "    X_unique = []\n",
    "    X_counts = []\n",
    "    X_indeces = []\n",
    "    for i, x in enumerate(X):\n",
    "        # Handle inconsistent behaviour with this little code\n",
    "        # `x not in X_unique` throws error if `x` is a numpy array, but an integer `x`\n",
    "        # cannot be converted to a list by `list(x)`, but only if `x` is an array-like object...\n",
    "        if hasattr(x, \"__len__\"):\n",
    "            x = x.tolist()\n",
    "        if x not in X_unique:\n",
    "            X_unique.append(x)\n",
    "            X_counts.append(1)\n",
    "            X_indeces.append([i])\n",
    "        else:\n",
    "            X_counts[X_unique.index(x)] += 1\n",
    "            X_indeces[X_unique.index(x)].append(i)\n",
    "    \n",
    "    return X_unique, X_counts, X_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(k, X_train, y_train, X_test, scale=True):\n",
    "    \"\"\"\n",
    "    Implements the K-nearest neighbour classifier.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    k : int\n",
    "        Number of neighbors to use in the classification.\n",
    "\n",
    "    X_train : ndarray or array-like in shape of (N, M)\n",
    "        Train dataset.\n",
    "\n",
    "    y_train : ndarray or array-like in shape of (M, P)\n",
    "        Train labels in similar shape as the corresponding train dataset.\n",
    "\n",
    "    X_test : ndarray or array-like in shape of (X, Y)\n",
    "        Test dataset.\n",
    "\n",
    "    scale : bool\n",
    "        Whether to standard scale the datasets.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : \n",
    "        Predicted array of data\n",
    "    \"\"\"\n",
    "    assert X_train.shape[0] == y_train.shape[0],\\\n",
    "            \"Shape of train data is {0}, while shape of train labels are {1}\".format(X_train.shape[0], y_train.shape[0])\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    # Scale data to have zero mean and unit variance\n",
    "    if scale:\n",
    "        X_train = scale_data(X_train)\n",
    "        X_test = scale_data(X_test)\n",
    "    \n",
    "    y_pred = []\n",
    "    # 3. For each point in the test data\n",
    "    for i, test in enumerate(X_test):\n",
    "        # 3.1. Calculate distance between test data and each row of training data\n",
    "        #      Store indeces of training data as well\n",
    "        indeces = np.zeros((len(X_train), ), dtype=int)\n",
    "        dist = np.zeros((len(X_train), ), dtype=float)\n",
    "        for j, train in enumerate(X_train):\n",
    "            indeces[j] = j\n",
    "            dist[j] = ln(a=test, b=train, n=2)\n",
    "        # 3.2. Based on the distance value, sort them in ascending order along indeces\n",
    "        dist, indeces = zip(*sorted(zip(dist, indeces)))\n",
    "        # 3.3. Choose the top K rows from the sorted array\n",
    "        top_k_points = X_train[list(indeces[:k])]\n",
    "        top_k_labels = y_train[list(indeces[:k])]\n",
    "        # 3.4. Assign a class to the test point based on most frequent class of these rows\n",
    "        #      In the case of a tie the closest point's or point cluster's class will be chosen\n",
    "        X_unique, X_counts, X_indeces = count_values(top_k_labels)\n",
    "        # Get the indeces of the most frequent classes. If there is a tie, it returns indeces\n",
    "        # for all top tied classes. If one class is superior, it returns indeces for only that class.\n",
    "        max_indeces = [k for k, v in enumerate(X_counts) if v==np.max(X_counts)]\n",
    "        # Naively we can simply return the class corresponding to the first index in this `max_indeces`\n",
    "        # array. The closest point to the test query will have this class in the superior point clusters.\n",
    "        # If we have only one promiment group of labels, we can simply use this method\n",
    "        if len(max_indeces) == 1:\n",
    "            y_pred.append(X_unique[max_indeces[0]])\n",
    "        # A more robust method is to return the label corresponding to the closest point cluster\n",
    "        # This is useful if we have more than one, equally prominent labels\n",
    "        else:\n",
    "            clust_dist = np.zeros(len(max_indeces), dtype=float)\n",
    "            for i, m in enumerate(max_indeces):\n",
    "                clust_dist[i] = ln(a=test, b=np.sum(top_k_points, axis=0), n=2)\n",
    "            # Find label of closest cluster\n",
    "            clust_dist, X_unique = zip(*sorted(zip(clust_dist, X_unique)))\n",
    "            y_pred.append(X_unique[0])\n",
    "\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_2_normal(X):\n",
    "    \"\"\"\n",
    "    Convert one-hot encoded matrix to 1D array of label indeces.\n",
    "    \"\"\"\n",
    "    return np.array([np.where(p == 1)[0][0]+1 for p in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of model prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_test : array-like of shape (N, )\n",
    "        Original labels of the test dataset.\n",
    "    \n",
    "    y_pred : array-like of shape (N, )\n",
    "        Predicted labels of the test dataset.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Accuracy of model in reference of the true test labels.\n",
    "    \"\"\"\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    correct = 0\n",
    "    for (t, p) in zip(y_test, y_pred):\n",
    "        if hasattr(t, '__len__'):\n",
    "            t = list(t)\n",
    "            p = list(p)\n",
    "        if t == p:\n",
    "            correct += 1\n",
    "    return correct / len(y_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_mat, y, title=None):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(figsize=(12,12))\n",
    "    axes.set_aspect('equal')\n",
    "\n",
    "    im = axes.imshow(conf_mat)\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for X in range(conf_mat.shape[0]):\n",
    "        for Y in range(conf_mat.shape[1]):\n",
    "            axes.text(Y, X, conf_mat[X, Y], fontsize=30,\n",
    "                      ha='center', va='center', color='white', fontweight='bold', \n",
    "                      bbox=dict(color=np.array((0,0,0,0.2)), lw=0)\n",
    "                     )\n",
    "\n",
    "    axes.set_xticks([i for i in range(len(set(y)))])\n",
    "    axes.set_xticklabels([i+1 for i in range(len(set(y)))])\n",
    "    axes.set_yticks([i for i in range(len(set(y)))])\n",
    "    axes.set_yticklabels([i+1 for i in range(len(set(y)))])\n",
    "\n",
    "    axes.set_xlabel('Predicted labels', fontsize=axislabelsize+5, fontweight='bold')\n",
    "    axes.set_ylabel('True labels', fontsize=axislabelsize+5, fontweight='bold')\n",
    "    axes.tick_params(axis='both', which='major', labelsize=axisticksize+5)\n",
    "    axes.xaxis.tick_top()\n",
    "    axes.xaxis.set_label_position('top') \n",
    "\n",
    "    axes.grid(False)\n",
    "\n",
    "    # Create an axis on the right side of `axes`. The width of `cax` will be 5%\n",
    "    # of `axes` and the padding between `cax` and axes will be fixed at 0.1 inch\n",
    "    divider = make_axes_locatable(axes)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.1)\n",
    "    cbar = plt.colorbar(mappable=im, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=axiscbarfontsize, colors='black')\n",
    "    cbar.set_label('Number of occurences', fontsize=axiscbarfontsize+10, labelpad=15, rotation=90)\n",
    "\n",
    "    plt.suptitle(title,\n",
    "                 fontsize=axistitlesize+5, y=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the valid-syntaxed input to the KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "X_train = np.array([[0.9, 0.2, 0.8] , [-1.2, 1.5, 0.7], [5.8, 0.0, 0.9], [6.2, 0.9, 0.9]])\n",
    "y_train = np.array([[0, 1], [0, 1], [1, 0], [0, 1]])\n",
    "X_test  = np.array([[0.8, 0.8, 0.6], [0.5, 0.4, 0.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_classifier(k, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predictions & interpretation\n",
    "\n",
    "- use every second (0, 2, 4, etc indicies) datapoint as training data and use the rest (1, 3, 5, ...) as test data\n",
    "- generate predictions with the implemented KNN with k=5\n",
    "- calculate the accuracy and the confusion matrix for the predictions\n",
    "- if the probability is the same for two or more classes select the first one from the left in the one-hot encoded version (or which has the smallest number in the original dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = df[features[:-1]]\n",
    "y = type_one_hot\n",
    "# Neighbours for KNN\n",
    "k = 5\n",
    "# Prepare train and test sets\n",
    "X_train = X[::2]\n",
    "y_train = y[::2]\n",
    "X_test = X[1::2]\n",
    "y_test = y[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_classifier(k, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_metric(y_test=y_test, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(onehot_2_normal(np.array(y_test)),\n",
    "                            onehot_2_normal(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y,\n",
    "                      title=('Fig. 3. Confusion matrix of the predictions of\\nmy custom KNN algorithm.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare it to Sklearn's KNN\n",
    "- using the same train/test split generate predictions with sklearn KNNs. Use 5 neighbors again\n",
    "- are the predictions the same as for our implementation? (they should be)\n",
    "   - note: to get sklearn perform the same algorithm as our implementation it expects you to provide non one-hot encoding labels. That's why we kept them in the first exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = df[features[:-1]]\n",
    "y = type_orig\n",
    "# Neighbours for KNN\n",
    "k = 5\n",
    "# Prepare train and test sets\n",
    "X_train = X[::2]\n",
    "y_train = y[::2]\n",
    "X_test = X[1::2]\n",
    "y_test = y[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_metric(y_test=y_test, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y,\n",
    "                      title=('Fig. 4. Confusion matrix of the predictions of\\nsklearn\\'s built-in KNN algorithm.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and discussion\n",
    "\n",
    "However there is a difference in predictions of `sklearn`'s and my own algorithm, I could manage the achieve the same accuracy as `sklearn` in this particular problem. I rejected the give advice that said \"if the probability is the same for two or more classes select the first one from the left in the one-hot encoded version (or which has the smallest number in the original dataset)\", since in numerous cases it could be inaccurate and woudl deteriorate the accuracy of my model. My more robust solution is detailed in the comments of the `knn_classifier()` function.\n",
    "\n",
    "While `sklearn`'s algorithm is better identifying the type classes `1`, `2` and `3`, my algorithm superiors in the identification of classes `4`, `5` and `6`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints:\n",
    " - On total you can get 10 points for fully completing all tasks.\n",
    " - Decorate your notebook with, questions, explanation etc, make it self contained and understandable!\n",
    " - Comments you code when necessary\n",
    " - Write functions for repetitive tasks!\n",
    " - Use the pandas package for data loading and handling\n",
    " - Use matplotlib and seaborn for plotting or bokeh and plotly for interactive investigation\n",
    " - Use the scikit learn package for almost everything\n",
    " - Use for loops only if it is really necessary!\n",
    " - Code sharing is not allowed between student! Sharing code will result in zero points.\n",
    " - If you use code found on web, it is OK, but, make its source clear! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
