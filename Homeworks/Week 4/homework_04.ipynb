{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/'\n",
    "out = './out/'\n",
    "\n",
    "# Bold print for Jupyter Notebook\n",
    "b1 = '\\033[1m'\n",
    "b0 = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some matplotlib and seaborn parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axistitlesize = 20\n",
    "axisticksize = 17\n",
    "axislabelsize = 26\n",
    "axislegendsize = 23\n",
    "axistextsize = 20\n",
    "axiscbarfontsize = 15\n",
    "\n",
    "# Set axtick dimensions\n",
    "major_size = 6\n",
    "major_width = 1.2\n",
    "minor_size = 3\n",
    "minor_width = 1\n",
    "mpl.rcParams['xtick.major.size'] = major_size\n",
    "mpl.rcParams['xtick.major.width'] = major_width\n",
    "mpl.rcParams['xtick.minor.size'] = minor_size\n",
    "mpl.rcParams['xtick.minor.width'] = minor_width\n",
    "mpl.rcParams['ytick.major.size'] = major_size\n",
    "mpl.rcParams['ytick.major.width'] = major_width\n",
    "mpl.rcParams['ytick.minor.size'] = minor_size\n",
    "mpl.rcParams['ytick.minor.width'] = minor_width\n",
    "\n",
    "mpl.rcParams.update({'figure.autolayout': False})\n",
    "\n",
    "# Seaborn style settings\n",
    "sns.set_style({'axes.axisbelow': True,\n",
    "               'axes.edgecolor': '.8',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': True,\n",
    "               'axes.labelcolor': '.15',\n",
    "               'axes.spines.bottom': True,\n",
    "               'axes.spines.left': True,\n",
    "               'axes.spines.right': True,\n",
    "               'axes.spines.top': True,\n",
    "               'figure.facecolor': 'white',\n",
    "               'font.family': ['sans-serif'],\n",
    "               'font.sans-serif': ['Arial',\n",
    "                'DejaVu Sans',\n",
    "                'Liberation Sans',\n",
    "                'Bitstream Vera Sans',\n",
    "                'sans-serif'],\n",
    "               'grid.color': '.8',\n",
    "               'grid.linestyle': '--',\n",
    "               'image.cmap': 'rocket',\n",
    "               'lines.solid_capstyle': 'round',\n",
    "               'patch.edgecolor': 'w',\n",
    "               'patch.force_edgecolor': True,\n",
    "               'text.color': '.15',\n",
    "               'xtick.bottom': True,\n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'in',\n",
    "               'xtick.top': True,\n",
    "               'ytick.color': '.15',\n",
    "               'ytick.direction': 'in',\n",
    "               'ytick.left': True,\n",
    "               'ytick.right': True})\n",
    "\n",
    "# Colorpalettes, colormaps, etc.\n",
    "sns.set_palette(palette='rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the provided .npy files. You can load it with numpy.\n",
    " - each file contains one vector, X and y\n",
    " - visualize X vs y on a scatter plot\n",
    " - fit an $y = w_0 + w_1\\cdot X + w_2 \\cdot X^2$ linear regression using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./a. Load and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(data + 'X.npy')\n",
    "y = np.load(data + 'y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "sc = 7\n",
    "axes.scatter(X, y, label='Original data',\n",
    "             color=cm.magma(y/y.max()/2 + 0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 1. Raw input data. The datapoints are shaded by the\\nvalue of their $y$ coordinates.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Clearly an intentional sawtooth-like/alternating pattern could be observed in the dataset visualized above. If we want to fit a polynomial on this dataset we're needed to be aware of this artifact, cause it could easily lead us to the textbook case of overfitting. (Obviously that's why this dataset is in the assignment.) Originally there are $1000$ datapoints were randomly generated in the interval $\\left[ -100, 100 \\right]$ to create the dataset X. An arbitrary second-order polynomial was then created to get the $y$ coordinates. Either the coordinates obtained or one of the original coefficients were then altered by some periodic wave-like function to get the final image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./b. Fit linear regression using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit from sklearn's documentation\n",
    "# https://scikit-learn.org/0.23/auto_examples/model_selection/plot_underfitting_overfitting.html\n",
    "pipeline = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "                     (\"linear_regression\", LinearRegression())])\n",
    "# Transform X for the PolynomialFeatures() and LinearRegression() class\n",
    "# Then fit on the pipeline the available data\n",
    "pipeline.fit(X[:, np.newaxis], y)\n",
    "# Get coefficients\n",
    "b, a = pipeline[1].coef_\n",
    "c = pipeline[1].intercept_\n",
    "# Evaluate the models using crossvalidation\n",
    "scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "# Test dataset to plot fitted function\n",
    "X_test = np.linspace(-100, 100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "sc = 7\n",
    "axes.scatter(X, y, label='Original data',\n",
    "             color=cm.magma(y/y.max()/2 + 0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "axes.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label='Fitted model',\n",
    "          color=cm.magma(0.93), lw=6, ls='--', alpha=0.8)\n",
    "\n",
    "axes.set_title('Equation of fitted polynomial: $({0:.3f}x^2) + ({1:.3f}x) + ({2:.3f})$'.format(a, b, c),\n",
    "               fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='best', fontsize=axislegendsize)\n",
    "\n",
    "fig.suptitle('Fig. 2. The fitted second order polynomial with sklearn\\'s\\nlinear regression on raw dataset.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./c. Test whether fit coefficients are actually right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2(X, a, b, c):\n",
    "    \n",
    "    return a*X**2 + b*X + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "sc = 7\n",
    "axes.scatter(X, y, label='Original data',\n",
    "             color=cm.magma(y/y.max()/2 + 0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "axes.plot(X_test, poly2(X=X_test, a=a, b=b, c=c), label='Fitted model',\n",
    "          color=cm.magma(0.93), lw=6, ls='--', alpha=0.8)\n",
    "\n",
    "axes.set_title('Equation of fitted polynomial: $({0:.3f}x^2) + ({1:.3f}x) + ({2:.3f})$'.format(a, b, c),\n",
    "               fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='best', fontsize=axislegendsize)\n",
    "\n",
    "fig.suptitle('Fig. 3. The second order polynomial draw by using the obtained\\nfitting coefficients from sklearn\\'s naive polynomial fit.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using different features\n",
    " - plot the residuals (the difference between the prediction and the actual $y$) vs the original $y$\n",
    " - a non-random-noise like pattern suggests non-linear connection between the features and the predictions\n",
    " - someone told us that the connection between X and y is $y = A \\cdot X^{2} + B \\cdot X + C\\cdot \\cos^3(X) + D$\n",
    "    - using sklearn's linear regression estimate $A, B, C, D$!\n",
    " - plot the residuals again! is it better now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./a. Plot residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "diff = y - pipeline.predict(X[:, np.newaxis])\n",
    "axes.plot(diff, label='Residuals',\n",
    "          color=cm.magma(0.5), lw=1, alpha=0.7)\n",
    "\n",
    "axes.set_title('RMS = {0:.3f}'.format(np.sqrt(np.mean(diff**2))), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y_{\\mathrm{groundtruth}} - y_{\\mathrm{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 4. Difference between predicted and original data\\nin the case of sklearn\\'s naive polynomial fit.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./b. Fit more accurate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2_var(X, a, b, c, d):\n",
    "    \n",
    "    return a*X**2 + b*X + c*np.cos(X)**3 + d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First test with an arbitrary value $c = 150$\n",
    "\n",
    "(Fits pretty well actually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "sc = 7\n",
    "axes.scatter(X, y, label='Original data',\n",
    "             color=cm.magma(y/y.max()/2 + 0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "axes.plot(X_test, poly2_var(X=X_test, a=a, b=b, c=150, d=c), label='Fitted model',\n",
    "          color=cm.magma(0.93), lw=3, ls='--', alpha=0.8)\n",
    "\n",
    "axes.set_title('Equation of fitted polynomial: $({0:.3f}\\,x^2) + ({1:.3f}\\,x) + ({2:.3f}\\,\\cos^3(x)) + ({3:.3f})$'.format(a, b, 150, c),\n",
    "               fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='best', fontsize=axislegendsize)\n",
    "\n",
    "fig.suptitle('Fig. 5. The altered second order polynomial with an arbitrary coefficient\\nfor its perturbating term.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the `sklearn` fit\n",
    "\n",
    "Fitting an arbitrary function with regression in `sklearn` is a bit tricky and needs two steps. First we need to transform our input data using an `sklearn` transformer, then we can input the obtained dataset into a regressor (eg. `LinearRegression()`). For the first step we need to generate the individual componenets (features) using our arbitrary function. For this I'll use the `FunctionTransformer()` class from the `sklearn.preprocessing` module, because it can be incuded in an `sklearn` pipeline the same way the `PolynomialFeatures()` did in the previous blocks. After that we can simply use linear regression as I stated before.\n",
    "\n",
    "##### Sources\n",
    "[1] : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2_reg(X):\n",
    "    \n",
    "    # Returns the transformed array using the equation\n",
    "    #    ```A * X^2 + B * cos^3(X) + C * X + D```\n",
    "    return np.hstack((np.cos(X)**3, X, X**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit from sklearn's documentation\n",
    "# https://scikit-learn.org/0.23/auto_examples/model_selection/plot_underfitting_overfitting.html\n",
    "pipeline = Pipeline([(\"polynomial_variation\", FunctionTransformer(poly2_reg)),\n",
    "                     (\"linear_regression\", LinearRegression())])\n",
    "# Transform X for the PolynomialFeatures() and LinearRegression() class\n",
    "# Then fit on the pipeline the available data\n",
    "pipeline.fit(X[:, np.newaxis], y)\n",
    "# Get coefficients\n",
    "c, b, a = pipeline[1].coef_\n",
    "d = pipeline[1].intercept_\n",
    "# Evaluate the models using crossvalidation\n",
    "scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "# Test dataset to plot fitted function\n",
    "X_test = np.linspace(-100, 100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "sc = 7\n",
    "axes.scatter(X, y, label='Original data',\n",
    "             color=cm.magma(y/y.max()/2 + 0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "axes.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label='Fitted model',\n",
    "          color=cm.magma(0.93), lw=3, ls='--', alpha=0.8)\n",
    "\n",
    "axes.set_title('Equation of fitted polynomial: $({0:.3f}\\,x^2) + ({1:.3f}\\,x) + ({2:.3f}\\,\\cos^3(x)) + ({3:.3f})$'.format(a, b, c, d),\n",
    "               fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='best', fontsize=axislegendsize)\n",
    "\n",
    "fig.suptitle('Fig. 6. The fitted, altered second order polynomial with sklearn\\'s\\nlinear regression on the raw dataset.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "diff = y - pipeline.predict(X[:, np.newaxis])\n",
    "axes.plot(diff, label='Residuals',\n",
    "          color=cm.magma(0.5), lw=1, alpha=0.7)\n",
    "\n",
    "axes.set_title('RMS = {0:.3f}'.format(np.sqrt(np.mean(diff**2))), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y_{\\mathrm{groundtruth}} - y_{\\mathrm{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 7. Difference between predicted and original data\\nin the case of sklearn\\'s altered polynomial fit.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Other methdods than sklearn for linear regression\n",
    " - using the statsmodels package perform the same linear regression as in 2.) (hint: use statsmodels.api.OLS)\n",
    " - is the result the same? if not guess, why? (did you not forget to add the constant term?)\n",
    " - try to get the same results with statsmodels as with sklearn!\n",
    " - using the analytic solution formula shown during the lecture, calculate the coefficients (A, B, C, D). are they the same compared to the two previous methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3./a. `statsmodels` on the **first** task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting coefficients are very similar to the coefficients obtained in the first task. Since these coefficients were tested already, we know that these are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataset by default now\n",
    "X_t = X[:, np.newaxis]\n",
    "# Create a second order polynomial with perturbation from the data\n",
    "X_t = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_t)\n",
    "# Append a constant factor for `statsmodels`\n",
    "X_t = sm.add_constant(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X_t)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, b, a = results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "sc = 7\n",
    "axes.scatter(X, y, label='Original data',\n",
    "             color=cm.magma(y/y.max()/2 + 0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "axes.plot(X_test, poly2(X_test, a, b, c), label='Fitted model',\n",
    "          color=cm.magma(0.93), lw=6, ls='--', alpha=0.8)\n",
    "\n",
    "axes.set_title('Equation of fitted polynomial: $({0:.3f}\\,x^2) + ({1:.3f}\\,x) + ({2:.3f})$'.format(a, b, c),\n",
    "               fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='best', fontsize=axislegendsize)\n",
    "\n",
    "fig.suptitle('Fig. 8. The fitted second order polynomial with statsmodels\\'\\nlinear regression on the raw dataset.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "diff = y - poly2(X, a, b, c)\n",
    "axes.plot(diff, label='Residuals',\n",
    "          color=cm.magma(0.5), lw=1, alpha=0.7)\n",
    "\n",
    "axes.set_title('RMS = {0:.3f}'.format(np.sqrt(np.mean(diff**2))), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y_{\\mathrm{groundtruth}} - y_{\\mathrm{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 9. Difference between predicted and original data\\nin the case of statsmodels\\' naive polynomial fit.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3./a. `statsmodels` on the **second** task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataset by default now\n",
    "X_t = X[:, np.newaxis]\n",
    "# Create a second order polynomial from the data\n",
    "X_t = FunctionTransformer(poly2_reg).fit_transform(X_t)\n",
    "# Append a constant factor for `statsmodels`\n",
    "X_t = sm.add_constant(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X_t)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, c, b, a = results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "sc = 7\n",
    "axes.scatter(X, y, label='Original data',\n",
    "             color=cm.magma(y/y.max()/2 + 0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "axes.plot(X_test, poly2_var(X_test, a, b, c, d), label='Fitted model',\n",
    "          color=cm.magma(0.93), lw=3, ls='--', alpha=0.8)\n",
    "\n",
    "axes.set_title('Equation of fitted polynomial: $({0:.3f}\\,x^2) + ({1:.3f}\\,x) + ({2:.3f}\\,\\cos^3(x)) + ({3:.3f})$'.format(a, b, c, d),\n",
    "               fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='best', fontsize=axislegendsize)\n",
    "\n",
    "fig.suptitle('Fig. 10. The fitted, altered second order polynomial with statsmodels\\'\\nlinear regression on the raw dataset',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "diff = y - poly2_var(X, a, b, c, d)\n",
    "axes.plot(diff, label='Residuals',\n",
    "          color=cm.magma(0.5), lw=1, alpha=0.7)\n",
    "\n",
    "axes.set_title('RMS = {0:.3f}'.format(np.sqrt(np.mean(diff**2))), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y_{\\mathrm{groundtruth}} - y_{\\mathrm{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 11. Difference between predicted and original data\\nin the case of statsmodels\\' altered polynomial fit.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second fit is trivially much-much better, as all scoring functions shows it in the upper right column. Also the obtained function form could be plotted seamlessly on the original data. There is a very slight difference between `sklearn` and `statsmodels`, which arises from the difference in their fitting algorithms, but they virtually gave the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze the real estate dataset\n",
    " - load the real_estate.csv to a pandas dataframe\n",
    " - drop the ID column and the geographic location columns\n",
    " - fit a linear regression model to predict the unit price using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./a. Load and \"preprocess\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data + 'real_estate.csv', sep=',', index_col='No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used = df.drop(columns=['X5 latitude', 'X6 longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_used.head())\n",
    "display(df_used.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./b. Fit a model using linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(X, m, b):\n",
    "    \n",
    "    Y = m * X + b\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_es = np.array(df_used[df_used.columns[:-1]])\n",
    "y_es = np.array(df_used[df_used.columns[-1]])\n",
    "\n",
    "X_es_train = X_es[::2]\n",
    "y_es_train = y_es[::2]\n",
    "X_es_test = X_es[1::2]\n",
    "y_es_test = y_es[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train = LinearRegression().fit(X_es_train, y_es_train)\n",
    "model_full = LinearRegression().fit(X_es, y_es)\n",
    "# Compute predictions\n",
    "pred_train = model_train.predict(X_es_train)\n",
    "pred_test = model_train.predict(X_es_test)\n",
    "pred_full = model_full.predict(X_es)\n",
    "# Compute R-squared error\n",
    "r_sq_train = model_train.score(X_es_train, y_es_train)\n",
    "r_sq_full = model_full.score(X_es, y_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b1 + 'TRAIN DATASET FIT' + b0 + '\\n' +\n",
    "           '-----------------')\n",
    "print(b1 + 'Coefficient of determination (TRAIN):' + b0, r_sq_train)\n",
    "print(b1 + 'Intercept (TRAIN):' + b0, model_train.intercept_)\n",
    "print(b1 + 'Slope coefficients (TRAIN):' + b0)\n",
    "for col, mc in zip(df_used.columns[:-1], model_train.coef_):\n",
    "    print('    {0} : {1}'.format(col[:2], mc))\n",
    "\n",
    "print()\n",
    "print(b1 + 'FULL DATASET FIT' + b0 + '\\n' +\n",
    "           '----------------')\n",
    "print(b1 + 'Coefficient of determination (FULL):' + b0, r_sq_full)\n",
    "print(b1 + 'Intercept (FULL):' + b0, model_full.intercept_)\n",
    "print(b1 + 'Slope coefficients (FULL):' + b0)\n",
    "for col, mc in zip(df_used.columns[:-1], model_full.coef_):\n",
    "    print('    {0} : {1}'.format(col[:2], mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*10, nrows*10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "sc = 6\n",
    "\n",
    "# Pred and groundtruth y values\n",
    "y_pred = [pred_train, pred_test, pred_full]\n",
    "y_grnd = [y_es_train, y_es_test, y_es]\n",
    "# Titles for subplots\n",
    "titles = ['Train dataset', 'Test dataset', 'Full dataset']\n",
    "# X values for the m=1, b=0 line\n",
    "x_line = np.linspace(0,80,80)\n",
    "for i in range(ncols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    ax.set_xlim(0,80)\n",
    "    ax.set_ylim(0,80)\n",
    "    \n",
    "    ax.scatter(y_grnd[i], y_pred[i],\n",
    "               color=cm.magma(0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "    ax.plot(x_line, line(x_line, m=1, b=0),\n",
    "            color=cm.magma(0.93), lw=3, ls='--', alpha=0.7)\n",
    "    ax.set_title(titles[i] + '\\nAccuracy : {0:.3f} %'.format(r2_score(y_grnd[i], y_pred[i]) * 100),\n",
    "                 fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "    ax.set_xlabel('$y_{\\mathrm{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('$y_{\\mathrm{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "    \n",
    "fig.suptitle('Fig. 12. Accuracy of predictions for different test samples. In the leftmost and middle figures the model was trained\\n' +\n",
    "             'only on half of the dataset then the train and test datasets were used respectively for prediction. In the\\n' +\n",
    "             'rightmost figure the full dataset was used for both training and prediction.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.00)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpret results\n",
    " - interpret the coefficients and their meaning shortly with your own words\n",
    " - plot the residuals for the predictions. if you had to decide only on this information, which house would you buy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./a. Interpret coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*9, nrows*9),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "sc = 4\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        # For abbreviation select relevant data for current subplot\n",
    "        ax = axes[j][i]\n",
    "        idx = i*ncols + j\n",
    "        col = df_used.columns[:-1][idx]\n",
    "        x_es = X_es[:, idx]\n",
    "        \n",
    "        ax.scatter(x_es, y_es,\n",
    "                   color=cm.magma(0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "        \n",
    "        ax.set_title('Feature : \"{0}\"'.format(col[3:]),\n",
    "                     fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "        ax.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "        ax.set_ylabel('$y_{\\mathrm{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "        ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 13. Connection between train features and house unit prices.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.06)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four obtained coefficients are clearly the slopes of linear functions fitted on the four point ensembles above. It could be seen by doing a linear fit on data on the subplots above.\n",
    "\n",
    "This individual analysis of features and house prices below really gives us coefficients close to the ones obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features\n",
    "N_features = X_es.shape[1]\n",
    "# Save coefficients and locations of intercept of linear fits\n",
    "coeffs = np.zeros(N_features)\n",
    "intercepts = np.zeros(N_features)\n",
    "for i in range(N_features):\n",
    "    model = LinearRegression().fit(X_es[:, i][:, np.newaxis], y_es)\n",
    "    coeffs[i] = model.coef_\n",
    "    intercepts[i] = model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*9, nrows*9),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.2)\n",
    "\n",
    "sc = 4\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        # For abbreviation select relevant data for current subplot\n",
    "        ax = axes[j][i]\n",
    "        idx = i*ncols + j\n",
    "        col = df_used.columns[:-1][idx]\n",
    "        x_es = X_es[:, idx]\n",
    "        \n",
    "        x_test = np.linspace(x_es.min(), x_es.max(), 80)\n",
    "        ax.scatter(x_es, y_es,\n",
    "                   color=cm.magma(0.5), marker='o', ec='none', s=sc**2, alpha=0.7)\n",
    "        ax.plot(x_test, line(x_test, m=model_full.coef_[idx], b=intercepts[idx]), label='Compact',\n",
    "                color=cm.magma(0.7), lw=3, ls='--', alpha=0.8)\n",
    "        ax.plot(x_test, line(x_test, m=coeffs[idx], b=intercepts[idx]), label='Individual',\n",
    "                color=cm.magma(0.93), lw=3, ls='--', alpha=0.8)\n",
    "        \n",
    "        ax.set_title('Feature : \"{0}\"\\nCompact slope : {1:.3e}\\nIndividual slope : {2:.3e}'.format(col[3:], model_full.coef_[idx], coeffs[idx]),\n",
    "                     fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "        ax.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "        ax.set_ylabel('$y_{\\mathrm{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "        ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "        ax.legend(loc='best', fontsize=axislegendsize)\n",
    "        \n",
    "fig.suptitle('Fig. 14. Connection between train features and house unit prices with linear fits.\\n' +\n",
    "             '\"Compact slope\" refers to the coefficients obtained in the original analysis, while the\\n' +\n",
    "             '\"Individual slope\" refers to the coefficients obtained by individually fitting linear functions\\n' +\n",
    "             'on the different subspaces of the real estate point enseble.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.06)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "The upper left subplot is actually correct. Since the intercept of the fit is a relatively large number ($b \\approx -8400$), the fitted line's position will be very sensitive to the value of its slope. That's why with $m \\approx 5.7$ the line of predictions runs really far away from the one with the coefficient $m \\approx 4.2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./b. Plot residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 1\n",
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "diff_es = y_es - pred_full\n",
    "axes.plot(diff_es, label='Residuals',\n",
    "          color=cm.magma(0.5), lw=1, alpha=0.7)\n",
    "\n",
    "axes.set_title('RMS = {0:.3f}'.format(np.sqrt(np.mean(diff**2))), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y_{\\mathrm{groundtruth}} - y_{\\mathrm{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 15. Difference between predicted and original data\\nin the case of the real estate dataset.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 1\n",
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "diff_es = y_es - pred_full\n",
    "y_es_s, diff_es_s = zip(*sorted(zip(y_es, diff_es)))\n",
    "axes.plot(y_es_s, diff_es_s, label='Residuals',\n",
    "          color=cm.magma(0.5), lw=1, alpha=0.7)\n",
    "\n",
    "axes.set_xlabel('$y_{\\mathrm{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y_{\\mathrm{groundtruth}} - y_{\\mathrm{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 16. Difference between predicted and original data\\nin the case of the real estate dataset plotted against\\nthe original price of the real estate.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the best choice is to buy the house with the lowest negative difference between $y_{\\mathrm{groundtruth}}$ and $y_{\\mathrm{pred}}$. This means that according to our model the house is seriously underpriced, so we could make the most profit buying it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints:\n",
    " - On total you can get 10 points for fully completing all tasks.\n",
    " - Decorate your notebook with, questions, explanation etc, make it self contained and understandable!\n",
    " - Comments you code when necessary\n",
    " - Write functions for repetitive tasks!\n",
    " - Use the pandas package for data loading and handling\n",
    " - Use matplotlib and seaborn for plotting or bokeh and plotly for interactive investigation\n",
    " - Use the scikit learn package for almost everything\n",
    " - Use for loops only if it is really necessary!\n",
    " - Code sharing is not allowed between student! Sharing code will result in zero points.\n",
    " - If you use code found on web, it is OK, but, make its source clear! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
