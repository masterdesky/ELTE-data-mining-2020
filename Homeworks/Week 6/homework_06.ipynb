{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/'\n",
    "os.makedirs(data, exist_ok=True)\n",
    "out = './out/'\n",
    "os.makedirs(out, exist_ok=True)\n",
    "\n",
    "# Bold print for Jupyter Notebook\n",
    "b1 = '\\033[1m'\n",
    "b0 = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some matplotlib and seaborn parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axistitlesize = 20\n",
    "axisticksize = 17\n",
    "axislabelsize = 26\n",
    "axislegendsize = 23\n",
    "axistextsize = 20\n",
    "axiscbarfontsize = 15\n",
    "\n",
    "# Set axtick dimensions\n",
    "major_size = 6\n",
    "major_width = 1.2\n",
    "minor_size = 3\n",
    "minor_width = 1\n",
    "mpl.rcParams['xtick.major.size'] = major_size\n",
    "mpl.rcParams['xtick.major.width'] = major_width\n",
    "mpl.rcParams['xtick.minor.size'] = minor_size\n",
    "mpl.rcParams['xtick.minor.width'] = minor_width\n",
    "mpl.rcParams['ytick.major.size'] = major_size\n",
    "mpl.rcParams['ytick.major.width'] = major_width\n",
    "mpl.rcParams['ytick.minor.size'] = minor_size\n",
    "mpl.rcParams['ytick.minor.width'] = minor_width\n",
    "\n",
    "mpl.rcParams.update({'figure.autolayout': False})\n",
    "\n",
    "# Seaborn style settings\n",
    "sns.set_style({'axes.axisbelow': True,\n",
    "               'axes.edgecolor': '.8',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': True,\n",
    "               'axes.labelcolor': '.15',\n",
    "               'axes.spines.bottom': True,\n",
    "               'axes.spines.left': True,\n",
    "               'axes.spines.right': True,\n",
    "               'axes.spines.top': True,\n",
    "               'figure.facecolor': 'white',\n",
    "               'font.family': ['sans-serif'],\n",
    "               'font.sans-serif': ['Arial',\n",
    "                'DejaVu Sans',\n",
    "                'Liberation Sans',\n",
    "                'Bitstream Vera Sans',\n",
    "                'sans-serif'],\n",
    "               'grid.color': '.8',\n",
    "               'grid.linestyle': '--',\n",
    "               'image.cmap': 'rocket',\n",
    "               'lines.solid_capstyle': 'round',\n",
    "               'patch.edgecolor': 'w',\n",
    "               'patch.force_edgecolor': True,\n",
    "               'text.color': '.15',\n",
    "               'xtick.bottom': True,\n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'in',\n",
    "               'xtick.top': True,\n",
    "               'ytick.color': '.15',\n",
    "               'ytick.direction': 'in',\n",
    "               'ytick.left': True,\n",
    "               'ytick.right': True})\n",
    "\n",
    "# Colorpalettes, colormaps, etc.\n",
    "sns.set_palette(palette='rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement a linear model\n",
    "\n",
    "* return the weight parameters $w = \\left\\{ w_{1}, w_{2}, \\dots , w_{p} \\right\\}$ and the intercept parameter $w_{0}$ separately where:\n",
    "\n",
    "$$\n",
    "\\hat{y} \\left( \\vec{w}, \\vec{x} \\right)\n",
    "=\n",
    "w_{0} + w_{1} x_{1} + \\dots + w_{p} x_{p}\n",
    "$$\n",
    "\n",
    "* check your returned coefficients with the built in `LinearRegression` class from the `sklearn` library, they should be within tolerance `1e-6`to each other\n",
    "\n",
    "* use a generated regression dataset `from sklearn.dataset import make_regression` API with parameters `n_samples=1000` and `n_features=20`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "# Get coefficients\n",
    "w = lin_reg.coef_\n",
    "w_0 = lin_reg.intercept_\n",
    "# Get predictions\n",
    "y_pred_skl = lin_reg.predict(X_test)\n",
    "# Calculate difference between predicted and test values\n",
    "diff_skl = y_test - y_pred_skl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own function to export $w$ and $w_{0}$ parameters\n",
    "\n",
    "I've decided to write a Python class for this. In the most basic examples, regression usually based on gradient descent (at least that's what I've learned 4 years ago in my deep learning class). We"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression(object):\n",
    "    \"\"\"\n",
    "    Performs linear regression with OLS method using gradient descent.\n",
    "    \n",
    "    Calculates the coefficients and interception parameter of a\n",
    "    multivariate linear equation in the form\n",
    "    \n",
    "    y(w, x) = w_0 + w_1 x_1 + ... + w_n x_n\n",
    "    \n",
    "    while minimizing the residual sum of squares between target values\n",
    "    and the predictions made using the input dataset and the current\n",
    "    coefficients.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=1e-04, iters=100000):\n",
    "        self.lr = lr\n",
    "        self.iters = iters\n",
    "\n",
    "    def __cost_func(self, X, y, coef_):\n",
    "        \"\"\"\n",
    "        Calculate the cost for OLS. Literally learned this from Andrew Ng.\n",
    "        \"\"\"\n",
    "        return np.sum(((X @ coef_) - y)**2) / (2 * len(y))\n",
    "\n",
    "    def __gradient_descent(self, X, y):\n",
    "        \"\"\"\n",
    "        Performs gradient descent with OLS to get the correct coefficients\n",
    "        and interception parameter for our model.\n",
    "        \"\"\"\n",
    "        self.cost_history = np.zeros(self.iters)\n",
    "        self.loss_history = np.zeros(self.iters)\n",
    "        coef_ = self.coef_\n",
    "        for i in range(self.iters):\n",
    "            # Calculate hypothesis\n",
    "            h = X @ coef_\n",
    "            # Difference between hypothesis and groundtruth\n",
    "            loss = h - y\n",
    "            self.loss_history[i] = np.mean(loss)\n",
    "            # Gradient calculation\n",
    "            gradient = (X.T @ loss) / len(y)\n",
    "            # Changing values of coef_ using gradient\n",
    "            coef_ = coef_ - self.lr * gradient            \n",
    "            # Calculate cost\n",
    "            cost = self.__cost_func(X, y, coef_)\n",
    "            self.cost_history[i] = cost\n",
    "            #if i % 6000 == 0:\n",
    "            #    print('Cost : {0}'.format(cost))\n",
    "        return coef_\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits and input dataset with X train data and y train labels\n",
    "        using gradient descent.\n",
    "        \"\"\"\n",
    "        # Number of features in the dataset\n",
    "        self.dim = X.shape[1]\n",
    "        # Initialize coef_ for gradient descent\n",
    "        self.coef_ = np.ones(self.dim)\n",
    "        # Perform gradient descent and update coef_ coeffs\n",
    "        self.coef_ = self.__gradient_descent(X, y)\n",
    "        self.intercept_ = np.sum(y - X @ self.coef_) / len(y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X @ self.coef_ + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression\n",
    "my_lin_reg = linear_regression(iters=300000)\n",
    "my_lin_reg.fit(X_train, y_train)\n",
    "# Get coefficients\n",
    "w = my_lin_reg.coef_\n",
    "w_0 = my_lin_reg.intercept_\n",
    "# Get predictions\n",
    "y_pred_myl = my_lin_reg.predict(X_test)\n",
    "# Calculate difference between predicted and test values\n",
    "diff_myl = y_test - y_pred_myl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*10, nrows*10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes[0].plot(my_lin_reg.cost_history,\n",
    "          color=cm.magma(0.5), lw=3)\n",
    "axes[1].plot(my_lin_reg.loss_history,\n",
    "             color=cm.magma(0.75), lw=3)\n",
    "\n",
    "y_label = ['Cost', 'Sum of losses']\n",
    "for i in range(ncols):\n",
    "    ax = axes[i]\n",
    "    ax.set_xlabel('Iterations', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel(y_label[i], fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "    ax.tick_params(axis='x', which='major', rotation=35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*11, nrows*11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "sc = 12\n",
    "ax = axes[0]\n",
    "ax.scatter(y_test, y_pred_myl, label='My method',\n",
    "           color=cm.magma(0.5), s=sc**2, ec='black', alpha=0.4)\n",
    "ax = axes[1]\n",
    "ax.scatter(y_test, y_pred_skl, label='sklearn',\n",
    "           color=cm.magma(0.75), s=sc**2, ec='black', alpha=0.4)\n",
    "\n",
    "diff = [diff_myl, diff_skl]\n",
    "for i in range(ncols):\n",
    "    ax = axes[i]\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    x_lim = 600\n",
    "    y_lim = 600\n",
    "    ax.plot([-x_lim, x_lim], [-y_lim, y_lim],\n",
    "            color=cm.magma(0.93), lw=4, ls='--', zorder=3, alpha=0.5)\n",
    "    \n",
    "    ax.set_xlim(-x_lim, x_lim)\n",
    "    ax.set_ylim(-y_lim, y_lim)\n",
    "\n",
    "    ax.set_title('MAE = {0:.7f}'.format(np.mean(np.abs(diff[i]))), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "    ax.set_xlabel('$\\mathrm{y_{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('$\\mathrm{y_{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', rotation=20, labelsize=axisticksize, colors='white')\n",
    "    \n",
    "    ax.legend(loc='lower right', fontsize=axislegendsize)\n",
    "\n",
    "fig.suptitle('Fig. 2. Difference between predicted and original data\\nin the case of my (on the left) and sklearn\\'s (on the right) naive linear regression.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes.plot(diff, label='Residuals',\n",
    "          color=cm.magma(0.5), lw=1, alpha=0.7)\n",
    "\n",
    "axes.set_title('MAE = {0:.7f}'.format(np.mean(np.abs(diff))), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y_{\\mathrm{groundtruth}} - y_{\\mathrm{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 3. Difference between predicted and original data\\nin the case of my naive linear regression.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes.plot(y_pred_myl - y_pred_skl, label='Residuals',\n",
    "          color=cm.magma(0.5), lw=1, alpha=0.7)\n",
    "\n",
    "axes.set_title('Max difference = {0:.5e}'.format(np.max(np.abs(y_pred_myl - y_pred_skl))), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$X$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$y_{\\mathrm{my\\_func}} - y_{\\mathrm{sklearn}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 4. Difference between the predictions of my\\nlinear regression and sklearn\\'s.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "The difference between the two estimators are below the limit of $10^{-6}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use of real data\n",
    "\n",
    "* download the [Communities and Crime Data Set](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime) from UCI, the task includes understanding the dataset: naming the appropiate data fields, handling missing values, etc.\n",
    "    \n",
    "* fit a `LinearRegression` model with 5-fold cross-validation - compare training and testing scores (R^2 by default) for the different CV splits, print the mean score and its standard deviation\n",
    "\n",
    "* find the best `Lasso` regression model with 5-fold grid search cross validation (`GridSearchCV`) on the parameters: `alpha, normalize, max_iter` and show the best parameter set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./a. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/static/public/183/communities+and+crime.zip\"\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(requests.get(url).content))\n",
    "zip_file.extractall(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "# Feature names start with `@attribute`, followed by the feature name,\n",
    "# then ending with the type of the feature values (numeric/string/etc.)\n",
    "with open(data + 'communities.names') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if '@attribute' in line.strip():\n",
    "            features.append(line.strip().split(' ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values are marked with an `?` in the dataset\n",
    "df = pd.read_csv(data + 'communities.data', sep=',', names=features, na_values=['?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./b. Handle missing/ID labels\n",
    "\n",
    "While missing values in meaningful features should be filled appropriately, columns representing ID-like variables can be deleted. Location and violent crime rates do correlates in real life, but an idea of a causal relationship between location and crime rates can be discarded now. Description for each feature can be accessed in the `community.names` file.\n",
    "\n",
    "#### ID-like columns\n",
    "The first 4 columns (`state`, `county`, `community`, `communityname`) can be delete, because they represent locational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[features[4:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `fold` is a debug feature from cross-validation, which can be also discarded for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[features[5:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features with missing values\n",
    "\n",
    "According to the feature descriptions, all remaining columns are in a decimal format and scaled into the inteval of $\\left[ 0, 1 \\right]$. The only exception is the feature `LemasGangUnitDeploy`, which is actually an ordinal with values $0.0$, $0.5$ and $1.0$. We can still however handle it as a decimal feature.\n",
    "\n",
    "There is a table in the `community.names` description file, which summarize the basic statistical attributes (mean, median, standard deviation, etc.) of each features in the dataset. According to this table any features with missing entries have exactly $1675$ missing values in each of them. (There is only one exception, the column `OtherPerCap`, where only $1$ value is missing.). It is entirely logical to assume that in this case the missing features are always missing from the same lines. If the hypothesis is true, we can test it by visualizing the missing values on a matrixplot. If we plot features on the $y$-axis, we should see only horizontal lines (which are interrupted by vertical gaps) in the dataset, instead of individual points scattered all around the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 100\n",
    "fig, axes = plt.subplots(figsize=(figsize,figsize))\n",
    "axes.set_aspect('equal')\n",
    "axes.grid(False)\n",
    "\n",
    "axes.imshow(df.isna().T)\n",
    "axes.set_xlabel('Locations', fontsize=50, fontweight='bold')\n",
    "axes.set_ylabel('Features', fontsize=50, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are indeed \"horizontal lines interrupted by vertical gaps\". However these features miss most of their values. In this case we should consider simply dropping these features from the model, since filling them up with artificial values could reasonably distort the impact of these features on our model. I'll try this method in for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with at least 50% of values missing\n",
    "df_n = df.dropna(axis=1, thresh=int(0.5 * len(df)), inplace=False)\n",
    "\n",
    "# Fill that 1 remaining entry with the mean of the corresponding feature\n",
    "df_n = df_n.fillna(df_n.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 100\n",
    "fig, axes = plt.subplots(figsize=(figsize,figsize))\n",
    "axes.set_aspect('equal')\n",
    "axes.grid(False)\n",
    "\n",
    "axes.imshow(df_n.isna().T)\n",
    "axes.set_xlabel('Locations', fontsize=50, fontweight='bold')\n",
    "axes.set_ylabel('Features', fontsize=50, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./c. Fit linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds\n",
    "folds = 5\n",
    "# Invoke the KFold class from sklearn for CV tests\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# The model we use is linear regression\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create the X and y datasets\n",
    "X = df_n[df_n.columns[:-1]]\n",
    "y = df_n[df_n.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test R^2 score\n",
    "# Refrence: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "scores = cross_val_score(model, X, y, scoring='r2', cv=cv)\n",
    "\n",
    "print('KFOLD SCORES:\\n' +\n",
    "      '----------------')\n",
    "print(scores)\n",
    "print('Mean of scores : {0:.4f}'.format(np.mean(scores)))\n",
    "print('Std of scores : {0:.4f}'.format(np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./d. Fit Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold search is needed\n",
    "folds = 5\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# Lasso estimator\n",
    "model = Lasso(random_state=None)\n",
    "# Paramters to explored:\n",
    "# alpha, normalize, max_iter\n",
    "param_grid = {\n",
    "    'alpha' : np.logspace(-10, 1, 100, dtype=int),\n",
    "    'max_iter' : np.logspace(4, 7, 10, dtype=int)\n",
    "}\n",
    "# Grid search cross-validation\n",
    "clf = GridSearchCV(estimator=model,\n",
    "                   param_grid=param_grid,\n",
    "                   cv=cv,\n",
    "                   n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf.fit(X_train, y_train).best_estimator_\n",
    "print('Best model : {0}'.format(best_model))\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(11,11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.plot([0, 1], [0, 1],\n",
    "          color=cm.magma(0.93), lw=4, ls='--', zorder=3, alpha=0.5)\n",
    "\n",
    "sc = 12\n",
    "axes.scatter(y_test, y_pred,\n",
    "             color=cm.magma(0.5), s=sc**2, ec='black', alpha=0.4)\n",
    "\n",
    "axes.set_xlim(0, 1)\n",
    "axes.set_ylim(0, 1)\n",
    "\n",
    "axes.set_title('MAE : {0:.4f}'.format(np.mean(np.abs(y_test - y_pred))), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$\\mathrm{y_{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$\\mathrm{y_{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', rotation=20, labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 5. Predictions made with the optimized\\n5-fold Lasso regression.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the results in 2./d.\n",
    "\n",
    "The grid search always returned the a very small (almost always thes smallest) alpha value in the analysis above. This wasn't actually an error, but the indication, that a linear regression can be efficiently used in case of our dataset. ($\\alpha \\to 0$ is equivalent to the linear regression in the case of the Lasso regression.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shrinkage\n",
    "\n",
    "* interpret Lasso model's findings based on its descriptive parameters by the shrinkage method described during the lecture (make a plot and check the names of the features that are not eliminated by the penalty parameter) on the data we have here (this is an explanatory data analysis problem, be to be creative)\n",
    "\n",
    "* fit Ridge model and apply the shrinkage method as well, did you get what you expect?\n",
    "\n",
    "* do you think normalization is needed here? If so, do not forget!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3./a. Lasso evaluation with shrinkage method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the analysis in task 2. returned, that normalization is not needed for this dataset. Thus, I'll won't normalize any data here, that's my answer on point 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lasso(X, y, alpha=1.0, max_iter=1e5):\n",
    "    \n",
    "    model = Lasso(alpha=alpha, max_iter=max_iter, random_state=None)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_alphas = []\n",
    "lasso_coeffs = []\n",
    "\n",
    "for a in np.logspace(-10, 1, 100):\n",
    "    lasso_alphas.append(a)\n",
    "    model = evaluate_lasso(X_train, y_train, alpha=a, max_iter=int(1e5))\n",
    "    lasso_coeffs.append(model.coef_)\n",
    "    \n",
    "lasso_alphas = np.array(lasso_alphas)\n",
    "lasso_coeffs = np.array(lasso_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*11,nrows*11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_xlim(np.log(lasso_alphas.min()), np.log(lasso_alphas.max()))\n",
    "ax.set_title('Full test range', fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-8,-2)\n",
    "ax.set_ylim(-0.35, 0.35)\n",
    "ax.set_title('Zoomed on interesting area', fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(np.log(lasso_alphas), lasso_coeffs,\n",
    "          lw=3, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel('$\\log \\\\left( \\\\alpha \\\\right)$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('Value of coefficients', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', rotation=20, labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 6. Shrinkage method used on the results of Lasso regression.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around $\\log \\left( \\alpha \\right)\\approx-5$ is where mostly the interesting events happen. That's the place, where a lot of coefficients diverges away from 0, while other coefficients vanish. Two other coefficients does the same, but with much a much smaller extent around $\\log \\left( \\alpha \\right) \\approx -12$, before vanishing quickly. Let's see which features are responsible for this last anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(35,35))\n",
    "\n",
    "axes.imshow(lasso_coeffs.T, aspect=0.8)\n",
    "\n",
    "axes.set_xticks([])\n",
    "axes.set_xticklabels([])\n",
    "\n",
    "axes.set_yticks([i for i in range(len(df_n.columns[:-1]))])\n",
    "axes.set_yticklabels(df_n.columns.tolist()[:-1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the coefficients with positive impact, the winner turned out to be the `PctIlleg` feature, followed by the `MalePctDivorce`, `HouseVacant`, `racePctBlack` and `PctPersDenseHous`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3./b. Ridge evaluation with shrinkage method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ridge(X, y, alpha=1.0, max_iter=1e5):\n",
    "    \n",
    "    model = Ridge(alpha=alpha, max_iter=max_iter, random_state=None)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_alphas = []\n",
    "ridge_coeffs = []\n",
    "\n",
    "for a in np.logspace(-10, 10, 100):\n",
    "    ridge_alphas.append(a)\n",
    "    model = evaluate_ridge(X_train, y_train, alpha=a, max_iter=int(1e5))\n",
    "    ridge_coeffs.append(model.coef_)\n",
    "    \n",
    "ridge_alphas = np.array(ridge_alphas)\n",
    "ridge_coeffs = np.array(ridge_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*11,nrows*11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_xlim(np.log(ridge_alphas.min()), np.log(ridge_alphas.max()))\n",
    "ax.set_title('Full test range', fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-0.35, 0.35)\n",
    "ax.set_title('Zoomed on interesting area', fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(np.log(ridge_alphas), ridge_coeffs,\n",
    "          lw=3, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel('$\\log \\\\left( \\\\alpha \\\\right)$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('Value of coefficients', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', rotation=20, labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 7. Shrinkage method used on the results of Ridge regression.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "That's exactly what I've waited for. If we read the documentation of `sklearn`'s `Lasso()` and `Ridge()` subroutines, we find, that `sklearn.linear_model.Lasso()` operates with `l1` regularization, while `sklearn.linear_model.Ridge()` does `l2`. For the `l2` regularization we want to get exactly these very smooth curves and decays, while `l1` tends to produce much more like sloppy/lumpy/zigzagy curves. Just like those, which we see above on Fig.6 and Fig.7.\n",
    "\n",
    "Let's se the matrix plot above for Ridge too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(35,35))\n",
    "\n",
    "axes.imshow(ridge_coeffs.T, aspect=0.8)\n",
    "\n",
    "axes.set_xticks([])\n",
    "axes.set_xticklabels([])\n",
    "\n",
    "axes.set_yticks([i for i in range(len(df_n.columns[:-1]))])\n",
    "axes.set_yticklabels(df_n.columns.tolist()[:-1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's almost impossible to see and analyze as easily, as one can do it with the Lasso results. I won't even try for now. Anyways, in later tasks, we'll get the same impactful features as result too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subset selection\n",
    "\n",
    "* Split the data to a training and test set and do recursive feature elimination until 10 remaining predictors with 5-fold cross-validated regressors (`RidgeCV`, `LassoCV`, `ElasticNetCV`) on the training set, plot their names and look up some of their meanings (recursive feature elimination is part of `sklearn` but you can do it with a for loop if you whish).\n",
    "\n",
    "* Do all models provide the same descriptors? Check their performance on the test set! Plot all model predictions compared to the `y_test` on 3 different plots, which model seems to be the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources\n",
    "\n",
    "- Helpful code snippets from : https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./a. RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold search is needed\n",
    "folds = 5\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# Create RidgeCV model\n",
    "ridge_cv = RidgeCV(alphas=np.logspace(-10, 1, 400), cv=cv)\n",
    "# Create an RFE selection\n",
    "rfe = RFE(estimator=ridge_cv, n_features_to_select=20, step=1)\n",
    "# Fit the selection with the 5-fold Ridge C-V as an estimator\n",
    "rfe_ridge_model = rfe.fit(X_train, y_train)\n",
    "# Get predictions\n",
    "ridge_prediction = rfe_ridge_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected features\n",
    "features_sel = df_n.columns[:-1][rfe_ridge_model.support_]\n",
    "coef_sel = rfe_ridge_model.estimator_.coef_\n",
    "# Sort them by coef_\n",
    "coef_sel, features_sel = zip(*sorted(zip(coef_sel, features_sel), reverse=True))\n",
    "# Calculate MAE and RMS\n",
    "ridge_mae = np.mean(np.abs(y_test - ridge_prediction))\n",
    "ridge_rms = np.sqrt(np.mean((y_test - ridge_prediction)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ridge Regression MAE: {0:.4f}'.format(ridge_mae))\n",
    "print('Ridge Regression RMS: {0:.4f}'.format(ridge_rms))\n",
    "print('Most prominent features:\\n'+\n",
    "      '---------------------------')\n",
    "[print('{0} : {1:.4f}'.format(f, c)) for (f, c) in zip(features_sel, coef_sel)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(11,11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.plot([0, 1], [0, 1],\n",
    "          color=cm.magma(0.93), lw=4, ls='--', zorder=3, alpha=0.5)\n",
    "\n",
    "sc = 12\n",
    "axes.scatter(y_test, ridge_prediction,\n",
    "             color=cm.magma(0.5), s=sc**2, ec='black', alpha=0.4)\n",
    "\n",
    "axes.set_xlim(0, 1)\n",
    "axes.set_ylim(0, 1)\n",
    "\n",
    "axes.set_title('MAE = {0:.4f}  |  RMS = {1:.4f}'.format(ridge_mae, ridge_rms),\n",
    "               fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$\\mathrm{y_{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$\\mathrm{y_{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', rotation=20, labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 8. Predictions made with the optimized\\n5-fold Ridge regression with recursive\\nfeature selection.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./b. LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold search is needed\n",
    "folds = 5\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# Create LassoCV model\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-10, 1, 400), max_iter=10000, cv=cv, n_jobs=-1)\n",
    "# Create an RFE selection\n",
    "rfe = RFE(estimator=lasso_cv, n_features_to_select=10, step=1)\n",
    "# Fit the selection with the 5-fold Lasso C-V as an estimator\n",
    "rfe_lasso_model = rfe.fit(X_train, y_train)\n",
    "# Get predictions\n",
    "lasso_prediction = rfe_lasso_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected features\n",
    "features_sel = df_n.columns[:-1][rfe_lasso_model.support_]\n",
    "coef_sel = rfe_lasso_model.estimator_.coef_\n",
    "# Sort them by coef_\n",
    "coef_sel, features_sel = zip(*sorted(zip(coef_sel, features_sel), reverse=True))\n",
    "# Calculate MAE and RMS\n",
    "lasso_mae = np.mean(np.abs(y_test - lasso_prediction))\n",
    "lasso_rms = np.sqrt(np.mean((y_test - lasso_prediction)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lasso Regression MAE: {0:.4f}'.format(lasso_mae))\n",
    "print('Lasso Regression RMS: {0:.4f}'.format(lasso_rms))\n",
    "print('Most prominent features:\\n'+\n",
    "      '---------------------------')\n",
    "[print('{0} : {1:.4f}'.format(f, c)) for (f, c) in zip(features_sel, coef_sel)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(15,10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "colors = [cm.magma(0.4 + 0.6/folds * i) for i in range(folds)]\n",
    "for i in range(folds):\n",
    "    axes.plot(np.log(rfe_lasso_model.estimator_.alphas),\n",
    "              rfe_lasso_model.estimator_.mse_path_[:, i],\n",
    "              label='Fold {0}'.format(i+1),\n",
    "              color=colors[i], lw=6, alpha=0.6)\n",
    "\n",
    "axes.set_xlabel('$\\log \\\\left( \\\\alpha \\\\right)$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('MAE path', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', rotation=20, labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='upper right', fontsize=axislegendsize)\n",
    "\n",
    "fig.suptitle('Fig. 9. MAE values for different $\\\\alpha$ parameters during the\\n'+\n",
    "             'optimization of the 5-fold Lasso regression with recursive\\n'+\n",
    "             'feature selection.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(11,11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.plot([0, 1], [0, 1],\n",
    "          color=cm.magma(0.93), lw=4, ls='--', zorder=3, alpha=0.5)\n",
    "\n",
    "sc = 12\n",
    "axes.scatter(y_test, lasso_prediction,\n",
    "             color=cm.magma(0.5), s=sc**2, ec='black', alpha=0.4)\n",
    "\n",
    "axes.set_xlim(0, 1)\n",
    "axes.set_ylim(0, 1)\n",
    "\n",
    "axes.set_title('MAE = {0:.4f}  |  RMS = {1:.4f}'.format(lasso_mae, lasso_rms),\n",
    "               fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$\\mathrm{y_{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$\\mathrm{y_{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', rotation=20, labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 10. Predictions made with the optimized\\n5-fold Lasso regression with recursive\\nfeature selection.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./c. ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold search is needed\n",
    "folds = 5\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# Create ElasticNetCV model\n",
    "elastic_net_cv = ElasticNetCV(alphas=np.logspace(-10, 1, 400), max_iter=10000, cv=cv, n_jobs=-1)\n",
    "# Create an RFE selection\n",
    "rfe = RFE(estimator=elastic_net_cv, n_features_to_select=10, step=1)\n",
    "# Fit the selection with the 5-fold ElasticNet C-V as an estimator\n",
    "rfe_elastic_net_model = rfe.fit(X_train, y_train)\n",
    "# Get predictions\n",
    "elastic_net_prediction = rfe_elastic_net_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected features\n",
    "features_sel = df_n.columns[:-1][rfe_elastic_net_model.support_]\n",
    "coef_sel = rfe_elastic_net_model.estimator_.coef_\n",
    "# Sort them by coef_\n",
    "coef_sel, features_sel = zip(*sorted(zip(coef_sel, features_sel), reverse=True))\n",
    "# Calculate MAE and RMS\n",
    "elastic_net_mae = np.mean(np.abs(y_test - elastic_net_prediction))\n",
    "elastic_net_rms = np.sqrt(np.mean((y_test - elastic_net_prediction)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ElasticNet Regression MAE: {0:.4f}'.format(elastic_net_mae))\n",
    "print('ElasticNet Regression RMS: {0:.4f}'.format(elastic_net_rms))\n",
    "print('Most prominent features:\\n'+\n",
    "      '---------------------------')\n",
    "[print('{0} : {1:.4f}'.format(f, c)) for (f, c) in zip(features_sel, coef_sel)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(15,10),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "colors = [cm.magma(0.4 + 0.6/folds * i) for i in range(folds)]\n",
    "for i in range(folds):\n",
    "    axes.plot(np.log(rfe_elastic_net_model.estimator_.alphas),\n",
    "              rfe_elastic_net_model.estimator_.mse_path_[:, i],\n",
    "              label='Fold {0}'.format(i+1),\n",
    "              color=colors[i], lw=6, alpha=0.6)\n",
    "\n",
    "axes.set_xlabel('$\\log \\\\left( \\\\alpha \\\\right)$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('MAE path', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', rotation=20, labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='upper right', fontsize=axislegendsize)\n",
    "\n",
    "fig.suptitle('Fig. 11. MAE values for different $\\\\alpha$ parameters during the\\n'+\n",
    "             'optimization of the 5-fold ElasticNet regression with recursive\\n'+\n",
    "             'feature selection.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(11,11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.plot([0, 1], [0, 1],\n",
    "          color=cm.magma(0.93), lw=4, ls='--', zorder=3, alpha=0.5)\n",
    "\n",
    "sc = 12\n",
    "axes.scatter(y_test, elastic_net_prediction,\n",
    "             color=cm.magma(0.5), s=sc**2, ec='black', alpha=0.4)\n",
    "\n",
    "axes.set_xlim(0, 1)\n",
    "axes.set_ylim(0, 1)\n",
    "\n",
    "axes.set_title('MAE = {0:.4f}  |  RMS = {1:.4f}'.format(elastic_net_mae, elastic_net_rms),\n",
    "               fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "axes.set_xlabel('$\\mathrm{y_{groundtruth}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('$\\mathrm{y_{pred}}$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', rotation=20, labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 12. Predictions made with the optimized\\n5-fold ElasticNet regression with recursive\\nfeature selection.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The best model seems to be Ridge, based on both the MAE and RMS scores of their predictions on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ElasticNet penalty surface\n",
    "\n",
    "* visualize the surface of the $\\mathrm{objective}(\\alpha, \\beta) $ parameters corresponding to the L1 and L2 regularizations. Select the best possible combination of the hyper-parameters that minimize the objective (clue: `from scipy.optimize import minimize`)\n",
    "    * this task is similar to what you've seen during class, just not for MSE vs. single penalty parameter but MSE vs. two penalty parameters $\\alpha, \\beta$\n",
    "\n",
    "* interpret the findings! do you think linear models are powerful enough on this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./a. Explore ElasticNet with `l1` and `l2` regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def evaluate_elastic_net(X, y, alpha=1.0, l1_ratio=0.5, normalize=False, max_iter=1e5):\n",
    "    \"\"\"\n",
    "    Evaluate an ElasticNet estimator with specific parameters. The goal of this function is to\n",
    "    discover the penalty surface of the ElasticNet for `l1` and `l2` regularizations.\n",
    "    \n",
    "    `l1` and `l2` can be selected in `sklearn.linear_model.ElasticNet` with the `l1_ratio` arg\n",
    "    * l1_ratio = 0 : l2\n",
    "    * l1_ratio = 1 : l1\n",
    "    \"\"\"\n",
    "    \n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, normalize=normalize, max_iter=max_iter)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `l1` regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net_l1_alphas = []\n",
    "elastic_net_l1_coeffs = []\n",
    "\n",
    "for a in np.logspace(-10, 0, 100):\n",
    "    elastic_net_l1_alphas.append(a)\n",
    "    model = evaluate_elastic_net(X_train, y_train, alpha=a, l1_ratio=1, normalize=False, max_iter=1e5)\n",
    "    elastic_net_l1_coeffs.append(model.coef_)\n",
    "    \n",
    "elastic_net_l1_alphas = np.array(elastic_net_l1_alphas)\n",
    "elastic_net_l1_coeffs = np.array(elastic_net_l1_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*11,nrows*11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_xlim(np.log(elastic_net_l1_alphas.min()), np.log(elastic_net_l1_alphas.max()))\n",
    "ax.set_title('Full test range', fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-8, -2)\n",
    "ax.set_ylim(-0.35, 0.35)\n",
    "ax.set_title('Zoomed on interesting area', fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(np.log(elastic_net_l1_alphas), elastic_net_l1_coeffs,\n",
    "          lw=3, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel('$\\log \\\\left( \\\\alpha \\\\right)$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('Value of coefficients', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', rotation=20, labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 13. Shrinkage method used on the results of ElasticNet regression with l1 regularization.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "The results are very similar to what we've got from the Ridge regression in task 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `l2` regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net_l2_alphas = []\n",
    "elastic_net_l2_coeffs = []\n",
    "\n",
    "for a in np.logspace(-10, 0, 100):\n",
    "    elastic_net_l2_alphas.append(a)\n",
    "    model = evaluate_elastic_net(X_train, y_train, alpha=a, l1_ratio=0, normalize=False, max_iter=1e5)\n",
    "    elastic_net_l2_coeffs.append(model.coef_)\n",
    "\n",
    "elastic_net_l2_alphas = np.array(elastic_net_l2_alphas)\n",
    "elastic_net_l2_coeffs = np.array(elastic_net_l2_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*11,nrows*11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_xlim(np.log(elastic_net_l2_alphas.min()), np.log(elastic_net_l2_alphas.max()))\n",
    "ax.set_title('Full test range', fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-8, -2)\n",
    "ax.set_ylim(-0.35, 0.35)\n",
    "ax.set_title('Zoomed on interesting area', fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(np.log(elastic_net_l2_alphas), elastic_net_l2_coeffs,\n",
    "          lw=3, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel('$\\log \\\\left( \\\\alpha \\\\right)$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('Value of coefficients', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', rotation=20, labelsize=axisticksize, colors='white')\n",
    "\n",
    "fig.suptitle('Fig. 14. Shrinkage method used on the results of ElasticNet regression with l2 regularization.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.03)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./b. Visualize the optimal penalty surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net_mix_alphas = []\n",
    "elastic_net_mix_coeffs = []\n",
    "\n",
    "for a in np.logspace(-10, 10, 100):\n",
    "    elastic_net_mix_alphas.append(a)\n",
    "    model = evaluate_elastic_net(X_train, y_train, alpha=a, l1_ratio=0.5, normalize=False, max_iter=1e5)\n",
    "    elastic_net_mix_coeffs.append(model.coef_)\n",
    "\n",
    "elastic_net_mix_alphas = np.array(elastic_net_mix_alphas)\n",
    "elastic_net_mix_coeffs = np.array(elastic_net_mix_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_ratio = a + b\n",
    "# alpha = a / (a + b)\n",
    "# ==> a = alpha * l1_ratio\n",
    "# ==> b = l1_ratio - a\n",
    "a = elastic_net_alphas * 0.5\n",
    "b = l1_ratio - a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(a, b)\n",
    "Z = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints:\n",
    " - On total you can get 10 points for fully completing all tasks.\n",
    " - Decorate your notebook with, questions, explanation etc, make it self contained and understandable!\n",
    " - Comments you code when necessary\n",
    " - Write functions for repetitive tasks!\n",
    " - Use the pandas package for data loading and handling\n",
    " - Use matplotlib and seaborn for plotting or bokeh and plotly for interactive investigation\n",
    " - Use the scikit learn package for almost everything\n",
    " - Use for loops only if it is really necessary!\n",
    " - Code sharing is not allowed between student! Sharing code will result in zero points.\n",
    " - If you use code found on web, it is OK, but, make its source clear! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
