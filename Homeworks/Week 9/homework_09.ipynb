{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected neural networks\n",
    "\n",
    "<u>On kooplex tensorflow 2 does not work, please work on Google Colab and then upload your solution to kooplex! Also, on Google Colab you can use GPUs</u>\n",
    "\n",
    "This week we will use the MNIST handwritten digits dataset! The weights.npy file is provided, which contains the weight vector for a traines fully connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from numba import jit\n",
    "from numba import int32, float64\n",
    "from numba.experimental import jitclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/'\n",
    "out = './out/'\n",
    "\n",
    "# Bold print for Jupyter Notebook\n",
    "b1 = '\\033[1m'\n",
    "b0 = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some matplotlib and seaborn parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axistitlesize = 20\n",
    "axisticksize = 17\n",
    "axislabelsize = 26\n",
    "axislegendsize = 23\n",
    "axistextsize = 20\n",
    "axiscbarfontsize = 15\n",
    "\n",
    "# Set axtick dimensions\n",
    "major_size = 6\n",
    "major_width = 1.2\n",
    "minor_size = 3\n",
    "minor_width = 1\n",
    "mpl.rcParams['xtick.major.size'] = major_size\n",
    "mpl.rcParams['xtick.major.width'] = major_width\n",
    "mpl.rcParams['xtick.minor.size'] = minor_size\n",
    "mpl.rcParams['xtick.minor.width'] = minor_width\n",
    "mpl.rcParams['ytick.major.size'] = major_size\n",
    "mpl.rcParams['ytick.major.width'] = major_width\n",
    "mpl.rcParams['ytick.minor.size'] = minor_size\n",
    "mpl.rcParams['ytick.minor.width'] = minor_width\n",
    "\n",
    "mpl.rcParams.update({'figure.autolayout': False})\n",
    "\n",
    "# Seaborn style settings\n",
    "sns.set_style({'axes.axisbelow': True,\n",
    "               'axes.edgecolor': '.8',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': True,\n",
    "               'axes.labelcolor': '.15',\n",
    "               'axes.spines.bottom': True,\n",
    "               'axes.spines.left': True,\n",
    "               'axes.spines.right': True,\n",
    "               'axes.spines.top': True,\n",
    "               'figure.facecolor': 'white',\n",
    "               'font.family': ['sans-serif'],\n",
    "               'font.sans-serif': ['Arial',\n",
    "                'DejaVu Sans',\n",
    "                'Liberation Sans',\n",
    "                'Bitstream Vera Sans',\n",
    "                'sans-serif'],\n",
    "               'grid.color': '.8',\n",
    "               'grid.linestyle': '--',\n",
    "               'image.cmap': 'rocket',\n",
    "               'lines.solid_capstyle': 'round',\n",
    "               'patch.edgecolor': 'w',\n",
    "               'patch.force_edgecolor': True,\n",
    "               'text.color': '.15',\n",
    "               'xtick.bottom': True,\n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'in',\n",
    "               'xtick.top': True,\n",
    "               'ytick.color': '.15',\n",
    "               'ytick.direction': 'in',\n",
    "               'ytick.left': True,\n",
    "               'ytick.right': True})\n",
    "\n",
    "# Colorpalettes, colormaps, etc.\n",
    "sns.set_palette(palette='rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 2. Implement fully connected neural network via using only numpy\n",
    "\n",
    "In this task we need to implement a small fully connected neural network that can generate predictions for us if we provide the weights and the input data!\n",
    "\n",
    " - implement the following function:\n",
    " ```python\n",
    " def pred_nn(weights, x_test):\n",
    "    ...\n",
    "    return predictions\n",
    " ```\n",
    " - x_test has a shape of (N_samples, 784)\n",
    " - predictions has a shape of (N_samples, 10)\n",
    " - then function implements a fully connected neural network with the follwing layers:\n",
    "    - 750 neuron, relu activation\n",
    "    - 500 neuron, relu activation\n",
    "    - 500 neuron, relu activation\n",
    "    - 10 neuron, softmax activation\n",
    " - weights is a numpy array of the weights\n",
    "    - 1st element is a shape of (784, 750), 2nd is (750,), the bias\n",
    "    - 3rd element is a shape of (750, 500), 4th is (500,)\n",
    "    ... the rest matches the weight dimensions of the above-mentioned architecture\n",
    " - use numpy's built-in vectorized operations, try not to write for loops!\n",
    "    \n",
    "An optimally implemented function runs < 1s for N_samples = 10.000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1./a. Implement an FC neural network from scratch using NumPy for fun\n",
    "\n",
    "A very nice tutorial can be found [here](https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65), which summarizes every component of an FC neural network and gives a possible implementation of an FC neural network. This neural net in its core is based on that article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "This FC NN is made to learn both binary or multiclass datasets. Its structure copies mostly how the `Sequential()` networks work in `TF/keras`, so FC and activation layers can be attached to the network at ease and at will.\n",
    "\n",
    "**However.**\n",
    "\n",
    "For multiclass data like MNIST, a \"meaningful\" or \"competent\" loss function should be chosen, eg. categorical cross-entropy or something similar. I had just a day for this notebook, so I couldn't finish the implementation of an appropriate loss function. This network is therefore useless for training in its current form, but it can still give predictions using pre-defined weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a template layer for FC and activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer():\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.Z = None\n",
    "\n",
    "    # Computes the output Z of a layer for a given input X, W and B\n",
    "    def forward_propagation(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Computes dE/dX for a given dE/dZ (and update parameters if any)\n",
    "    def backward_propagation(self, Z_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the activation layer with different options for activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TANH\n",
    "@jit(nopython=True)\n",
    "def _tanh(X):\n",
    "    return np.tanh(X)\n",
    "@jit(nopython=True)\n",
    "def _tanh_prime(X):\n",
    "    return 1 - _tanh(X)**2\n",
    "\n",
    "# RELU\n",
    "@jit(nopython=True)\n",
    "def _relu(X):\n",
    "    return X * (X > 0)\n",
    "@jit(nopython=True)\n",
    "def _relu_prime(X):\n",
    "    return np.ones_like(X) * (X > 0)\n",
    "\n",
    "# SIGMOID\n",
    "@jit(nopython=True)\n",
    "def _sigm(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "@jit(nopython=True)\n",
    "def _sigm_prime(X):\n",
    "    return self.sigm(X) * (1 - self.sigm(X))\n",
    "\n",
    "# SOFTMAX\n",
    "@jit(nopython=True)\n",
    "def _soft(X):\n",
    "    shiftx = X - np.max(X)\n",
    "    return np.exp(shiftx) / np.sum(np.exp(shiftx))\n",
    "@jit(nopython=True)\n",
    "def _soft_prime(X):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray of shape (input_size, output_size)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_error : numpy.ndarray (input_size, output_size)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Nice tutorials at:\n",
    "        `https://themaverickmeerkat.com/2019-10-23-Softmax/`\n",
    "        `https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/`\n",
    "        `https://aerinykim.medium.com/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d`\n",
    "\n",
    "    \"\"\"\n",
    "    return _soft(X) * (1 - _soft(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ACTIVATIONS = ['tanh', 'relu', 'sigm', 'soft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activations():\n",
    "    \"\"\"\n",
    "    Defines activation functions and their derivatives.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : str\n",
    "        Chooses the activation method. Possible choices:\n",
    "        - 'tanh' : Hyperbolic tangent\n",
    "        - 'relu' : ReLU\n",
    "        - 'sigm' : Sigmoid\n",
    "        - 'soft' : Softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, a):\n",
    "        assert a.lower() in _ACTIVATIONS,\\\n",
    "            \"Possible activations are {0}!\".format(_ACTIVATIONS)\n",
    "        if a.lower() == 'tanh' : self.a = [_tanh,\n",
    "                                             _tanh_prime]\n",
    "        elif a.lower() == 'relu' : self.a = [_relu,\n",
    "                                               _relu_prime]\n",
    "        elif a.lower() == 'sigm' : self.a = [_sigm,\n",
    "                                               _sigm_prime]\n",
    "        elif a.lower() == 'soft' : self.a = [_soft,\n",
    "                                               _soft_prime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_layer(layer):\n",
    "    \"\"\"\n",
    "    Implements an activation step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : str\n",
    "        Chooses the activation method. Possible choices:\n",
    "        - 'tanh' : Hyperbolic tangent\n",
    "        - 'relu' : ReLU\n",
    "        - 'sigm' : Sigmoid\n",
    "    \"\"\"\n",
    "    def __init__(self, a='relu'):\n",
    "        self.a = a\n",
    "        activation = activations(a)\n",
    "        self.activation = activation.a[0]\n",
    "        self.activation_prime = activation.a[1]\n",
    "\n",
    "    # Returns the activated input\n",
    "    def forward_propagation(self, X, **kwargs):\n",
    "        self.X = X\n",
    "        self.Z = self.activation(self.X)\n",
    "        return self.Z\n",
    "\n",
    "    # Returns X_error dE/dX for a given Z_error dE/dZ.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, Z_error, learning_rate):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z_error : numpy.ndarray of shape (output_size, )\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_error : numpy.ndarray of shape (output_size, input_size)\n",
    "        \"\"\"\n",
    "        return Z_error * self.activation_prime(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement a scoring class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def _catcross(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : numpy.ndarray of shape (n_samples, )\n",
    "        Groundtruth labels.\n",
    "    y_pred : numpy.ndarray of shape (n_samples, )\n",
    "        Predicted labels.\n",
    "    \"\"\"\n",
    "    return -np.sum(y_true * np.log(_soft(y_pred)+1e-09))\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _catcross_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : numpy.ndarray of shape (n_samples, )\n",
    "        Groundtruth labels.\n",
    "    y_pred : numpy.ndarray of shape (n_samples, )\n",
    "        Predicted labels.\n",
    "    \"\"\"\n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LOSSES = ['catcross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss():\n",
    "    \"\"\"\n",
    "    Defines loss functions and their derivatives.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : str\n",
    "        Chooses the loss function. Possible choices:\n",
    "        - 'mse' : Mean Squared Error\n",
    "    \"\"\"\n",
    "    def __init__(self, s='mse'):\n",
    "        assert s.lower() in _LOSSES,\\\n",
    "            \"Possible loss scores are {0}!\".format(_LOSSES)\n",
    "        if s.lower() == 'catcross' : self.l = [_catcross,\n",
    "                                               _catcross_prime]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement a template for a general FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_layer():\n",
    "    \"\"\"\n",
    "    Implements an FC layer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : int\n",
    "        The number of input neurons.\n",
    "    output_size : int\n",
    "        The number of output neurons.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Initializes random weights at the start for the FC layer.\n",
    "        \n",
    "        W : numpy.ndarray of shape (input_size, output_size)\n",
    "            Weights of the layer\n",
    "        B : numpy.ndarray of shape (output_size, )\n",
    "            Biases of the layer\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.W = np.random.rand(self.input_size, self.output_size) - 0.5\n",
    "        self.B = np.random.rand(1, self.output_size) - 0.5\n",
    "\n",
    "    def forward_propagation(self, X, **kwargs):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation step for the current\n",
    "        FC layer. Returns output for a given input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray of shape (input_size, output_size)\n",
    "            Input data of the forward propagataion of the FC layer.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Z : numpy.ndarray of shape(output_size, )\n",
    "            Output of the forward propagation of the FC layer.\n",
    "        \"\"\"\n",
    "        if 'w' in kwargs : self.W = kwargs['w']\n",
    "        if 'b' in kwargs : self.B = kwargs['b']\n",
    "        self.X = X.reshape(1, -1) if X.ndim == 1 else X\n",
    "        self.Z = np.dot(self.X, self.W) + self.B\n",
    "        return self.Z\n",
    "\n",
    "    def backward_propagation(self, Z_error, learning_rate):\n",
    "        \"\"\"\n",
    "        Implements the backward propagation step for the current\n",
    "        FC layer. Computes dE/dW, dE/dB for a given output_error\n",
    "        dE/dZ. Returns input_error dE/dX.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z_error : numpy.ndarray of shape (output_size, )\n",
    "            Output error of the FC layer.\n",
    "        learning_rate : float\n",
    "            Learning rate for the parameter convergence.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X_error : numpy.ndarray of shape (output_size, input_size)\n",
    "            Input error of the FC layer.\n",
    "        \"\"\"\n",
    "        X_error = np.dot(Z_error, self.W.T)\n",
    "        W_error = np.dot(self.X.T, Z_error)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W -= learning_rate * W_error\n",
    "        self.B -= learning_rate * Z_error\n",
    "        return X_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create network class to build arbitrary networks with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_nn():\n",
    "    \"\"\"\n",
    "    Implements an FC neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, s='mse'):\n",
    "        # Layers\n",
    "        self.layers = []\n",
    "        self._fitted = False\n",
    "\n",
    "    # Add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def config(self, s='catcross'):\n",
    "        # Loss function\n",
    "        self.s = s\n",
    "        loss = Loss(s)\n",
    "        self.loss = loss.l[0]\n",
    "        self.loss_prime = loss.l[1]\n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "        \"\"\"\n",
    "        Make predictions using the weights and biases of\n",
    "        a fitted model. This function is the task to do in this\n",
    "        assignment.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray of shape (input_size, output_size)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for Z in X:\n",
    "            # Forward propagation\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                params = {}\n",
    "                if 'W' in kwargs : params['w'] = kwargs['W'][i//2]\n",
    "                if 'B' in kwargs : params['b'] = kwargs['B'][i//2]\n",
    "                Z = layer.forward_propagation(X=Z, **params)\n",
    "            result.append(Z)\n",
    "        return np.array(result)\n",
    "\n",
    "    # Train the network\n",
    "    def fit(self, X_train, y_train, epochs, learning_rate=0.01):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        for i in range(epochs):\n",
    "            # Number of train data\n",
    "            n_samples = len(X_train)            \n",
    "            \n",
    "            # I'm training without batches here\n",
    "            pbar = tqdm(range(n_samples))   # \n",
    "            err = 0                         # Log accuracy\n",
    "            for j in pbar:\n",
    "                Z = X_train[j]\n",
    "                y_true = y_train[j]\n",
    "                # I. Forward propagation\n",
    "                for layer in self.layers:\n",
    "                    Z = layer.forward_propagation(Z)\n",
    "\n",
    "                # Compute loss\n",
    "                err += self.loss(y_true, Z)\n",
    "                \n",
    "                # II. Backward propagation\n",
    "                Z_error = self.loss_prime(y_true, Z)\n",
    "                for layer in reversed(self.layers):\n",
    "                    Z_error = layer.backward_propagation(Z_error, learning_rate)\n",
    "\n",
    "                # calculate average error on all samples\n",
    "                pbar.set_description('Epoch {0}/{1}  loss : {2:.3f}'.format(i+1, epochs, err/n_samples))\n",
    "        # Fitting finished\n",
    "        self._fitted = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data and predict the network with the given weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make classification\n",
    "X, y = make_classification(n_samples=10000, n_features=784, n_informative=40, n_classes=10)\n",
    "# Create one-hot encoded y dataset\n",
    "b = np.zeros((y.size, y.max()+1))\n",
    "b[np.arange(y.size),y] = 1\n",
    "y = b\n",
    "# Create Train/Test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network\n",
    "model = fc_nn()\n",
    "model.add(fc_layer(784, 750))\n",
    "model.add(activation_layer(a='relu'))\n",
    "model.add(fc_layer(750, 500))\n",
    "model.add(activation_layer(a='relu'))\n",
    "model.add(fc_layer(500, 500))\n",
    "model.add(activation_layer(a='relu'))\n",
    "model.add(fc_layer(500, 10))\n",
    "model.add(activation_layer(a='soft'))\n",
    "\n",
    "model.config(s='catcross')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=5, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_2_normal(X):\n",
    "    \"\"\"\n",
    "    Convert one-hot encoded matrix to 1D array of label indeces.\n",
    "    \"\"\"\n",
    "    return np.array([np.where(p == 1)[0][0]+1 for p in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of model prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test : array-like of shape (N, )\n",
    "        Original labels of the test dataset.\n",
    "    \n",
    "    y_pred : array-like of shape (N, )\n",
    "        Predicted labels of the test dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Accuracy of model in reference of the true test labels.\n",
    "    \"\"\"\n",
    "    # Binarize labels\n",
    "    y_test = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    y_pred = label_binarize(y_pred, classes=np.unique(y_pred))\n",
    "\n",
    "    correct = 0\n",
    "    for (t, p) in zip(y_test, y_pred):\n",
    "        if t.tolist() == p.tolist():\n",
    "            correct += 1\n",
    "    return correct / len(y_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_mat, y, labels=None, title=None):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(figsize=(14,14))\n",
    "    axes.set_aspect('equal')\n",
    "\n",
    "    im = axes.imshow(conf_mat)\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for X in range(conf_mat.shape[0]):\n",
    "        for Y in range(conf_mat.shape[1]):\n",
    "            axes.text(Y, X, conf_mat[X, Y], fontsize=30,\n",
    "                      ha='center', va='center', color='white', fontweight='bold', \n",
    "                      bbox=dict(color=np.array((0,0,0,0.2)), lw=0)\n",
    "                     )\n",
    "\n",
    "    # Set axis tick locations and labels\n",
    "    ticks = [i for i in range(len(set(y)))]\n",
    "    if labels is None:\n",
    "        ticklabels = [i+1 for i in range(len(set(y)))]\n",
    "    else:\n",
    "        ticklabels = list(labels)\n",
    "\n",
    "    axes.set_xticks(ticks)\n",
    "    axes.set_xticklabels(ticklabels)\n",
    "    axes.set_yticks(ticks)\n",
    "    axes.set_yticklabels(ticklabels)\n",
    "\n",
    "    axes.set_xlabel('Predicted labels', fontsize=axislabelsize+5, fontweight='bold')\n",
    "    axes.set_ylabel('True labels', fontsize=axislabelsize+5, fontweight='bold')\n",
    "    axes.tick_params(axis='both', which='major', labelsize=axisticksize+5)\n",
    "    axes.xaxis.tick_top()\n",
    "    axes.xaxis.set_label_position('top') \n",
    "\n",
    "    axes.grid(False)\n",
    "\n",
    "    # Create an axis on the right side of `axes`. The width of `cax` will be 5%\n",
    "    # of `axes` and the padding between `cax` and axes will be fixed at 0.1 inch\n",
    "    divider = make_axes_locatable(axes)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.1)\n",
    "    cbar = plt.colorbar(mappable=im, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=axiscbarfontsize, colors='black')\n",
    "    cbar.set_label('Number of occurences', fontsize=axiscbarfontsize+10, labelpad=15, rotation=90)\n",
    "\n",
    "    plt.suptitle(title,\n",
    "                 fontsize=axistitlesize+5, y=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).reshape(y_test.shape)\n",
    "for i in range(len(y_test)):\n",
    "    l = y_pred[i]\n",
    "    y_pred[i] = (l == np.max(l)) * np.ones_like(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded preds and tests to normal arrays\n",
    "y_test_n = onehot_2_normal(y_test)\n",
    "y_pred_n = onehot_2_normal(y_pred)\n",
    "\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test_n, y_pred=y_pred_n)\n",
    "conf_mat = confusion_matrix(y_test_n, y_pred_n, labels=[i for i in range(1,11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y_test_n, labels=[i for i in range(1,11)],\n",
    "                      title=('Fig. 1. Confusion matrix of the predictions\\n' +\n",
    "                             'on the test set of my own FC-NN model with training.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights to model during predictions (actual task)\n",
    "\n",
    "Obviously these weights and biases has no relation to my randomly generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.load(data + 'weights.npy', allow_pickle=True)\n",
    "for i in range(W.shape[0]):\n",
    "    print('Layer {0} shape: {1}'.format(i+1, W[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, W=W[::2], B=W[1::2]).reshape(y_test.shape)\n",
    "for i in range(len(y_test)):\n",
    "    l = y_pred[i]\n",
    "    y_pred[i] = (l == np.max(l)) * np.ones_like(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded preds and tests to normal arrays\n",
    "y_test_n = onehot_2_normal(y_test)\n",
    "y_pred_n = onehot_2_normal(y_pred)\n",
    "\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test_n, y_pred=y_pred_n)\n",
    "conf_mat = confusion_matrix(y_test_n, y_pred_n, labels=[i for i in range(1,11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y_test_n, labels=[i for i in range(1,11)],\n",
    "                      title=('Fig. 2. Confusion matrix of the predictions\\n' +\n",
    "                             'on the test set of my own FC-NN model with pre-def weights.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Same architecture via tensorflow/keras\n",
    "\n",
    " - Implement the same architecture with tensorflow/keras as we did in 1-2). \n",
    " - Load the provided weights for the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the network\n",
    "model = Sequential()\n",
    "# FC + activation layers\n",
    "model.add(Dense(750, input_dim=784, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "for i in range(len(y_test)):\n",
    "    l = y_pred[i]\n",
    "    y_pred[i] = (l == np.max(l)) * np.ones_like(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded preds and tests to normal arrays\n",
    "y_test_n = onehot_2_normal(y_test)\n",
    "y_pred_n = onehot_2_normal(y_pred)\n",
    "\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test_n, y_pred=y_pred_n)\n",
    "conf_mat = confusion_matrix(y_test_n, y_pred_n, labels=[i for i in range(1,11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y_test_n, labels=[i for i in range(1,11)],\n",
    "                      title=('Fig. 3. Confusion matrix of the predictions\\n' +\n",
    "                             'on the test set of keras\\'s FC-NN model.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-5. Compate performances\n",
    " - load the MNIST dataset from the tensorflow/keras built-in dataset\n",
    " - use the original train/test split!\n",
    " - divide each pixel's value by 255 & reshape to have 1D input vector (784) instead of the 2D matrix (28x28)\n",
    "   - eg for the test set you will have a (10000, 784) shaped vector\n",
    " - generate prediction for the 10.000 test images with both methods!\n",
    " - calculate the categorical cross-entropy loss and the accuracy for both methods! are they the same? (if not, it indicates a bug somewhere...) Hint: you should get ~97% accuracy\n",
    " - show the confusion matrix of the predictions (predicted values vs actual labels)\n",
    " - where does the model make mistakes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "# Scale and reshape data\n",
    "X_train = X_train.reshape(60000, 784) / 255\n",
    "X_test = X_test.reshape(10000, 784) / 255\n",
    "y_train = label_binarize(y_train, classes=np.unique(y_train))\n",
    "y_test = label_binarize(y_test, classes=np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I./a. Train MNIST with TF/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the network\n",
    "model = Sequential()\n",
    "# FC + activation layers\n",
    "model.add(Dense(750, input_dim=784, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "for i in range(len(y_test)):\n",
    "    l = y_pred[i]\n",
    "    y_pred[i] = (l == np.max(l)) * np.ones_like(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded preds and tests to normal arrays\n",
    "y_test_n = onehot_2_normal(y_test)\n",
    "y_pred_n = onehot_2_normal(y_pred)\n",
    "\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test_n, y_pred=y_pred_n)\n",
    "conf_mat = confusion_matrix(y_test_n, y_pred_n, labels=[i for i in range(1,11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y_test_n, labels=[i for i in range(1,11)],\n",
    "                      title=('Fig. 4. Confusion matrix of the predictions\\n' +\n",
    "                             'on the MNIST test set of keras\\'s FC-NN model.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I./b. Load weights for MNIST with TF/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "for i in range(len(y_test)):\n",
    "    l = y_pred[i]\n",
    "    y_pred[i] = (l == np.max(l)) * np.ones_like(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded preds and tests to normal arrays\n",
    "y_test_n = onehot_2_normal(y_test)\n",
    "y_pred_n = onehot_2_normal(y_pred)\n",
    "\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test_n, y_pred=y_pred_n)\n",
    "conf_mat = confusion_matrix(y_test_n, y_pred_n, labels=[i for i in range(1,11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y_test_n, labels=[i for i in range(1,11)],\n",
    "                      title=('Fig. 5. Confusion matrix of the predictions\\n' +\n",
    "                             'on the MNIST test set of keras\\'s FC-NN model with.\\n' +\n",
    "                             'pre-computed and loaded model weights.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. MNIST with own FC-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network\n",
    "model = fc_nn()\n",
    "model.add(fc_layer(784, 750))\n",
    "model.add(activation_layer(a='relu'))\n",
    "model.add(fc_layer(750, 500))\n",
    "model.add(activation_layer(a='relu'))\n",
    "model.add(fc_layer(500, 500))\n",
    "model.add(activation_layer(a='relu'))\n",
    "model.add(fc_layer(500, 10))\n",
    "model.add(activation_layer(a='soft'))\n",
    "\n",
    "model.config(s='catcross')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.load(data + 'weights.npy', allow_pickle=True)\n",
    "for i in range(W.shape[0]):\n",
    "    print('Layer {0} shape: {1}'.format(i+1, W[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, W=W[::2], B=W[1::2]).reshape(y_test.shape)\n",
    "for i in range(len(y_test)):\n",
    "    l = y_pred[i]\n",
    "    y_pred[i] = (l == np.max(l)) * np.ones_like(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded preds and tests to normal arrays\n",
    "y_test_n = onehot_2_normal(y_test)\n",
    "y_pred_n = onehot_2_normal(y_pred)\n",
    "\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test_n, y_pred=y_pred_n)\n",
    "conf_mat = confusion_matrix(y_test_n, y_pred_n, labels=[i for i in range(1,11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y_test_n, labels=[i for i in range(1,11)],\n",
    "                      title=('Fig. 6. Confusion matrix of the predictions\\n' +\n",
    "                             'on the MNIST test set of my own FC-NN model.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The accuracies are the same for both Keras and my own FC-NN model, which means the predictions are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints:\n",
    " - On total you can get 10 points for fully completing all tasks.\n",
    " - Decorate your notebook with, questions, explanation etc, make it self contained and understandable!\n",
    " - Comments you code when necessary\n",
    " - Write functions for repetitive tasks!\n",
    " - Use the pandas package for data loading and handling\n",
    " - Use matplotlib and seaborn for plotting or bokeh and plotly for interactive investigation\n",
    " - Use the scikit learn package for almost everything\n",
    " - Use for loops only if it is really necessary!\n",
    " - Code sharing is not allowed between student! Sharing code will result in zero points.\n",
    " - If you use code found on web, it is OK, but, make its source clear! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
