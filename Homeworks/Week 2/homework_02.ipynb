{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Unsupervised learning & clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: I'm so sorry this turned out to be really long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/home/masterdesky/data/world-bank'\n",
    "out = './out/'\n",
    "\n",
    "# Bold print for Jupyter Notebook\n",
    "b1 = '\\033[1m'\n",
    "b0 = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some matplotlib and seaborn parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axistitlesize = 20\n",
    "axisticksize = 17\n",
    "axislabelsize = 26\n",
    "axislegendsize = 23\n",
    "axistextsize = 20\n",
    "axiscbarfontsize = 15\n",
    "\n",
    "# Set axtick dimensions\n",
    "major_size = 6\n",
    "major_width = 1.2\n",
    "minor_size = 3\n",
    "minor_width = 1\n",
    "mpl.rcParams['xtick.major.size'] = major_size\n",
    "mpl.rcParams['xtick.major.width'] = major_width\n",
    "mpl.rcParams['xtick.minor.size'] = minor_size\n",
    "mpl.rcParams['xtick.minor.width'] = minor_width\n",
    "mpl.rcParams['ytick.major.size'] = major_size\n",
    "mpl.rcParams['ytick.major.width'] = major_width\n",
    "mpl.rcParams['ytick.minor.size'] = minor_size\n",
    "mpl.rcParams['ytick.minor.width'] = minor_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading data\n",
    "The woldbank_development_2015.csv (can be found in the same folder with this notebook) file contains the World Development Indicators for the 2015 year, downloaded from [The World Bank's webpage](https://databank.worldbank.org/source/world-development-indicators#).\n",
    "\n",
    "\n",
    " - Look at the data in any text editor. Build up an overall sense how the data is built up and how is the missing values are represented.\n",
    " - Read the file into a pandas dataframe and tell pandas which special pattern means if a value is missing. \n",
    " - The data is in a long format. Convert it into a wide format, where each row is a single country and the columns are the measured features, the Series Codes (eg the first column is 'EG.CFT.ACCS.ZS', the second is 'EG.ELC.ACCS.ZS'. Order of the columns does not matter)!\n",
    " - Keep only those rows, which represents counties and NOT regions. Luckily they are well separated in the order they occur!\n",
    " - Convert the features to numeric format, which will be needed for modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./a. Load in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 2021\n",
    "df = pd.read_csv(os.path.join(data, f'{Y}/world-bank-{Y}.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(n=10))\n",
    "display(df.tail(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two columns in the dataset are just appended information to the table, and could be left out from the analysis. The three lines preceding them are simply separating these two lines from the actual dataset, so they could be also left out. In summary, we can safely cut off the last 5 rows from the end of the table as they are not part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./b. Explore columns and NaN values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to analyze missing entries easier\n",
    "nan_mask = df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = nan_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count of missing values:\\n' +\n",
    "      '========================')\n",
    "print(tabulate([[c, nan_count[c]] for c in nan_count.index], headers=['Feature', 'Count of NaNs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrected number of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very important to notice, there are lot of entries in the column `2015 [YR2015]`, which are simply denoted by `..`. These should be considered to be NaN values, but the `pandas.DataFrame.isna()` method won't detect them as such. We need to delete these manually to \"replace\" them with \"true\" NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '..' with NaN\n",
    "df = df.replace({'..' : float('nan')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(n=10))\n",
    "display(df.tail(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to analyze missing entries easier\n",
    "nan_mask = df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = nan_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count of missing values:\\n' +\n",
    "      '========================')\n",
    "print(tabulate([[c, nan_count[c]] for c in nan_count.index], headers=['Feature', 'Count of NaNs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./c. Drop regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect name of countries/regions in order of their occurences in the DataFrame\n",
    "countries_regions = []\n",
    "for c in df['Country Name']:\n",
    "    if c not in countries_regions:\n",
    "        countries_regions.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the order of countries in the `countries_regions` list we see, that countries in alphabetic order are folloed by regions in also alphabetic order. Since Zimbabwe is the last country in the alphabet by its name, we should drop every row in the DataFrame, which come after the last entry of `Zimbabwe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Zimbabwe in the `countries_regions` list and drop everything after that\n",
    "countries = countries_regions[:countries_regions.index('Zimbabwe') + 1]\n",
    "# Drop region data from the DataFrame\n",
    "df_c = df[df['Country Name'].isin(countries)]\n",
    "print('Dropped number of regions : {0}'.format(len(countries_regions) - len(countries)))\n",
    "print('Dropped number of rows : {0}'.format(df.shape[0] - df_c.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./d. Convert long to wide format\n",
    "\n",
    "Since we truncate the dataset by fitting countries into a single line, we have to drop a column with unique entries. Why? The new wide format would look like similar to this:\n",
    "\n",
    "| Index    | Country Name | Country Code | EG.CFT.ACCS.ZS | EG.ELC.ACCS.ZS | $\\dots$  |\n",
    "|:--------:|:------------:|:------------:|:--------------:|:--------------:|:--------:|\n",
    "| 0        | Afghanistan  | AFG          | $30.1$         | $71.5$         | $\\dots$  |\n",
    "| 1        | Albania      | ALB          | $75.37$        | $100$          | $\\dots$  |\n",
    "| 2        | Algeria      | DZA          | $92.7$         | $99.931...$    | $\\dots$  |\n",
    "| $\\vdots$ | $\\vdots$     | $\\vdots$     | $\\vdots$       | $\\vdots$       | $\\ddots$ |\n",
    "\n",
    "This format is however missing the `Series Name` column. To be able to clearly handle the description of each series by attaching it to the series itself in the table, we would require another, a third dimension. Because this is not feasible within the framework of `pandas`, I'm leaving this column out of the dataset. I create another dictionary however, to store this information for later use. Also I might use `Country Name` and the index column, and leave out the `Country Code`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occurence of series codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect unique series codes and the number of their occurence in the dataset\n",
    "sc = np.unique(df_c['Series Code'], return_counts=True)\n",
    "# Collect the unique occurence counts of series codes in the dataset\n",
    "# Ideally all series codes should exists for each country, thus this\n",
    "# function should return with only one unique value\n",
    "sc_c = np.unique(sc[1], return_counts=True)\n",
    "print('There are {0} different features for each country.'.format(len(sc[0])))\n",
    "print('There are {0} different countries exist in the dataset'.format(np.sum(sc_c[0])))\n",
    "if len(sc_c[0]) == 1:\n",
    "    print('All features exist for each country (however their values still could be NaN).')\n",
    "else:\n",
    "    print('There are features, which only exist for some countries!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all series codes/features have an exsting entry in the dataset for each country, we can proceed to the next step of the current task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the new wide format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First save the descriptions of each series codes\n",
    "features = {k : v for (k, v) in zip(df_c['Series Code'], df_c['Series Name'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple pivot would convert the table to the desirable wide format\n",
    "df_wide = df_c.pivot(index='Country Name', columns='Series Code', values=f'{Y} [YR{Y}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./d. Convert features to numeric format\n",
    "\n",
    "I don't really understand what does \"coverting features to numeric format\" would like to mean here. I'll simply convert all entries/**feature values** into numeric format. Let us first count the number of entries in the data table, which isn't in a numeric format, but in eg. `str`. It could be easily seen, that the number of non-float values are equal to the number of string values in the dataset. This trivially means, all non-numeric entries are indeed simple Python `str`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_float_in_df(df):\n",
    "    # Convert dataframe into an array of dimension (MxN),\n",
    "    # where M is the number of features, while N is the number of countries\n",
    "    df_arr = np.array(df).T\n",
    "    # Create an array to store str() counts for all feature columns\n",
    "    df_str_c = np.zeros(df_arr.shape[0])\n",
    "\n",
    "    # Count number of string values in each feature columns\n",
    "    for i, f in enumerate(df_arr):\n",
    "        df_str_c[i] = sum(1 for v in f if type(v) == str)\n",
    "\n",
    "    return df_str_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_str_c = count_non_float_in_df(df=df_wide)\n",
    "\n",
    "print('Total number of non-float feature values in the dataset : {0}'.format(int(sum(df_wide_str_c))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to numeric format\n",
    "for c in df_wide.columns:\n",
    "    # Convert column 'c' of the DataFrame to numeric format\n",
    "    df_wide[c] = pd.to_numeric(df_wide[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_str_c = count_non_float_in_df(df=df_wide)\n",
    "\n",
    "print('Total number of non-float feature values in the dataset (after conversion, should be 0 if successful) : {0}'.format(int(sum(df_wide_str_c))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing and inspection\n",
    "\n",
    " - Visualize the missing values!\n",
    " - Keep only those countries which has less than 700 missing features in the original table and keep features which were missing in less than 20 country in the original table\n",
    " - Visualize the missing values again! Now really only a few entries are missing. Impute the missing values with its feature's mean value!\n",
    " - How many counties and features do we have left?\n",
    " - Read the kept features' descriptions. In the original table the Series Name describe the meaning of the features. What do you think, based only on these information, which counties are the most similar to Hungary? And Norway?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./a. Visualize the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(40,40))\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.imshow(np.array(df_wide.isna()))\n",
    "\n",
    "# Remove XY ticks for now\n",
    "axes.set_xticks([])\n",
    "axes.set_yticks([])\n",
    "\n",
    "axes.set_title('Fig. 1. Missing entries in the original wide-format dataset',\n",
    "               fontsize=axistitlesize+15, y=-0.28)\n",
    "# Lapelpad needed because of the removed ticks\n",
    "lp = 20\n",
    "axes.set_xlabel('Columns', fontsize=axislabelsize+15, fontweight='bold', labelpad=lp)\n",
    "axes.set_ylabel('Rows', fontsize=axislabelsize+15, fontweight='bold', labelpad=lp)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This in itself is a truly mesmerizing and interesting picture, which I'll definitely set as a cover photo somewhere, but we can also observe a number of important and meaningful things about the dataset on it. It seem so there are columns, which do not store any information whatsover because they're completely filled with NaN entries. I will live with the assumption for now that the \"missingness\" of entires itself do not contain any useful information for this homework, and I'll simply delete them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only those columns, which have absolutely none number values\n",
    "df_wide_dn = df_wide.dropna(axis='columns', thresh=1)\n",
    "print('Number of dropped columns : {0}'.format(df_wide.columns.size - df_wide_dn.columns.size))\n",
    "print('Number of remaining columns : {0}'.format(df_wide_dn.columns.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(40,40))\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.imshow(np.array(df_wide_dn.isna()))\n",
    "\n",
    "# Remove XY ticks for now\n",
    "axes.set_xticks([])\n",
    "axes.set_yticks([])\n",
    "\n",
    "axes.set_title('Fig. 2. Missing entries in the wide-format dataset without the full NaN columns',\n",
    "               fontsize=axistitlesize+15, y=-0.28)\n",
    "# Lapelpad needed because of the removed ticks\n",
    "lp = 20\n",
    "axes.set_xlabel('Columns', fontsize=axislabelsize+15, fontweight='bold', labelpad=lp)\n",
    "axes.set_ylabel('Rows', fontsize=axislabelsize+15, fontweight='bold', labelpad=lp)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./b. Drop out countries with lot of missing values\n",
    "\n",
    "Since there are $1437$ series as we've seen in the first task, we need to drop countries with at least $737$ existing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only those columns, which have absolutely none number values\n",
    "df_w = df_wide_dn.dropna(axis='index', thresh=346)\n",
    "print('Number of dropped countries : {0}'.format(df_wide_dn.index.size - df_w.index.size))\n",
    "print('Number of remaining countries : {0}'.format(df_w.index.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(40,40))\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.imshow(np.array(df_w.isna()))\n",
    "\n",
    "# Remove XY ticks for now\n",
    "axes.set_xticks([])\n",
    "axes.set_yticks([])\n",
    "\n",
    "axes.set_title('Fig. 3. Missing entries in the wide-format dataset without the full NaN columns and dropped out countries',\n",
    "               fontsize=axistitlesize+15, y=-0.32)\n",
    "# Lapelpad needed because of the removed ticks\n",
    "lp = 20\n",
    "axes.set_xlabel('Columns', fontsize=axislabelsize+15, fontweight='bold', labelpad=lp)\n",
    "axes.set_ylabel('Rows', fontsize=axislabelsize+15, fontweight='bold', labelpad=lp)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./c. Fill NaN entries\n",
    "\n",
    "\n",
    "Filling all missings entries by the mean of its feature values as asked in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The new variable has an extra `f` at its end\n",
    "df_wf = df_w.fillna(df_w.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected final list of countries\n",
    "countries_sc = list(df_wf.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./d. Which country is the closest to Hungary? And Norway?\n",
    "\n",
    "Based on the current dataset `df_wf` it is not completely trivial to answer this question. For example we can define a score, which could measure the \"distance\" of each country from Hungary and Norway. This score could be interpeted as some function of the differences between features of two separate countries. In this framework we should find the country corresponding to the global extrema of this scoring function (eg. a minima in the case of the distance interpretation). I'll simply use the MSE of features compared to Hungary/Norway as a scoring function for this sub-task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create anf calculate score for a country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_country_2_MSE(MSE, ref):\n",
    "\n",
    "    # Calculate MSE values\n",
    "    mse_list = []\n",
    "    for c in df_wf.index:\n",
    "        mse_list.append(mean_squared_error(df_wf.loc[ref], df_wf.loc[c]))\n",
    "    # Normalize dataset\n",
    "    mse_list = np.array(mse_list) / np.max(mse_list)\n",
    "    MSE[ref] = {c : v for (c, v) in zip(df_wf.index, mse_list)}\n",
    "\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = add_country_2_MSE(MSE, ref='Hungary')\n",
    "MSE = add_country_2_MSE(MSE, ref='Norway')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the scores for Hungary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_MSE_country(country):\n",
    "\n",
    "    # Sort countries by score values\n",
    "    x = list(df_wf.index)\n",
    "    y = list(MSE[country].values())\n",
    "    y, x = (list(z) for z in zip(*sorted(zip(y, x))))\n",
    "\n",
    "    # Documentation was very useful @:\n",
    "    # https://plotly.com/python/bar-charts/\n",
    "    fig = px.bar(y=x[::-1], x=y[::-1], text=y[::-1],\n",
    "                 color=y[::-1],\n",
    "                 log_x=True,\n",
    "                 title='Fig. 4. MSE of features compared to Hungary',\n",
    "                 labels={\n",
    "                     'y': 'Countries in ascending order by score',\n",
    "                     'x': 'Log of score values',\n",
    "                 },\n",
    "                 orientation='h', width=1300, height=2600)\n",
    "    fig.update_traces(hovertemplate='Country : %{y}<br></br>MSE : %{x:.3e}',\n",
    "                      texttemplate='%{text:.3e}',\n",
    "                      textposition='outside')\n",
    "    fig.update_layout(coloraxis_colorbar=dict(\n",
    "                            dtick=0.05,\n",
    "                            tickfont=dict(\n",
    "                                family='Arial',\n",
    "                                color='black',\n",
    "                                size=22)\n",
    "                            ),\n",
    "                      font=dict(\n",
    "                          family='Arial',\n",
    "                          size=12,\n",
    "                          color='black'\n",
    "                            ),\n",
    "                      uniformtext_minsize=4,\n",
    "                      uniformtext_mode='hide'\n",
    "                     )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_MSE_country(country='Hungary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "According to a simple MSE score, Kazahstan is the closest one to our country in economic aspects, which is a bit unsettling information. The closest country to Norway is Denmark, which is quite more comforting to Norway people. Extra info: I asked my dad, who's an expert and researcher in macroeconomics and he confirmed that Kazahstan is a pretty good tip and it is actually considered as one of closer countries to Hungary in many development indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_country_by_MSE(ref):\n",
    "    \n",
    "    x = list(df_wf.index)\n",
    "    y = list(MSE[ref].values())\n",
    "    y, x = (list(z) for z in zip(*sorted(zip(y, x))))\n",
    "    \n",
    "    print('The \"closest\" country to [{0}] by this scoring function is [{1}].'.format(ref, x[::-1][-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_country_by_MSE(ref='Hungary')\n",
    "closest_country_by_MSE(ref='Norway')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA\n",
    " - Perform PCA with 3 principal components on the filtered, imputed data (from now on, data refers to the filtered, imputed dataset)\n",
    " - Plot the three embedded 2D combination next to each other (0 vs 1, 0 vs 2 and 1 vs 2)\n",
    " - It seems that the embedding is really dominated by a single direction. Normalize the data (each feature should have zero mean and unit variance after normalization) and re-do the PCA and the plotting (do not delete the previous plots, just make new ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3./a. Calculate and visualize PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I actually salvaged these old PCA calc. and plotting functions of mine from one of my projects from last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X):\n",
    "    \"\"\"\n",
    "    Normalize the data to have zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    scaler = StandardScaler()\n",
    "    # Compute the mean and standard dev. and scale the dataset `X`\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), index=df_wf.index, columns=df_wf.columns)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, n_components=3):\n",
    "    \n",
    "    # Permorm the PCA and return the newly created dataset\n",
    "    pca = decomposition.PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    \n",
    "    # Ex-variance to measure impact of PCA features\n",
    "    ex_var = np.var(X_pca, axis=0)\n",
    "    ex_var_ratio = ex_var/np.sum(ex_var)\n",
    "    \n",
    "    return pca, X_pca, ex_var_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_scatter(X_pca, input_title=None, zoom=10):\n",
    "    \n",
    "    ncols = 3\n",
    "    nrows = (len(perplexity)-1)//ncols + 1\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*10, nrows*10))\n",
    "    \n",
    "    # Set a colormap for scatter points\n",
    "    # Color points by countries in alphabetic order\n",
    "    colors = cm.viridis(np.linspace(0, 1, num=len(X_pca)))\n",
    "    alpha = 0.6\n",
    "    \n",
    "    # Which pair of PCs plotted in different columns on the plot\n",
    "    pairs = [(0, 1), (1, 2), (0, 2)]\n",
    "    labels = [('First', 'Second'), ('Second', 'Third'), ('First', 'Third')]\n",
    "\n",
    "    for j in range(nrows):\n",
    "        for i in range(ncols):\n",
    "            ax = axes[j][i]\n",
    "            \n",
    "            X = X_pca[:, pairs[i][0]]\n",
    "            Y = X_pca[:, pairs[i][1]]\n",
    "            ax.scatter(X, Y,\n",
    "                       color=colors, alpha=alpha)\n",
    "            \n",
    "            # Some string magic to hide offset text and write scale on the axis labels\n",
    "            # 1. Turn off offset\n",
    "            plt.setp(ax.get_xaxis().get_offset_text(), visible=False)\n",
    "            plt.setp(ax.get_yaxis().get_offset_text(), visible=False)\n",
    "            # 2. Get scale of axes\n",
    "            x_lim = ax.get_xlim()\n",
    "            y_lim = ax.get_ylim()\n",
    "            # 3. Select greater limit in magnitude, force it to scientific notation\n",
    "            # and trim down the exponent\n",
    "            x_max = np.max(np.abs(x_lim))\n",
    "            y_max = np.max(np.abs(y_lim))\n",
    "            xy_max = np.max((x_max, y_max))\n",
    "            \n",
    "            # Do nothing special in the first row\n",
    "            if j == 0:\n",
    "                x_offset = int(('%E' % x_max).split('E')[-1])\n",
    "                y_offset = int(('%E' % y_max).split('E')[-1])\n",
    "            # Zoom on the center of the dataset in the second row\n",
    "            elif j == 1:\n",
    "                c = (np.mean(X), np.mean(Y))\n",
    "                ax.set_xlim(c[0] - xy_max/zoom, c[0] + xy_max/zoom)\n",
    "                ax.set_ylim(c[1] - xy_max/zoom, c[1] + xy_max/zoom)\n",
    "                # Scale offsets accordingly to zooming/scalong factor\n",
    "                x_offset = int(('%E' % xy_max).split('E')[-1]) - int(np.log10(zoom))\n",
    "                y_offset = int(('%E' % xy_max).split('E')[-1]) - int(np.log10(zoom))\n",
    "            \n",
    "            # Format labels and ticks\n",
    "            ax.set_xlabel('{0} PC [$\\\\times 10^{{{1}}}$]'.format(labels[i][0], x_offset),\n",
    "                               fontsize=axislabelsize+6, fontweight='bold')\n",
    "            ax.set_ylabel('{0} PC [$\\\\times 10^{{{1}}}$]'.format(labels[i][1], y_offset),\n",
    "                               fontsize=axislabelsize+6, fontweight='bold')\n",
    "            ax.tick_params(axis='both', which='major', labelsize=axisticksize+6)\n",
    "    \n",
    "    fig.suptitle(input_title,\n",
    "                 fontsize=axistitlesize+6, y=0.04)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impactful_features(pca, features, get_n=8):\n",
    "    \n",
    "    pca_basis = pca.components_\n",
    "    \n",
    "    pc_f_idx = []\n",
    "    pc_f = []\n",
    "    for pc in range(pca_basis.shape[0]):\n",
    "        pc_current = pca_basis[pc]\n",
    "        pc_n_f_idx = []\n",
    "        pc_n_f = []\n",
    "        for i in range(get_n):\n",
    "            f_idx = np.where(np.abs(pc_current) == sorted(np.abs(pc_current))[::-1][i])[0][0]\n",
    "            pc_n_f_idx.append(f_idx)\n",
    "            pc_n_f.append(list(features.keys())[f_idx])\n",
    "        pc_f_idx.append(pc_n_f_idx)\n",
    "        pc_f.append(pc_n_f)\n",
    "\n",
    "    return pc_f_idx, pc_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_components(pca, pc_f_idx, pc_f,\n",
    "                   input_title=None):\n",
    "    \n",
    "    nrows = 1\n",
    "    ncols = 3\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*16, nrows*10))\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    \n",
    "    pca_basis = pca.components_\n",
    "    for i in range(ncols):\n",
    "        ax = axes[i]\n",
    "        x = pc_f[i]\n",
    "        y = pca_basis[i][pc_f_idx[i]]\n",
    "        \n",
    "        # Set a colormap for bars\n",
    "        # Color bars by their value using a diverging cmap\n",
    "        y_norm = np.array([e/np.abs(np.min(y)) if e < 0 else e/np.abs(np.max(y)) for e in y])\n",
    "        m = interp1d([-1, 1], [0, 1])\n",
    "        colors = cm.RdBu_r(m(y_norm))\n",
    "        ax.barh(x, y,\n",
    "                color=colors, align='center')\n",
    "        \n",
    "        ax.set_title('PC$_{{{0}}}$ components'.format(i+1), fontsize=axistitlesize, fontweight='bold')\n",
    "        ax.set_xlabel('Impact of features', fontsize=axislabelsize, fontweight='bold')\n",
    "        ax.tick_params(axis='both', which='major', labelsize=axisticksize)\n",
    "    \n",
    "    fig.suptitle(input_title,\n",
    "                 fontsize=axistitlesize+25, y=0.0)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unscaled PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, X_pca, ex_var_ratio = PCA(X=df_wf, n_components=3)\n",
    "print(ex_var_ratio)\n",
    "print('Weight if first three PC : {0:.3f}%'.format(ex_var_ratio[:3].sum()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_title = 'Fig. 5. Feature map of the components outputted from the PCA of the dataset'\n",
    "plot_pca_scatter(X_pca=X_pca, input_title=input_title, zoom=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of the N most impactful features in absolute values\n",
    "pc_f_idx, pc_f = get_impactful_features(pca, features, get_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components(pca, pc_f_idx, pc_f,\n",
    "               input_title='Fig. 6. Top features by their impact in absolute value on different PCs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3./b. Normalize the dataset and redo the PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled dataset in wide format, with already filled NaN values\n",
    "df_wf_sc = scale_data(X=df_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, X_pca, ex_var_ratio = PCA(X=df_wf_sc, n_components=3)\n",
    "print(ex_var_ratio)\n",
    "print('Weight if first three PC : {0:.3f}%'.format(ex_var_ratio[:3].sum()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_title = 'Fig. 7. Feature map of the components outputted from the PCA of the scaled and normalized dataset'\n",
    "plot_pca_scatter(X_pca=X_pca, input_title=input_title, zoom=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of the N most impactful features in absolute values\n",
    "pc_f_idx, pc_f = get_impactful_features(pca, features, get_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components(pca, pc_f_idx, pc_f,\n",
    "               input_title='Fig. 8. Top features by their impact in absolute value on different PCs with the scaled dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. T-SNE\n",
    " - Perform T-SNE on the scaled data with 2 components\n",
    " - Plot the embeddings results. Add a text label for each point to make it possible to interpret the results. It will not be possible to read all, but try to make it useful, see the attached image as an example!\n",
    " - Highlight Hungary and Norway! Which countries are the closest one to Hungary and Norway? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./a. Perform and visualize t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE in the scaled data with 2 components\n",
    "df_embedded_variation = []\n",
    "perplexity = [2, 5, 10, 30, 50, 100]\n",
    "for p in perplexity:\n",
    "    df_embedded_variation.append(TSNE(n_components=2, perplexity=p).fit_transform(df_wf_sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color countries in alphabetic order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12*ncols, 12*nrows))\n",
    "\n",
    "# scatter point radius\n",
    "sc_rad = 9\n",
    "\n",
    "# Set a colormap for scatter points\n",
    "# Color points by countries in alphabetic order\n",
    "colors = cm.viridis(np.linspace(0, 1, num=len(df_wf_sc)))\n",
    "markup_colors = ['tab:red', 'tab:orange']\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        ax = axes[i][j]\n",
    "        # Select embedding with the correct perplexity\n",
    "        df_embedded = df_embedded_variation[i*ncols + j]\n",
    "        x = df_embedded[:,0]\n",
    "        y = df_embedded[:,1]\n",
    "        ax.scatter(x, y,\n",
    "                   color=colors, s=sc_rad**2, alpha=0.7)\n",
    "        for c_idx, c in enumerate(['Hungary', 'Norway']):\n",
    "            idx = list(df_wf_sc.index).index(c)\n",
    "            ax.scatter(x[idx], y[idx], label=c,\n",
    "                       color=markup_colors[c_idx], s=(sc_rad+10)**2, alpha=0.8)\n",
    "\n",
    "        ax.set_title('Perplexity = {0}'.format(perplexity[i*ncols + j]), fontsize=axistitlesize, fontweight='bold')\n",
    "        ax.set_xlabel('Embedding dim. 1', fontsize=axislabelsize, fontweight='bold', labelpad=15)\n",
    "        ax.set_ylabel('Embedding dim. 2', fontsize=axislabelsize, fontweight='bold', labelpad=15)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=axisticksize)\n",
    "        ax.legend(loc='best', fontsize=axislegendsize)\n",
    "\n",
    "fig.suptitle('Fig. 9. Embeddings of a t-SNE performed with two components.\\nCountries are colored in alphabetic order.',\n",
    "             fontsize=axistitlesize+12, y=0.04)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color countries by their geographic position\n",
    "\n",
    "I'll color the points corresponding to countries by their latitude coordinates. The reason for this, because there is well-known correlation of WDI and a country being on the northern/southern hemisphere. The reason for this correlation could be found in the arrangement of climate zones, which happens to be varying along the axis of latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from  geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = []\n",
    "long = []\n",
    "for c in list(df_wf_sc.index):\n",
    "    loc = geolocator.geocode(c.split(',')[0])\n",
    "    lat.append(loc.latitude)\n",
    "    long.append(loc.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = (len(perplexity)-1)//ncols + 1\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12*ncols, 12*nrows))\n",
    "\n",
    "# scatter point radius\n",
    "sc_rad = 9\n",
    "\n",
    "# Set a colormap for scatter points\n",
    "# Color points by countries in alphabetic order\n",
    "color_by = lat\n",
    "coord_ref = np.max((np.abs(np.min(color_by)), np.abs(np.max(color_by))))\n",
    "m = interp1d([-coord_ref, coord_ref], [0, 1])\n",
    "colors = cm.seismic(m(color_by))\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        ax = axes[i][j]\n",
    "        # Select embedding with the correct perplexity\n",
    "        df_embedded = df_embedded_variation[i*ncols + j]\n",
    "        x = df_embedded[:,0]\n",
    "        y = df_embedded[:,1]\n",
    "        ax.scatter(x, y,\n",
    "                   color=colors, s=sc_rad**2, alpha=0.7)\n",
    "\n",
    "        ax.set_title('Perplexity = {0}'.format(perplexity[i*ncols + j]), fontsize=axistitlesize, fontweight='bold')\n",
    "        ax.set_xlabel('Embedding dim. 1', fontsize=axislabelsize, fontweight='bold', labelpad=15)\n",
    "        ax.set_ylabel('Embedding dim. 2', fontsize=axislabelsize, fontweight='bold', labelpad=15)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=axisticksize)\n",
    "\n",
    "# Thank you Stackoverflow\n",
    "# https://stackoverflow.com/questions/43805821/matplotlib-add-colorbar-to-non-mappable-object\n",
    "# https://stackoverflow.com/questions/15908371/matplotlib-colorbars-and-its-text-labels\n",
    "# https://stackoverflow.com/questions/16702479/matplotlib-colorbar-placement-and-size\n",
    "# https://stackoverflow.com/questions/33443334/how-to-decrease-colorbar-width-in-matplotlib\n",
    "norm = mpl.colors.Normalize(vmin=int(-coord_ref)-1, vmax=int(coord_ref)+1)\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.seismic, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar_ticks = np.linspace(int(-coord_ref)-1, int(coord_ref)+1, 10, endpoint=True)\n",
    "cbar = fig.colorbar(sm, ticks=cbar_ticks,\n",
    "                    boundaries=np.arange(int(-coord_ref)-1, int(coord_ref)+1+0.01, 0.01),\n",
    "                    pad=0.01, aspect=30,\n",
    "                    ax=axes.ravel().tolist())\n",
    "cbar.set_label('Latitudes [$\\mathrm{arc\\ degrees}$]', fontsize=axiscbarfontsize+20, labelpad=15, rotation=90)\n",
    "cbar.ax.set_yticklabels(['{:.0f}'.format(x) for x in cbar_ticks], fontsize=axiscbarfontsize+10, weight='bold')\n",
    "\n",
    "\n",
    "fig.suptitle('Fig. 10. Embeddings of a t-SNE performed with two components.\\nCountries are colored by their latitude coordinates.',\n",
    "             fontsize=axistitlesize+12, y=0.04)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./b. Annotate figures with country names\n",
    "\n",
    "Add a text label for each point to make it possible to interpret the results. It will not be possible to read all, but try to make it useful, see the attached image as an example!\n",
    "\n",
    "**No.** This is just so bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = (len(perplexity)-1)//ncols + 1\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12*ncols, 12*nrows))\n",
    "\n",
    "# scatter point radius\n",
    "sc_rad = 9\n",
    "\n",
    "# Set a colormap for scatter points\n",
    "# Color points by countries in alphabetic order\n",
    "colors = cm.viridis(np.linspace(0, 1, num=len(df_wf_sc)))\n",
    "markup_colors = ['tab:red', 'tab:orange']\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        ax = axes[i][j]\n",
    "        # Select embedding with the correct perplexity\n",
    "        df_embedded = df_embedded_variation[i*ncols + j]\n",
    "        x = df_embedded[:,0]\n",
    "        y = df_embedded[:,1]\n",
    "        ax.scatter(x, y,\n",
    "                   color=colors, s=sc_rad**2, alpha=0.7)\n",
    "        for c_idx, c in enumerate(['Hungary', 'Norway']):\n",
    "            idx = list(df_wf_sc.index).index(c)\n",
    "            ax.scatter(x[idx], y[idx], label=c,\n",
    "                       color=markup_colors[c_idx], s=folium(sc_rad+10)**2, alpha=0.8)\n",
    "\n",
    "        # Annotation\n",
    "        for k, txt in enumerate(list(df_wf_sc.index)):\n",
    "            ax.annotate(txt, (x[k], y[k]))\n",
    "        \n",
    "        ax.set_title('Perplexity = {0}'.format(perplexity[i*ncols + j]), fontsize=axistitlesize, fontweight='bold')\n",
    "        ax.set_xlabel('Embedding dim. 1', fontsize=axislabelsize, fontweight='bold', labelpad=15)\n",
    "        ax.set_ylabel('Embedding dim. 2', fontsize=axislabelsize, fontweight='bold', labelpad=15)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=axisticksize)\n",
    "        ax.legend(loc='best', fontsize=axislegendsize)\n",
    "\n",
    "fig.suptitle('Fig. 11. Embeddings of a t-SNE performed with two components with annotations.\\nCountries are colored in alphabetic order.',\n",
    "             fontsize=axistitlesize+12, y=0.04)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./c. Countries closest to Hungary and Norway\n",
    "\n",
    "We should find a distance measure to interpet the \"closest country\" to another country in the ensemble above. I'll simply use the `l1` and `l2` measures here to find the closest points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln(a, b, n):\n",
    "    \"\"\"\n",
    "    Calculates the n-norm of two input vectors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    a, b : np.array\n",
    "        The two arrays between the n-norm should be measured\n",
    "    \"\"\"\n",
    "    return np.linalg.norm((a - b), ord=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_from_country(country):\n",
    "    \"\"\"\n",
    "    Returns the l1 and l2 distances of all countries to a specific country.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    country : str\n",
    "        The reference country's name in the dataset.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    distances : dict\n",
    "        This dictionary would store all l1 and l2 distances in different embeddings ordered by countries as its keys.\n",
    "        This would look something like this:\n",
    "        ```python\n",
    "        distances = {\n",
    "            'Afghanistan' : [[emb_p_2_l1, emb_p_2_l2], [emb_p_5_l1, emb_p_5_l1], ... , [emb_p_100_l1, emb_p_100_l2]],\n",
    "            'Albania'     : [ ... ],\n",
    "            ...\n",
    "        }\n",
    "        ```\n",
    "    \"\"\"\n",
    "    distances = {}\n",
    "    # List of all countries\n",
    "    countries = list(df_wf_sc.index)\n",
    "    # Index of country\n",
    "    ref_idx = countries.index(country)\n",
    "    # Calculate l1 and l2 distances\n",
    "    for c_idx, c in enumerate(countries):\n",
    "        if c != country:\n",
    "            distances[c] = []\n",
    "            for emb in df_embedded_variation:\n",
    "                a = emb[ref_idx]\n",
    "                b = emb[c_idx]\n",
    "                d_ln = []\n",
    "                for n in range(1,3):\n",
    "                    d_ln.append(ln(a, b, n))\n",
    "                distances[c].append(d_ln)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose some of the closest countries and plot their distance per t-SNE embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_countries_tsne(distances, top_n=8):\n",
    "    \"\"\"\n",
    "    Collect the best N countries and their corresponding data from the different t-SNE runs.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Reshape into shape (N, p_n, n_c)\n",
    "    # Where\n",
    "    #   `N` is number of different distance measures (currently l1 and l2)\n",
    "    #   `p_n` is the number of different perplexity values used for t-SNE\n",
    "    #   `c_n` is the number of countires in the `distances` dictionary\n",
    "    arr = np.array(list(distances.values())).T\n",
    "    \n",
    "    # List of countries\n",
    "    countries = list(distances.keys())\n",
    "    \n",
    "    best = []\n",
    "    for lN in arr:\n",
    "        best_N = []\n",
    "        for emb in lN:\n",
    "            # Sort countries by values in emb\n",
    "            e, c = (list(z) for z in zip(*sorted(zip(emb, countries))))\n",
    "            # Select the top n countries\n",
    "            best_N.append([c[:top_n], e[:top_n]])\n",
    "        best.append(best_N)\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_closest_tsne(best,\n",
    "                      input_title=None):\n",
    "    \"\"\"\n",
    "    Visualize the output from `closest_countries_tsne()`\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    best : list\n",
    "        Who would have thought? This is the output from `closest_countries_tsne()`! Contains information\n",
    "    \"\"\"\n",
    "    \n",
    "    ncols = 3\n",
    "    nrows = (len(perplexity)-1)//ncols + 1\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*14, nrows*10))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    \n",
    "    colors = ['firebrick', 'cornflowerblue']\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            ax = axes[i][j]\n",
    "            x_ticks = []\n",
    "            x_ticklabels = []\n",
    "            for n in range(2):\n",
    "                data = best[n][i*ncols + j]\n",
    "                c = data[0]\n",
    "                e = data[1]\n",
    "                x = np.array([k for k in range(len(c))]) + 0.2 * (-1)**n\n",
    "                \n",
    "                ax.bar(x, e, label='L{n} distance'.format(n=n+1),\n",
    "                       color=colors[n], width=0.4, align='center')\n",
    "\n",
    "                x_ticks += list(x)\n",
    "                x_ticklabels += c\n",
    "            \n",
    "            # Set xticks\n",
    "            ax.set_xticks(x_ticks)\n",
    "            ax.set_xticklabels(x_ticklabels)\n",
    "        \n",
    "            ax.set_title('Perplexity = {0}'.format(perplexity[i*ncols + j]), fontsize=axistitlesize, fontweight='bold')\n",
    "            ax.set_ylabel('Distance to reference country', fontsize=axislabelsize, fontweight='bold', labelpad=15)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=axisticksize)\n",
    "            ax.tick_params(axis='x', which='major', rotation=80)\n",
    "            \n",
    "            ax.legend(fontsize=axislegendsize)\n",
    "    \n",
    "    fig.suptitle(input_title,\n",
    "                 fontsize=axistitlesize+12, y=-0.02)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_hu = distance_from_country(country='Hungary')\n",
    "distances_nw = distance_from_country(country='Norway')\n",
    "distances_ag = distance_from_country(country='Afghanistan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hu = closest_countries_tsne(distances=distances_hu, top_n=8)\n",
    "best_nw = closest_countries_tsne(distances=distances_nw, top_n=8)\n",
    "best_ag = closest_countries_tsne(distances=distances_ag, top_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest_tsne(best=best_hu,\n",
    "                  input_title='Fig. 12. The 8 closest countries to Hungary in the t-SNE analysis by L1 and L2 metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest_tsne(best=best_nw,\n",
    "                  input_title='Fig. 13. The 8 closest countries to Norway in the t-SNE analysis by L1 and L2 metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest_tsne(best=best_ag,\n",
    "                  input_title='Fig. 14. The 8 closest countries to Afghanistan in the t-SNE analysis by L1 and L2 metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hierarchical clustering\n",
    "\n",
    " - Perform hierarchical clustering on the filtered and scaled data (hint: use seaborn **(\\*sklearn lol)**)\n",
    " - Try to plot in a way that all country's name is visible\n",
    " - Write down your impressions that you got from this plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./a. Perform clustering\n",
    "\n",
    "For the clustering I've used the hierarchical agglomerative clustering with Ward's minimum variance method from `sklearn`. Ward's method is an agglomerative (\"bottom to top up\") type clustering, based on the minimalization of the Within Cluster Variance. To understand this, first a $\\varrho$ euclidean distance should be defined between two individual points in the feature space:\n",
    "\n",
    "$$\n",
    "\\varrho_{ij} = \\varrho \\left( X_{i}, X_{j} \\right) = \\left| \\left| X_{i} - X_{j} \\right| \\right|\n",
    "$$\n",
    "\n",
    "With this definition, we can also define a $\\sigma_{ij}^{2}$ variance or equivalently the $\\varrho_{ij}^{2}$ squared euclidean distance (SED):\n",
    "\n",
    "$$\n",
    "\\sigma_{ij}^{2} = \\varrho_{ij}^{2} = \\left| \\left| X_{i} - X_{j} \\right| \\right|^{\\ 2}\n",
    "$$\n",
    "\n",
    "At the beginning, we assign all individual feature vectors into a singleton (a cluster with a single element). Thus, at the first step for $N$ elements we have $N$ number of clusters. At the next step it is considered, that with the merge of two clusters $A$ and $B$, how much the total variance will be increased as follows:\n",
    "\n",
    "$$\n",
    "\\Delta \\left( A, B \\right)\n",
    "=\n",
    "\\sum_{i \\in A \\cup B} \\left| \\left| \\vec{x}_{i} - \\vec{c}_{A \\cup B} \\right| \\right|^{\\ 2}\n",
    "-\n",
    "\\sum_{i \\in A} \\left| \\left| \\vec{x}_{i} - \\vec{c}_{A} \\right| \\right|^{\\ 2}\n",
    "-\n",
    "\\sum_{i \\in B} \\left| \\left| \\vec{x}_{i} - \\vec{c}_{B} \\right| \\right|^{\\ 2}\n",
    "$$\n",
    "\n",
    "Here $\\Delta \\left( A, B \\right)$ is the merging cost (variance growth) for merging clusters $A$ and $B$, $\\vec{x}_{i}$ is the $i$th feature vector in the corresponding cluster and $\\vec{c}$ is the centroid of the same cluster.  \n",
    "Ward's method tries to keep this variance change as small as possible, creating $k$ clusters at the end, which $k$ parameter is predefined at the start of the clustering.\n",
    "\n",
    "#### Sourced from:\n",
    "- Joe H. Ward Jr. (1963) Hierarchical Grouping to Optimize an Objective Function, Journal of the American Statistical Association, 58:301, 236-244, DOI: [10.1080/01621459.1963.10500845](https://doi.org/10.1080/01621459.1963.10500845)\n",
    "- http://scikit-learn.sourceforge.net/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "- https://en.wikipedia.org/wiki/Ward's_method\n",
    "- https://www.stat.cmu.edu/~cshalizi/350/lectures/08/lecture-08.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(data):\n",
    "    \"\"\"\n",
    "    Gather the centroids of the created clusters from a datatable with additional `cluster_id` info.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : `pandas.DataFrame`\n",
    "        desc\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    centroids : `pandas.DataFrame`\n",
    "        Centroids of the created clusters by features\n",
    "    \"\"\"\n",
    "    indeces = list(set(data['cluster_id'].values))\n",
    "    \n",
    "    # Get the mean values of each feature inside the clusters and define these as centroids\n",
    "    # For similar behaviour in eg. `sklearn`, see __doc__ for 'sklearn.cluster.KMeans().cluster_centers_':\n",
    "    # (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "    cd = {idx : [data[(data['cluster_id']==idx)].values.mean() \\\n",
    "               for feature in data.columns[:-1]] for idx in indeces}\n",
    "    \n",
    "    # Create dataframe to contain new centroids\n",
    "    centroids = pd.DataFrame(data=cd)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_agglomerative(data, n_clusters=6, linkage='ward'):\n",
    "    \"\"\"\n",
    "    Perform and hierarchical agglomerative clustering using `sklearn` with an arbitrary linking method\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : `pandas.DataFrame`\n",
    "        desc\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    agglomerative : `sklearn.cluster._agglomerative.AgglomerativeClustering`\n",
    "        The agglomerative clustering model fitted on the input data.\n",
    "        \n",
    "    df : `pandas.DataFrame`\n",
    "        The input data with an additional column\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    # Perform clustering\n",
    "    agglomerative = AgglomerativeClustering(linkage=linkage, n_clusters=n_clusters)\n",
    "    # Fit on the full dataset\n",
    "    agglomerative.fit(df)\n",
    "    # Predict on the original dataset\n",
    "    df['cluster_id'] = agglomerative.labels_\n",
    "    # Return the number of elements of clusters\n",
    "    cluster_sizes = df['cluster_id'].value_counts()\n",
    "\n",
    "    return agglomerative, df, cluster_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible linkages: [ 'ward' | 'average' | 'complete' | 'single' ]\n",
    "# Suggested: Ward's method ['ward']\n",
    "linkage = 'ward'\n",
    "n_clusters = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative, df_clustering, cluster_sizes = perform_agglomerative(df_wf_sc, n_clusters=n_clusters, linkage=linkage)\n",
    "\n",
    "# Reporting after agglomerative clustering\n",
    "print(b1 + 'REPORT FOR THE SCALED, FILLED DATASET:' + b0)\n",
    "clusters_indeces = list(set(df_clustering.cluster_id.values))\n",
    "N_CLUSTERS = len(clusters_indeces)\n",
    "centroids = get_centroids(df_clustering) \n",
    "print(b1 + 'Cluster centroids:' + b0 + '\\n{0}\\n'.format(centroids))\n",
    "print((b1 + 'Total number of created clusters:' + b0 + ' {0}\\n' + b0).format(N_CLUSTERS))\n",
    "print(b1 + 'Cluster sizes:' + b0 + '\\n{0}'.format(cluster_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./b. Visualize clustering\n",
    "\n",
    "Color the scattered points on the t-SNE graphs by clusters. Label the clusters on the graph also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = (len(perplexity)-1)//ncols + 1\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12*ncols, 12*nrows))\n",
    "\n",
    "# Scatter point radiuses\n",
    "sc_rad = 9\n",
    "clustersize = 900\n",
    "clustertextsize = 250\n",
    "\n",
    "# Color scatter points by their corresponding cluster\n",
    "# I've written this part of code last year, just copy-pasing  this into this notebook.\n",
    "cluster_labels = df_clustering['cluster_id'] \n",
    "indeces = list(set(df_clustering['cluster_id'].values))\n",
    "    # SPAGHETTI CODE IM SO SORRY\n",
    "    # These are the indeces of countries in the created clusters\n",
    "country_indeces_in_clstrs = [[countries_sc.index(country) for country in list(df_clustering[df_clustering['cluster_id']==idx].index)] for idx in indeces]\n",
    "colors = cm.nipy_spectral(cluster_labels.astype(float) / len(indeces))\n",
    "index_colors = cm.nipy_spectral(np.array(indeces)/len(indeces))\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        ax = axes[i][j]\n",
    "        # Select embedding with the correct perplexity\n",
    "        df_embedded = df_embedded_variation[i*ncols + j]\n",
    "        x = df_embedded[:,0]\n",
    "        y = df_embedded[:,1]\n",
    "        ax.scatter(x, y,\n",
    "                   color=colors, s=sc_rad**2, alpha=0.7)\n",
    "        \n",
    "        # Create the centers for cluster indeces\n",
    "        centers = np.array([[np.mean(x[elems]), np.mean(y[elems])] for elems in country_indeces_in_clstrs])\n",
    "        \n",
    "        # Draw white circles at cluster centers and label them\n",
    "        ax.scatter(centers[:, 0], centers[:, 1],\n",
    "                   marker='o', c='white', alpha=0.7,\n",
    "                   s=clustersize, edgecolor='black')\n",
    "        for k, c in enumerate(centers):\n",
    "            ax.scatter(c[0], c[1],\n",
    "                       marker='${0}$'.format(indeces[k]), alpha=0.9,\n",
    "                       s=clustertextsize, edgecolor='black',\n",
    "                       color=np.array(index_colors[k][:-1]))\n",
    "\n",
    "        ax.set_title('Perplexity = {0}'.format(perplexity[i*ncols + j]), fontsize=axistitlesize, fontweight='bold')\n",
    "        ax.set_xlabel('Embedding dim. 1', fontsize=axislabelsize, fontweight='bold', labelpad=15)\n",
    "        ax.set_ylabel('Embedding dim. 2', fontsize=axislabelsize, fontweight='bold', labelpad=15)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=axisticksize)\n",
    "\n",
    "fig.suptitle('Fig. 15. Embeddings of a t-SNE performed with two components.\\nThe countries are colored by their corresponding cluster.',\n",
    "             fontsize=axistitlesize+12, y=0.04)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./c. Dendorgram\n",
    "\n",
    "#### Sourced from:\n",
    "- https://github.com/scikit-learn/scikit-learn/blob/70cf4a676caa2d2dad2e3f6e4478d64bcb0506f7/examples/cluster/plot_hierarchical_clustering_dendrogram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model):\n",
    "    \"\"\"\n",
    "    Plot dendogram from the results of a clustering. Also mark countries on the figure.\n",
    "    \n",
    "    Paramters:\n",
    "    ----------\n",
    "    model : `sklearn.cluster.*`\n",
    "        The clustering model fitted on the input data.\n",
    "    \"\"\"\n",
    "    \n",
    "    nrows = 1\n",
    "    ncols = 1\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*30, nrows*45))\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, p=30, truncate_mode=None,\n",
    "               orientation='right', no_labels=False,\n",
    "               ax=axes)\n",
    "\n",
    "    # Change ticklabels on Y-axis\n",
    "    y_ticks_loc = axes.get_yticks(minor=False)\n",
    "    y_ticks_str = axes.get_yticklabels(minor=False)\n",
    "    y_ticks_int = []\n",
    "    for t in y_ticks_str:\n",
    "        y_ticks_int.append(int(t.get_text()))\n",
    "    new_y_ticks = np.array(list(df_wf_sc.index))[y_ticks_int]\n",
    "\n",
    "    # Write countries instead of indeces\n",
    "    axes.set_yticks(y_ticks_loc)\n",
    "    axes.set_yticklabels(new_y_ticks)\n",
    "\n",
    "    axes.tick_params(axis='both', which='major', labelsize=axisticksize)\n",
    "    \n",
    "    fig.suptitle('Fig. 16. Dendogram of the agglomerative clustering on the scaled dataset.',\n",
    "                 fontsize=axistitlesize+12, y=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(model=agglomerative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coloring should indicate something about the depth of the agglomeration or clusters?? I really don't know, sorry... However the neighbouring countries on this dendrogam correlates pretty well with the observations in the previous task. For example Hungary (in the middle of the graph) was grouped together with the countries said to be the closest to us by the t-SNE analysis. So I can say, we created a robust model and statistics to decide, which countries are the closest to us in WDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for the plot\n",
    "# rip attached image :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "header### Hints:\n",
    " - On total you can get 10 points for fully completing all tasks.\n",
    " - Decorate your notebook with, questions, explanation etc, make it self contained and understandable!\n",
    " - Comments you code when necessary\n",
    " - Write functions for repetitive tasks!\n",
    " - Use the pandas package for data loading and handling\n",
    " - Use matplotlib and seaborn for plotting or bokeh and plotly for interactive investigation\n",
    " - Use the scikit learn package for almost everything\n",
    " - Use for loops only if it is really necessary!\n",
    " - Code sharing is not allowed between student! Sharing code will result in zero points.\n",
    " - If you use code found on web, it is OK, but, make its source clear! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
