{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. SVC methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/'\n",
    "out = './out/'\n",
    "\n",
    "# Bold print for Jupyter Notebook\n",
    "b1 = '\\033[1m'\n",
    "b0 = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some matplotlib and seaborn parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axistitlesize = 20\n",
    "axisticksize = 17\n",
    "axislabelsize = 26\n",
    "axislegendsize = 23\n",
    "axistextsize = 20\n",
    "axiscbarfontsize = 15\n",
    "\n",
    "# Set axtick dimensions\n",
    "major_size = 6\n",
    "major_width = 1.2\n",
    "minor_size = 3\n",
    "minor_width = 1\n",
    "mpl.rcParams['xtick.major.size'] = major_size\n",
    "mpl.rcParams['xtick.major.width'] = major_width\n",
    "mpl.rcParams['xtick.minor.size'] = minor_size\n",
    "mpl.rcParams['xtick.minor.width'] = minor_width\n",
    "mpl.rcParams['ytick.major.size'] = major_size\n",
    "mpl.rcParams['ytick.major.width'] = major_width\n",
    "mpl.rcParams['ytick.minor.size'] = minor_size\n",
    "mpl.rcParams['ytick.minor.width'] = minor_width\n",
    "\n",
    "mpl.rcParams.update({'figure.autolayout': False})\n",
    "\n",
    "# Seaborn style settings\n",
    "sns.set_style({'axes.axisbelow': True,\n",
    "               'axes.edgecolor': '.8',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': True,\n",
    "               'axes.labelcolor': '.15',\n",
    "               'axes.spines.bottom': True,\n",
    "               'axes.spines.left': True,\n",
    "               'axes.spines.right': True,\n",
    "               'axes.spines.top': True,\n",
    "               'figure.facecolor': 'white',\n",
    "               'font.family': ['sans-serif'],\n",
    "               'font.sans-serif': ['Arial',\n",
    "                'DejaVu Sans',\n",
    "                'Liberation Sans',\n",
    "                'Bitstream Vera Sans',\n",
    "                'sans-serif'],\n",
    "               'grid.color': '.8',\n",
    "               'grid.linestyle': '--',\n",
    "               'image.cmap': 'rocket',\n",
    "               'lines.solid_capstyle': 'round',\n",
    "               'patch.edgecolor': 'w',\n",
    "               'patch.force_edgecolor': True,\n",
    "               'text.color': '.15',\n",
    "               'xtick.bottom': True,\n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'in',\n",
    "               'xtick.top': True,\n",
    "               'ytick.color': '.15',\n",
    "               'ytick.direction': 'in',\n",
    "               'ytick.left': True,\n",
    "               'ytick.right': True})\n",
    "\n",
    "# Colorpalettes, colormaps, etc.\n",
    "sns.set_palette(palette='rocket')\n",
    "rocket_cmap = sns.color_palette('rocket', as_cmap=True)\n",
    "\n",
    "target_colors = np.array([cm.magma(0.5), cm.magma(0.75), cm.magma(0.93)])\n",
    "feature_colors = np.array([rocket_cmap(0.17), cm.magma(0.45), cm.magma(0.60), cm.magma(0.75)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Linear SVC in case of linear separation\n",
    "\n",
    "- load the Iris dataset (can be found in sklearn API)\n",
    "- scale the data and plot the petal length vs petal width in a scatterplot colored with the target\n",
    "- train an SVC model with linear kernel with default parameter settings, but once with C=1 and then C=1000\n",
    "- visualize the model's decision boundary and the margins based on the coefficients learnt by the model\n",
    "- interpret the results, what is the role of the C hyperparameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X):\n",
    "    \"\"\"\n",
    "    Normalize the data to have zero mean and unit variance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray or array-like in shape of (N, M)\n",
    "        The unscaled dataset.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray in shape of (N, M)\n",
    "        The already scaled dataset with zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    scaler = StandardScaler()\n",
    "    # Compute the mean and standard dev. and scale the dataset `X`\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./a. Load the Iris dataset and discover feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris values and labels\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "features = iris.feature_names\n",
    "# Scale dataset\n",
    "X = pd.DataFrame(scale_data(X), columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm just testing, which plots are easier to read...\n",
    "#### Default colors for `seaborn`'s `rocket` palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*8,nrows*8),\n",
    "                         sharey=True,\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.09, wspace=0.04)\n",
    "\n",
    "# Visualization style of figure\n",
    "methods = ['poly', 'bars']\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.histplot(X, stat='count', element=methods[i], cumulative=False, common_bins=True,\n",
    "                 ax=ax)\n",
    "    ax.set_xlabel('Feature value [cm]', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('Count', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize=16) # for legend text\n",
    "\n",
    "plt.suptitle('Fig. 1. Distribution of the feature values in the\\n' +\n",
    "             'Iris dataset. (Only test figure to test color palette.)',\n",
    "             color='white',\n",
    "             fontsize=axistitlesize+5, y=0.0)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*8,nrows*8),\n",
    "                         sharey=True,\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.09, wspace=0.04)\n",
    "\n",
    "# Visualization style of figure\n",
    "methods = ['poly', 'bars']\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.histplot(X, stat='count', element=methods[i], cumulative=False, common_bins=True,\n",
    "                 palette=feature_colors,\n",
    "                 ax=ax)\n",
    "    ax.set_xlabel('Feature value [cm]', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('Count', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize=16) # for legend text\n",
    "\n",
    "axes[0].set_title('Maybe this one is the best imho', fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "\n",
    "plt.suptitle('Fig. 1. Distribution of the feature values in the\\n' +\n",
    "             'Iris dataset. (Only test figure to test color palette.)',\n",
    "             color='white',\n",
    "             fontsize=axistitlesize+5, y=0.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize iris dataset\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*6, nrows*6),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.09, wspace=0.09)\n",
    "\n",
    "sc = 5\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        ax = axes[i][j]\n",
    "        if i==j:\n",
    "            ax.text(0.5, 0.5, features[i], color='white',\n",
    "                    fontsize=28, ha='center', va='center')\n",
    "            ax.grid(False)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            ax.scatter(X[features[i]], X[features[j]], s=sc**2,\n",
    "                       color=target_colors[y])\n",
    "            ax.tick_params(axis='both', which='major', labelsize=12, colors='white')\n",
    "\n",
    "plt.suptitle('Fig. 2. Feature space of the Iris dataset colored by target labels.', color='white',\n",
    "             fontsize=axistitlesize+5, y=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./b. Fit the default SVC model from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of model prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_test : array-like of shape (N, )\n",
    "        Original labels of the test dataset.\n",
    "    \n",
    "    y_pred : array-like of shape (N, )\n",
    "        Predicted labels of the test dataset.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Accuracy of model in reference of the true test labels.\n",
    "    \"\"\"\n",
    "    # Binarize labels\n",
    "    y_test = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    y_pred = label_binarize(y_pred, classes=np.unique(y_pred))\n",
    "\n",
    "    correct = 0\n",
    "    for (t, p) in zip(y_test, y_pred):\n",
    "        if all(t == p):\n",
    "            correct += 1\n",
    "    return correct / len(y_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_mat, y, labels=None, title=None):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(figsize=(10,10))\n",
    "    axes.set_aspect('equal')\n",
    "\n",
    "    im = axes.imshow(conf_mat)\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for X in range(conf_mat.shape[0]):\n",
    "        for Y in range(conf_mat.shape[1]):\n",
    "            axes.text(Y, X, conf_mat[X, Y], fontsize=30,\n",
    "                      ha='center', va='center', color='white', fontweight='bold', \n",
    "                      bbox=dict(color=np.array((0,0,0,0.2)), lw=0)\n",
    "                     )\n",
    "\n",
    "    # Set axis tick locations and labels\n",
    "    ticks = [i for i in range(len(set(y)))]\n",
    "    if labels is None:\n",
    "        ticklabels = [i+1 for i in range(len(set(y)))]\n",
    "    else:\n",
    "        ticklabels = list(labels)\n",
    "\n",
    "    axes.set_xticks(ticks)\n",
    "    axes.set_xticklabels(ticklabels)\n",
    "    axes.set_yticks(ticks)\n",
    "    axes.set_yticklabels(ticklabels)\n",
    "\n",
    "    axes.set_xlabel('Predicted labels', fontsize=axislabelsize+5, fontweight='bold')\n",
    "    axes.set_ylabel('True labels', fontsize=axislabelsize+5, fontweight='bold')\n",
    "    axes.tick_params(axis='both', which='major', labelsize=axisticksize+5)\n",
    "    axes.xaxis.tick_top()\n",
    "    axes.xaxis.set_label_position('top') \n",
    "\n",
    "    axes.grid(False)\n",
    "\n",
    "    # Create an axis on the right side of `axes`. The width of `cax` will be 5%\n",
    "    # of `axes` and the padding between `cax` and axes will be fixed at 0.1 inch\n",
    "    divider = make_axes_locatable(axes)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.1)\n",
    "    cbar = plt.colorbar(mappable=im, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=axiscbarfontsize, colors='black')\n",
    "    cbar.set_label('Number of occurences', fontsize=axiscbarfontsize+10, labelpad=15, rotation=90)\n",
    "\n",
    "    plt.suptitle(title,\n",
    "                 fontsize=axistitlesize+5, y=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC with regularization parameter `C`=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit the SVC model with C=1 and linear kernel\n",
    "svc = SVC(C=1, kernel='linear')\n",
    "svc.fit(X, y);\n",
    "# Make predictions\n",
    "y_pred = svc.predict(X)\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y,\n",
    "                      title=('Fig. 3. Confusion matrix of the predictions\\nof sklearn\\'s SVC model with $C=1$ and linear kernel.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC with regularization parameter `C`=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit the SVC model with C=1000 and linear kernel\n",
    "svc = SVC(C=1000, kernel='linear')\n",
    "svc.fit(X, y);\n",
    "# Make predictions\n",
    "y_pred = svc.predict(X)\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y,\n",
    "                      title=('Fig. 4. Confusion matrix of the predictions\\nof sklearn\\'s SVC model with $C=1000$ and linear kernel.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./c. Plotting decision boundary using fit coefficients learned by the model\n",
    "\n",
    "#### Source used\n",
    "- https://stackoverflow.com/questions/51297423/plot-scikit-learn-sklearn-svm-decision-boundary-surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(X_i, X_j, h=200):\n",
    "    \n",
    "    xi_min, xi_max = X_i.min() - 1, X_i.max() + 1\n",
    "    xj_min, xj_max = X_j.min() - 1, X_j.max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(xi_min, xi_max, h),\n",
    "                         np.linspace(xj_min, xj_max, h),\n",
    "                         copy=True)\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, svc, xx, yy, i, j, **kwargs):\n",
    "    xx_f, yy_f = xx.ravel(), yy.ravel()\n",
    "    X_pred = np.zeros((xx_f.shape[0], svc.shape_fit_[1]))\n",
    "    X_pred[:, i], X_pred[:, j] = xx_f, yy_f\n",
    "    #W = svc.coef_[:,(i,j)]\n",
    "    #Z = np.sum((np.dot(np.c_[xx_f, yy_f], W.T) + svc.intercept_ < 0), axis=1)\n",
    "    Z = svc.predict(X_pred)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize iris dataset\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*6, nrows*6),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.09, wspace=0.09)\n",
    "\n",
    "sc = 5\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        ax = axes[i][j]\n",
    "        if i==j:\n",
    "            ax.text(0.5, 0.5, features[i], color='white',\n",
    "                    fontsize=28, ha='center', va='center')\n",
    "            ax.grid(False)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            # Set-up grid for plotting\n",
    "            X_i, X_j = X[features[i]], X[features[j]]\n",
    "            xx, yy = make_meshgrid(X_i, X_j, h=300)\n",
    "            plot_contours(ax, svc, xx, yy, i, j, levels=2, colors=target_colors, alpha=0.4)\n",
    "            ax.scatter(X_i, X_j, s=sc**2,\n",
    "                       color=target_colors[y])\n",
    "            ax.tick_params(axis='both', which='major', labelsize=12, colors='white')\n",
    "\n",
    "plt.suptitle('Fig. 5. Feature space of the Iris dataset with the predicted decision bundaries\\n' +\n",
    "             'also visualized on the graph.',\n",
    "             color='white',\n",
    "             fontsize=axistitlesize+5, y=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Linear SVC but non-linear separation\n",
    "\n",
    "- create a dataset with the following: X, y = sklearn.datasets.make_moons(noise=0.1, random_state=0)\n",
    "- perform the same steps just as in the previous exercise and use the linear kernel for  the SVC\n",
    "- since linear SVC cannot do non-linear separation, you will need to do some workaround, for example adding polynomial features (3rd order would be a good choice)\n",
    "- write down with your own words in few sentences how the support vector machine works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_feature_colors = np.array([rocket_cmap(0.25), cm.magma(0.75)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./a. Generate dataset and perfrom same steps as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.make_moons(noise=0.1, random_state=0)\n",
    "# Scale dataset (Should I??)\n",
    "#X = scale_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize iris dataset\n",
    "fig, axes = plt.subplots(figsize=(12, 12),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "sc = 15\n",
    "axes.scatter(X[:, 0], X[:, 1], s=sc**2,\n",
    "           color=gen_feature_colors[y], ec='k', alpha=0.8)\n",
    "axes.tick_params(axis='both', which='major', labelsize=12, colors='white')\n",
    "\n",
    "plt.suptitle('Fig. 6. Feature space of the generated dataset', color='white',\n",
    "             fontsize=axistitlesize+5, y=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*8,nrows*8),\n",
    "                         sharey=True,\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.09, wspace=0.04)\n",
    "\n",
    "# Visualization style of figure\n",
    "methods = ['poly', 'bars']\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.histplot(X, stat='count', element=methods[i], cumulative=False, common_bins=True,\n",
    "                 palette=gen_feature_colors,\n",
    "                 ax=ax)\n",
    "    ax.set_xlabel('Feature value', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('Count', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize=16) # for legend text\n",
    "\n",
    "axes[0].set_title('Maybe this one is the best imho', fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "\n",
    "plt.suptitle('Fig. 7. Distribution of the feature values in the\\n' +\n",
    "             'dataset generated using sklearn\\'s make_moons() function.',\n",
    "             color='white',\n",
    "             fontsize=axistitlesize+5, y=0.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC with regularization parameter `C`=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit the SVC model with C=1 and linear kernel\n",
    "svc = SVC(C=1, kernel='linear')\n",
    "svc.fit(X, y);\n",
    "# Make predictions\n",
    "y_pred = svc.predict(X)\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y, y_pred, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y, labels=[1,0],\n",
    "                      title=('Fig. 8. Confusion matrix of the predictions\\nof sklearn\\'s SVC model with $C=1$ and linear kernel.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC with regularization parameter `C`=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit the SVC model with C=1000 and linear kernel\n",
    "svc = SVC(C=1000, kernel='linear')\n",
    "svc.fit(X, y);\n",
    "# Make predictions\n",
    "y_pred = svc.predict(X)\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y, y_pred, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y, labels=[1,0],\n",
    "                      title=('Fig. 9. Confusion matrix of the predictions\\nof sklearn\\'s SVC model with $C=1000$ and linear kernel.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, off=0.5, h=200):\n",
    "    x_min, x_max = x.min() - off, x.max() + off\n",
    "    y_min, y_max = y.min() - off, y.max() + off\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, h), np.linspace(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **kwargs):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **kwargs)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_i, X_j = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X_i, X_j, off=0.25, h=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12,12),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "sc = 15\n",
    "plot_contours(axes, svc, xx, yy, levels=1, colors=gen_feature_colors, alpha=0.8)\n",
    "axes.scatter(X_i, X_j, color=gen_feature_colors[y], s=sc**2,\n",
    "             ec='k', alpha=0.8)\n",
    "\n",
    "axes.set_xlabel('Feature value of feature $i$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('Feature value of feature $j$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "plt.suptitle('Fig. 10. Feature space of the generated dataset with the\\n' +\n",
    "             'predicted linear SVC decision bundaries also visualized\\n' +\n",
    "             'on the graph.',\n",
    "             color='white',\n",
    "             fontsize=axistitlesize+5, y=0.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./b. Add polynomial features to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit from sklearn's documentation\n",
    "# https://scikit-learn.org/0.23/auto_examples/model_selection/plot_underfitting_overfitting.html\n",
    "pipeline = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=3, include_bias=False)),\n",
    "                     (\"svc\", SVC(C=1000.0, kernel='linear'))])\n",
    "# Transform X for the PolynomialFeatures() and LinearRegression() class\n",
    "# Then fit on the pipeline the available data\n",
    "pipeline.fit(X, y)\n",
    "# Evaluate the models using crossvalidation\n",
    "scores = cross_val_score(pipeline, X, y,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12,12),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "sc = 15\n",
    "plot_contours(axes, pipeline, xx, yy, levels=1, colors=gen_feature_colors, alpha=0.8)\n",
    "axes.scatter(X_i, X_j, color=gen_feature_colors[y], s=sc**2,\n",
    "             ec='k', alpha=0.8)\n",
    "\n",
    "axes.set_xlabel('Feature value of feature $i$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('Feature value of feature $j$', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "plt.suptitle('Fig. 11. Feature space of the generated dataset with the\\n' +\n",
    "             'predicted decision bundaries also visualized on the graph.\\n' +\n",
    "             'The predicting model was an SVC with third-order polynomial\\n' +\n",
    "             'features added to it.',\n",
    "             color='white',\n",
    "             fontsize=axistitlesize+5, y=0.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./c. How do support vector machines work?\n",
    "\n",
    "ASD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Load the dataset from 2 weeks ago and build/evaluate the SVC with default settings\n",
    "\n",
    "Reminder:\n",
    "- you need to build a classifier that predicts the probability of a sample coming from a cancerous (tumor type is normal or not) person based on the measured protein levels\n",
    "- train the SVM classifier (SVC in sklearn API) on every second sample (not first 50% of the data (!), use every second line)\n",
    "- generate prediction for the samples that were not used during the training\n",
    "\n",
    "To-do now:\n",
    "- build default SVC, but set it to predict probabilities\n",
    "- plot the ROC curve and calculate the confusion matrix for the predictions\n",
    "- do the same for the CancerSEEK predictions and compare your model's performance to CancerSEEK performance (as a reference, plot it on the same figure)\n",
    "- how good is the performance of the new model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3./a. Load the preprocess dataset from the 5th homework\n",
    "\n",
    "I won't copy the whole preprocessing pipeline here, but I'll only save the processed dataset into a file and load that from here for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data + 'final_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3./b. SVC model to predict cancerous samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset\n",
    "X = df[df.columns[2:-3]]\n",
    "y = df[df.columns[-1]]\n",
    "\n",
    "# Train-test split as given by task description\n",
    "X_train, X_test = X[::2], X[1::2]\n",
    "y_train, y_test = y[::2], y[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC with regularization parameter `C`=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit the SVC model with C=1\n",
    "svc = SVC(C=1)\n",
    "svc.fit(X_train, y_train);\n",
    "# Make predictions\n",
    "y_pred = svc.predict(X_test)\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y, labels=[1,0],\n",
    "                      title=('Fig. 12. Confusion matrix of the predictions\\nof sklearn\\'s SVC model with C=1\\n' +\n",
    "                             'on the CancerSEEK tumor recognition dataset.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC with regularization parameter `C`=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit the SVC model with C=1000\n",
    "svc = SVC(C=1000)\n",
    "svc.fit(X_train, y_train);\n",
    "# Make predictions\n",
    "y_pred = svc.predict(X_test)\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y, labels=[1,0],\n",
    "                      title=('Fig. 13. Confusion matrix of the predictions\\nof sklearn\\'s SVC model with C=1\\n' +\n",
    "                             'on the CancerSEEK tumor recognition dataset.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3./c. Plot ROC curve and confusion matrix of probabilistic predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit the SVC model with C=1 to predict probabilites\n",
    "svc = SVC(probability=True)\n",
    "svc.fit(X_train, y_train);\n",
    "# Make predictions\n",
    "y_pred = svc.predict(X_test)\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y, labels=[1,0],\n",
    "                      title=('Fig. 14. Confusion matrix of the predictions\\nof sklearn\\'s SVC model with C=1\\n' +\n",
    "                             'on the CancerSEEK tumor recognition dataset.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize labels\n",
    "y = label_binarize(y, classes=np.unique(y))\n",
    "y_test = np.array(y_test)[:, np.newaxis]\n",
    "y_test_cS = np.array(df['CancerSEEK Test Result'])[1::2][:, np.newaxis]\n",
    "n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = svc.fit(X_train, y_train).decision_function(X_test)\n",
    "if n_classes == 1:\n",
    "    y_score = y_score[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc(y_test, y_score, n_classes):\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_my, tpr_my, roc_auc_my = compute_roc(y_test=y_test,\n",
    "                                         y_score=y_score, n_classes=n_classes)\n",
    "fpr_cS, tpr_cS, roc_auc_cS = compute_roc(y_test=y_test_cS,\n",
    "                                         y_score=y_score, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(11, 11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes.plot([0, 1], [0, 1], color=rocket_cmap(0.25), lw=5, linestyle='--')\n",
    "\n",
    "# My model\n",
    "axes.plot(fpr_my['micro'], tpr_my['micro'],\n",
    "          label='ROC curve micro [My model]\\n(area = %0.3f)' % roc_auc_my['micro'],\n",
    "          color=cm.magma(0.75), lw=4, alpha=0.8)\n",
    "# CancerSEEK\n",
    "axes.plot(fpr_cS['micro'], tpr_cS['micro'],\n",
    "          label='ROC curve micro [CancerSEEK]\\n(area = %0.3f)' % roc_auc_cS['micro'],\n",
    "          color=cm.magma(0.93), lw=4, alpha=0.8)\n",
    "\n",
    "axes.set_xlim([0.0, 1.0])\n",
    "axes.set_ylim([0.0, 1.0])\n",
    "\n",
    "#ax.set_title('Tumor type : {0}'.format(tumor_dict[k+1]), fontsize=axistitlesize, fontweight='bold')\n",
    "axes.set_xlabel('False Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('True Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='lower right', fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 15. ROC curves to compare the CancerSEEK\\nand my resuls in cancer recognition',\n",
    "             fontsize=axistitlesize+5, y=0.04, color='white')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Scale data and try different kernels\n",
    "\n",
    "- scale your data before applying the SVC model\n",
    "- plot the ROC curve and calculate the confusion matrix for the predictions\n",
    "- do your model perform better or worse after scaling? \n",
    "- try out other kernels (linear, poly) and evaluate the performance of the model the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./a. Scale dataset and test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset\n",
    "X = df[df.columns[2:-3]]\n",
    "y = df[df.columns[-1]]\n",
    "X = pd.DataFrame(scale_data(X), columns=X.columns)\n",
    "\n",
    "# Train-test split as given by task description\n",
    "X_train, X_test = X[::2], X[1::2]\n",
    "y_train, y_test = y[::2], y[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC with regularization parameter `C`=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit the SVC model with C=1\n",
    "svc = SVC(C=1)\n",
    "svc.fit(X_train, y_train);\n",
    "# Make predictions\n",
    "y_pred = svc.predict(X_test)\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y, labels=[1,0],\n",
    "                      title=('Fig. 16. Confusion matrix of the predictions\\nof sklearn\\'s SVC model with C=1\\n' +\n",
    "                             'on the SCALED CancerSEEK tumor recognition dataset.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize labels\n",
    "y = label_binarize(y, classes=np.unique(y))\n",
    "y_test = np.array(y_test)[:, np.newaxis]\n",
    "n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = svc.fit(X_train, y_train).decision_function(X_test)\n",
    "if n_classes == 1:\n",
    "    y_score = y_score[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_my, tpr_my, roc_auc_my = compute_roc(y_test=y_test,\n",
    "                                         y_score=y_score, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 12),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes.plot([0, 1], [0, 1], color=rocket_cmap(0.25), lw=5, linestyle='--')\n",
    "\n",
    "# My model\n",
    "axes.plot(fpr_my['micro'], tpr_my['micro'],\n",
    "          label='ROC curve micro [My model]\\n(area = %0.3f)' % roc_auc_my['micro'],\n",
    "          color=cm.magma(0.75), lw=4, alpha=0.8)\n",
    "# CancerSEEK\n",
    "axes.plot(fpr_cS['micro'], tpr_cS['micro'],\n",
    "          label='ROC curve micro [CancerSEEK]\\n(area = %0.3f)' % roc_auc_cS['micro'],\n",
    "          color=cm.magma(0.93), lw=4, alpha=0.8)\n",
    "\n",
    "axes.set_xlim([0.0, 1.0])\n",
    "axes.set_ylim([0.0, 1.0])\n",
    "\n",
    "#ax.set_title('Tumor type : {0}'.format(tumor_dict[k+1]), fontsize=axistitlesize, fontweight='bold')\n",
    "axes.set_xlabel('False Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('True Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='lower right', fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 17. ROC curves to compare the CancerSEEK\\nand my resuls in cancer recognition with the\\nscaled dataset.',\n",
    "             fontsize=axistitlesize+5, y=0.04, color='white')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The dataset performs slightly better now, and it seems it even outperforms CancerSEEK in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./b. Try other kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset\n",
    "X = df[df.columns[2:-3]]\n",
    "y = df[df.columns[-1]]\n",
    "X = pd.DataFrame(scale_data(X), columns=X.columns)\n",
    "\n",
    "# Train-test split as given by task description\n",
    "X_train, X_test = X[::2], X[1::2]\n",
    "y_train, y_test = y[::2], y[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize labels\n",
    "y = label_binarize(y, classes=np.unique(y))\n",
    "y_test = np.array(y_test)[:, np.newaxis]\n",
    "n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "y_preds = {}\n",
    "fprs = {}\n",
    "tprs = {}\n",
    "roc_aucs = {}\n",
    "for kernel in kernels:\n",
    "    # Define and fit the SVC model with C=1 to predict probabilites\n",
    "    svc = SVC(kernel=kernel, probability=True)\n",
    "    svc.fit(X_train, y_train);\n",
    "    # Make predictions\n",
    "    y_preds[kernel] = svc.predict(X_test)\n",
    "    \n",
    "    # Calculate scores between classes\n",
    "    y_score = svc.fit(X_train, y_train).decision_function(X_test)\n",
    "    y_score = y_score[:, np.newaxis]\n",
    "    \n",
    "    fprs[kernel], tprs[kernel], roc_aucs[kernel] = compute_roc(y_test=y_test,\n",
    "                                                               y_score=y_score, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*11, nrows*11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        k = kernels[i * ncols + j]\n",
    "        ax = axes[i][j]\n",
    "        ax.plot([0, 1], [0, 1], color=rocket_cmap(0.25), lw=5, linestyle='--')\n",
    "\n",
    "        # My model\n",
    "        ax.plot(fprs[k]['micro'], tprs[k]['micro'],\n",
    "                label='ROC curve micro [My model]\\n(area = %0.3f)' % roc_aucs[k]['micro'],\n",
    "                color=cm.magma(0.75), lw=4, alpha=0.8)\n",
    "        # CancerSEEK\n",
    "        ax.plot(fpr_cS['micro'], tpr_cS['micro'],\n",
    "                label='ROC curve micro [CancerSEEK]\\n(area = %0.3f)' % roc_auc_cS['micro'],\n",
    "                color=cm.magma(0.93), lw=4, alpha=0.8)\n",
    "\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "        ax.set_title('Kernel : {0}'.format(kernels[i*ncols + j]), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "        ax.set_xlabel('False Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "        ax.set_ylabel('True Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "        ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "        ax.legend(loc='lower right', fontsize=axislegendsize-5)\n",
    "\n",
    "plt.suptitle('Fig. 18. ROC curves to compare the CancerSEEK and my resuls in cancer recognition with the\\n' +\n",
    "             'scaled dataset. Different kernels for the SVC models are considered and compared between\\n' +\n",
    "             'each of the subplots.',\n",
    "             fontsize=axistitlesize+5, y=0.06, color='white')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) Split the data randomly to 3 parts: 70% train, 15% validation, 15% test data and tune hyperparameters\n",
    "\n",
    "- prepare data as described in the title, then scale all input based on the training set\n",
    "- select your best performing SVC model from the previous exercise\n",
    "- check the behaviour of the SVC by modifying at least 3 of its hyperparameters (C, gamma, ...) and plot the AUC value vs the modified parameter (logscale may be better for visualization)\n",
    "- create plots (at least 2) that shows the train, val and test accuracy based on a given hyperparameter's different values. Is it a good idea to rely on validation data when tuning hyperparameter in this case?\n",
    "- select the best settings, train the SVC and evaluate with reference to CancerSEEK results with the ROC curve and the confusion matrix (match your results with CancerSEEK's results on the same dataset splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./a. Preprocess dataset as given in title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset\n",
    "X = df[df.columns[2:-3]]\n",
    "y = df[df.columns[-1]]\n",
    "\n",
    "train_c = 0.7\n",
    "test_c = 0.15\n",
    "valid_c = 0.15\n",
    "assert train_c + test_c + valid_c == 1, \"Train+Test+Validation should cover the whole dataset!\"\n",
    "random_state = None\n",
    "\n",
    "# Binarize labels\n",
    "y_t = label_binarize(y, classes=np.unique(y))\n",
    "n_classes = y_t.shape[1]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test \\\n",
    "    = train_test_split(X, y, test_size=test_c, random_state=random_state)\n",
    "X_train, X_val, y_train, y_val \\\n",
    "    = train_test_split(X_train, y_train, test_size=valid_c / (train_c + test_c), random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./b. Hyperparameter tuning\n",
    "\n",
    "Possible parameters for the `sklearn.svm.SVC()` class:\n",
    "\n",
    "**`C`**  \n",
    "Default : `1.0`\n",
    "> Scale of regularization. Can be used for any kernels, must be strictly positive.  \n",
    "> **FIRST HYPERPARAMETER TO MODIFY**\n",
    "\n",
    "**`kernel`**  \n",
    "Default : `'rbf'`\n",
    "> Specifies the kernel type to be used in the algorithm, can be callable. Best kernel should be selected according\n",
    "to the task, using the results from the previous task.  \n",
    "*CAN'T BE USED FOR HYPERPARAMETER TUNING HERE*\n",
    "\n",
    "**`degree`**  \n",
    "Default : `3`\n",
    "> Degree of the polynomial kernel function in case of `kernel='poly'` .  \n",
    "> *CAN'T BE USED FOR HYPERPARAMETER TUNING HERE*\n",
    "\n",
    "**`gamma`**  \n",
    "Default : `'scale'`\n",
    "> Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.  \n",
    "> **SECOND HYPERPARAMETER TO MODIFY**\n",
    "\n",
    "**`coef0`**  \n",
    "Default : `0.0`\n",
    ">\n",
    "> *CAN BE USED FOR HP TUNING, BUT I WON'T NOW*\n",
    "\n",
    "**`shrinking`**  \n",
    "Default : `True`\n",
    "> Whether to use the shrinking heuristic.  \n",
    "> **THIRD HYPERPARAMETER TO MODIFY**\n",
    "\n",
    "**`probability`**  \n",
    "Default : `False`\n",
    "> *CAN'T BE USED FOR HYPERPARAMETER TUNING HERE*\n",
    "\n",
    "**`tol`**  \n",
    "Default : `0.001`\n",
    "> Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.\n",
    "> *CAN'T BE USED FOR HYPERPARAMETER TUNING HERE*\n",
    "\n",
    "**`cache_size`**  \n",
    "Default : `200`\n",
    "> \n",
    "> *CAN'T BE USED FOR HYPERPARAMETER TUNING HERE*\n",
    "\n",
    "**`class_weight`**  \n",
    "Default : `None`\n",
    "> \n",
    "> *CAN'T BE USED FOR HYPERPARAMETER TUNING HERE*\n",
    "\n",
    "**`verbose`**  \n",
    "Default : `False`\n",
    "> \n",
    "> *CAN'T BE USED FOR HYPERPARAMETER TUNING HERE*\n",
    "\n",
    "**`max_iter`**  \n",
    "Default : `-1`\n",
    "> \n",
    "> *CAN'T BE USED FOR HYPERPARAMETER TUNING HERE*\n",
    "\n",
    "**`decision_function_shape`**\n",
    "Default : `'ovr'`\n",
    "> \n",
    "> *CAN'T BE USED FOR HYPERPARAMETER TUNING HERE*\n",
    "\n",
    "**`break_ties`**  \n",
    "Default : `False` \n",
    "> \n",
    "> *CAN'T BE USED FOR HYPERPARAMETER TUNING HERE*\n",
    "\n",
    "**`random_state`**  \n",
    "Default : `None`\n",
    "> Set a random seed for the algorithm for reproducibility.  \n",
    "> *CAN'T BE USED FOR HYPERPARAMETER TUNING HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "C_vals = np.linspace(1, 1000, 100)\n",
    "kernel = 'rbf'\n",
    "gammas = ['scale', 'auto']\n",
    "shrinkings = [True, False]\n",
    "random_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "fprs = {}\n",
    "tprs = {}\n",
    "roc_aucs = {}\n",
    "\n",
    "for gamma in gammas:\n",
    "    for shrinking in shrinkings:\n",
    "        key = (gamma, shrinking)\n",
    "        acc = []\n",
    "        #fp = np.zeros(len(C_vals))\n",
    "        #tp = np.zeros(len(C_vals))\n",
    "        roc = np.zeros(len(C_vals))\n",
    "        for i, C in enumerate(C_vals):\n",
    "            svc = SVC(\n",
    "                        C=C,\n",
    "                        kernel=kernel,\n",
    "                        degree=3,\n",
    "                        gamma=gamma,\n",
    "                        coef0=0.0,\n",
    "                        shrinking=shrinking,\n",
    "                        probability=False,\n",
    "                        tol=0.001,\n",
    "                        cache_size=200,\n",
    "                        class_weight=None,\n",
    "                        verbose=False,\n",
    "                        max_iter=-1,\n",
    "                        decision_function_shape='ovr',\n",
    "                        break_ties=False,\n",
    "                        random_state=random_state\n",
    "                     )\n",
    "            svc.fit(X_train, y_train)\n",
    "            # Make predictions for all sets (train, test, valid)\n",
    "            y_pred_train = svc.predict(X_train)\n",
    "            y_pred_test = svc.predict(X_test)\n",
    "            y_pred_val = svc.predict(X_val)\n",
    "            # Calculate and accuracy metric\n",
    "            acc.append({\n",
    "                    'train' : accuracy_metric(y_test=y_train, y_pred=y_pred_train),\n",
    "                    'test' : accuracy_metric(y_test=y_test, y_pred=y_pred_test),\n",
    "                    'val' : accuracy_metric(y_test=y_val, y_pred=y_pred_val)\n",
    "            })\n",
    "\n",
    "            # Calculate scores between classes for the test case\n",
    "            y_score = svc.fit(X_train, y_train).decision_function(X_test)\n",
    "            y_score = y_score[:, np.newaxis]\n",
    "            # Get ROC curve for test data\n",
    "            _, _, r = compute_roc(y_test=np.array(y_test)[:, np.newaxis],\n",
    "                                  y_score=y_score, n_classes=n_classes)\n",
    "            roc[i] = r['micro']\n",
    "        # Add values to corresponding hyperparameter sets\n",
    "        roc_aucs[key] = roc\n",
    "        accuracies[key] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notes\n",
    "\n",
    "Modifying the parameters above, we'll have 4 different sets of hypereparameters, each of them having the same series for `C` values in the following order:\n",
    "1. gamma: `'scale'`; shrinking: `True`  \n",
    "2. gamma: `'scale'`; shrinking: `False`  \n",
    "3. gamma: `'auto'`; shrinking: `True`  \n",
    "4. gamma: `'auto'`; shrinking: `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*11, nrows*11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.25, wspace=0.22)\n",
    "\n",
    "for i, (keys, values) in enumerate(roc_aucs.items()):\n",
    "    m = i // nrows\n",
    "    n = i % ncols\n",
    "    ax = axes[m][n]\n",
    "    \n",
    "    ax.plot(C_vals, values,\n",
    "            color=cm.magma(0.93), lw=4, alpha=0.8)\n",
    "    \n",
    "    ax.set_title('Hyperparams\\n(`gamma`, `shrinkage`):\\n{0}'.format(keys), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "    ax.set_xlabel('Regularization parameter `C`', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('ROC AUC', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "plt.suptitle('Fig. 19. Area under the ROC curve plotted against the`C` regularization parameter',\n",
    "             fontsize=axistitlesize+5, y=0.06, color='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*11, nrows*11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.25, wspace=0.22)\n",
    "\n",
    "for i, (keys, values) in enumerate(accuracies.items()):\n",
    "    m = i // nrows\n",
    "    n = i % ncols\n",
    "    ax = axes[m][n]\n",
    "    \n",
    "    \n",
    "    ax.plot(C_vals, [d['train'] for d in values], label='Train acc.',\n",
    "            color=cm.magma(0.5), lw=4, alpha=0.8)\n",
    "    ax.plot(C_vals, [d['test'] for d in values], label='Test acc.',\n",
    "            color=cm.magma(0.75), lw=4, alpha=0.8)\n",
    "    ax.plot(C_vals, [d['val'] for d in values], label='Validation acc.',\n",
    "            color=cm.magma(0.93), lw=4, alpha=0.8)\n",
    "    \n",
    "    ax.set_title('Hyperparams\\n(`gamma`, `shrinkage`):\\n{0}'.format(keys), fontsize=axistitlesize, fontweight='bold', color='white')\n",
    "    ax.set_xlabel('Regularization parameter `C`', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.set_ylabel('ROC AUC', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "    ax.legend(loc='upper left', fontsize=axislegendsize-5)\n",
    "    \n",
    "plt.suptitle('Fig. 20. Train, Test and Validation accuracy plotted against the`C` regularization parameter',\n",
    "             fontsize=axistitlesize+5, y=0.06, color='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the hyperparameters\n",
    "\n",
    "In the case of the `gamma` parameter set to `auto` the model doesn't learn anything. It is an interesting thing to note for sure.\n",
    "\n",
    "### Note on the test-validation accuracy\n",
    "\n",
    "The validation dataset functions as a smaller test set in this case, and isn't used to explicitly regularize the parameters of the trained model. Thus it simply reduce the number of our train/test data and worsens our model. Using CV would be much more sensible in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./c. Select best model and compare it to CancerSEEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset\n",
    "X = df[df.columns[2:-3]]\n",
    "y = df[df.columns[-1]]\n",
    "\n",
    "test_c = 0.15\n",
    "random_state = None\n",
    "\n",
    "# Binarize labels\n",
    "y_t = label_binarize(y, classes=np.unique(y))\n",
    "n_classes = y_t.shape[1]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test \\\n",
    "    = train_test_split(X, y, test_size=test_c, random_state=random_state)\n",
    "\n",
    "# Convert test data to correct shape\n",
    "y_test = np.array(y_test)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model hyperparams\n",
    "gamma = 'scale'\n",
    "shrinking = True\n",
    "\n",
    "# Calcualte best C\n",
    "roc_values = list(roc_aucs.values())\n",
    "max_local = [np.max(v) for v in roc_values]\n",
    "max_global = np.max(max_local)\n",
    "max_global_loc = max_local.index(max_global)\n",
    "C = C_vals[np.where(roc_values[max_global_loc] == max_global)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit the SVC model with C=1\n",
    "svc = SVC(C=C, gamma=gamma, shrinking=shrinking)\n",
    "svc.fit(X_train, y_train);\n",
    "# Make predictions\n",
    "y_pred = svc.predict(X_test)\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y, labels=[1,0],\n",
    "                      title=('Fig. 21. Confusion matrix of the predictions\\n' +\n",
    "                             'of sklearn\\'s SVC model with optimalized parameters\\n' +\n",
    "                             'on the SCALED CancerSEEK tumor recognition dataset.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = svc.fit(X_train, y_train).decision_function(X_test)\n",
    "if n_classes == 1:\n",
    "    y_score = y_score[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_my, tpr_my, roc_auc_my = compute_roc(y_test=y_test,\n",
    "                                         y_score=y_score, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 12),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes.plot([0, 1], [0, 1], color=rocket_cmap(0.25), lw=5, linestyle='--')\n",
    "\n",
    "# My model\n",
    "axes.plot(fpr_my['micro'], tpr_my['micro'],\n",
    "          label='ROC curve micro [My model]\\n(area = %0.3f)' % roc_auc_my['micro'],\n",
    "          color=cm.magma(0.75), lw=4, alpha=0.8)\n",
    "# CancerSEEK\n",
    "axes.plot(fpr_cS['micro'], tpr_cS['micro'],\n",
    "          label='ROC curve micro [CancerSEEK]\\n(area = %0.3f)' % roc_auc_cS['micro'],\n",
    "          color=cm.magma(0.93), lw=4, alpha=0.8)\n",
    "\n",
    "axes.set_xlim([0.0, 1.0])\n",
    "axes.set_ylim([0.0, 1.0])\n",
    "\n",
    "#ax.set_title('Tumor type : {0}'.format(tumor_dict[k+1]), fontsize=axistitlesize, fontweight='bold')\n",
    "axes.set_xlabel('False Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('True Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='lower right', fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 22. ROC curves to compare the CancerSEEK\\n' +\n",
    "             'and my resuls in cancer recognition with the\\n' +\n",
    "             'best model choosen with hyperparameter tuning.',\n",
    "             fontsize=axistitlesize+5, y=0.04, color='white')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints:\n",
    " - On total you can get 10 points for fully completing all tasks.\n",
    " - Decorate your notebook with, questions, explanation etc, make it self contained and understandable!\n",
    " - Comments you code when necessary\n",
    " - Write functions for repetitive tasks!\n",
    " - Use the pandas package for data loading and handling\n",
    " - Use matplotlib and seaborn for plotting or bokeh and plotly for interactive investigation\n",
    " - Use the scikit learn package for almost everything\n",
    " - Use for loops only if it is really necessary!\n",
    " - Code sharing is not allowed between student! Sharing code will result in zero points.\n",
    " - If you use code found on web, it is OK, but, make its source clear! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
