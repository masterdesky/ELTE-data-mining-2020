{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. SVC methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/'\n",
    "out = './out/'\n",
    "\n",
    "# Bold print for Jupyter Notebook\n",
    "b1 = '\\033[1m'\n",
    "b0 = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some matplotlib and seaborn parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "axistitlesize = 20\n",
    "axisticksize = 17\n",
    "axislabelsize = 26\n",
    "axislegendsize = 23\n",
    "axistextsize = 20\n",
    "axiscbarfontsize = 15\n",
    "\n",
    "# Set axtick dimensions\n",
    "major_size = 6\n",
    "major_width = 1.2\n",
    "minor_size = 3\n",
    "minor_width = 1\n",
    "mpl.rcParams['xtick.major.size'] = major_size\n",
    "mpl.rcParams['xtick.major.width'] = major_width\n",
    "mpl.rcParams['xtick.minor.size'] = minor_size\n",
    "mpl.rcParams['xtick.minor.width'] = minor_width\n",
    "mpl.rcParams['ytick.major.size'] = major_size\n",
    "mpl.rcParams['ytick.major.width'] = major_width\n",
    "mpl.rcParams['ytick.minor.size'] = minor_size\n",
    "mpl.rcParams['ytick.minor.width'] = minor_width\n",
    "\n",
    "mpl.rcParams.update({'figure.autolayout': False})\n",
    "\n",
    "# Seaborn style settings\n",
    "sns.set_style({'axes.axisbelow': True,\n",
    "               'axes.edgecolor': '.8',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': True,\n",
    "               'axes.labelcolor': '.15',\n",
    "               'axes.spines.bottom': True,\n",
    "               'axes.spines.left': True,\n",
    "               'axes.spines.right': True,\n",
    "               'axes.spines.top': True,\n",
    "               'figure.facecolor': 'white',\n",
    "               'font.family': ['sans-serif'],\n",
    "               'font.sans-serif': ['Arial',\n",
    "                'DejaVu Sans',\n",
    "                'Liberation Sans',\n",
    "                'Bitstream Vera Sans',\n",
    "                'sans-serif'],\n",
    "               'grid.color': '.8',\n",
    "               'grid.linestyle': '--',\n",
    "               'image.cmap': 'rocket',\n",
    "               'lines.solid_capstyle': 'round',\n",
    "               'patch.edgecolor': 'w',\n",
    "               'patch.force_edgecolor': True,\n",
    "               'text.color': '.15',\n",
    "               'xtick.bottom': True,\n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'in',\n",
    "               'xtick.top': True,\n",
    "               'ytick.color': '.15',\n",
    "               'ytick.direction': 'in',\n",
    "               'ytick.left': True,\n",
    "               'ytick.right': True})\n",
    "\n",
    "# Colorpalettes, colormaps, etc.\n",
    "sns.set_palette(palette='rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Linear SVC in case of linear separation\n",
    "\n",
    "- load the Iris dataset (can be found in sklearn API)\n",
    "- scale the data and plot the petal length vs petal width in a scatterplot colored with the target\n",
    "- train an SVC model with linear kernel with default parameter settings, but once with C=1 and then C=1000\n",
    "- visualize the model's decision boundary and the margins based on the coefficients learnt by the model\n",
    "- interpret the results, what is the role of the C hyperparameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X):\n",
    "    \"\"\"\n",
    "    Normalize the data to have zero mean and unit variance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray or array-like in shape of (N, M)\n",
    "        The unscaled dataset.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray in shape of (N, M)\n",
    "        The already scaled dataset with zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    scaler = StandardScaler()\n",
    "    # Compute the mean and standard dev. and scale the dataset `X`\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Linear SVC but non-linear separation\n",
    "\n",
    "- create a dataset with the following: X, y = sklearn.datasets.make_moons(noise=0.1, random_state=0)\n",
    "- perform the same steps just as in the previous exercise and use the linear kernel for  the SVC\n",
    "- since linear SVC cannot do non-linear separation, you will need to do some workaround, for example adding polynomial features (3rd order would be a good choice)\n",
    "- write down with your own words in few sentences how the support vector machine works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Load the dataset from 2 weeks ago and build/evaluate the SVC with default settings\n",
    "\n",
    "Reminder:\n",
    "- you need to build a classifier that predicts the probability of a sample coming from a cancerous (tumor type is normal or not) person based on the measured protein levels\n",
    "- train the SVM classifier (SVC in sklearn API) on every second sample (not first 50% of the data (!), use every second line)\n",
    "- generate prediction for the samples that were not used during the training\n",
    "\n",
    "To-do now:\n",
    "- build default SVC, but set it to predict probabilities\n",
    "- plot the ROC curve and calculate the confusion matrix for the predictions\n",
    "- do the same for the CancerSEEK predictions and compare your model's performance to CancerSEEK performance (as a reference, plot it on the same figure)\n",
    "- how good is the performance of the new model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Scale data and try different kernels\n",
    "\n",
    "- scale your data before applying the SVC model\n",
    "- plot the ROC curve and calculate the confusion matrix for the predictions\n",
    "- do your model perform better or worse after scaling? \n",
    "- try out other kernels (linear, poly) and evaluate the performance of the model the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) Split the data randomly to 3 parts: 70% train, 15% validation, 15% test data and tune hyperparameters\n",
    "\n",
    "- prepare data as described in the title, then scale all input based on the training set\n",
    "- select your best performing SVC model from the previous exercise\n",
    "- check the behaviour of the SVC by modifying at least 3 of its hyperparameters (C, gamma, ...) and plot the AUC value vs the modified parameter (logscale may be better for visualization)\n",
    "- create plots (at least 2) that shows the train, val and test accuracy based on a given hyperparameter's different values. Is it a good idea to rely on validation data when tuning hyperparameter in this case?\n",
    "- select the best settings, train the SVC and evaluate with reference to CancerSEEK results with the ROC curve and the confusion matrix (match your results with CancerSEEK's results on the same dataset splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints:\n",
    " - On total you can get 10 points for fully completing all tasks.\n",
    " - Decorate your notebook with, questions, explanation etc, make it self contained and understandable!\n",
    " - Comments you code when necessary\n",
    " - Write functions for repetitive tasks!\n",
    " - Use the pandas package for data loading and handling\n",
    " - Use matplotlib and seaborn for plotting or bokeh and plotly for interactive investigation\n",
    " - Use the scikit learn package for almost everything\n",
    " - Use for loops only if it is really necessary!\n",
    " - Code sharing is not allowed between student! Sharing code will result in zero points.\n",
    " - If you use code found on web, it is OK, but, make its source clear! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
