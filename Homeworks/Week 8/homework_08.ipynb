{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Decision tree based models\n",
    "\n",
    "This week we will use the https://archive.ics.uci.edu/ml/datasets/Early+stage+diabetes+risk+prediction+dataset.# diabetes dataset. Our goal is to classify people based on their symptoms if they have diabetes or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/'\n",
    "out = './output/'\n",
    "\n",
    "# Bold print for Jupyter Notebook\n",
    "b1 = '\\033[1m'\n",
    "b0 = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some matplotlib and seaborn parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axistitlesize = 20\n",
    "axisticksize = 17\n",
    "axislabelsize = 26\n",
    "axislegendsize = 23\n",
    "axistextsize = 20\n",
    "axiscbarfontsize = 15\n",
    "\n",
    "# Set axtick dimensions\n",
    "major_size = 6\n",
    "major_width = 1.2\n",
    "minor_size = 3\n",
    "minor_width = 1\n",
    "mpl.rcParams['xtick.major.size'] = major_size\n",
    "mpl.rcParams['xtick.major.width'] = major_width\n",
    "mpl.rcParams['xtick.minor.size'] = minor_size\n",
    "mpl.rcParams['xtick.minor.width'] = minor_width\n",
    "mpl.rcParams['ytick.major.size'] = major_size\n",
    "mpl.rcParams['ytick.major.width'] = major_width\n",
    "mpl.rcParams['ytick.minor.size'] = minor_size\n",
    "mpl.rcParams['ytick.minor.width'] = minor_width\n",
    "\n",
    "mpl.rcParams.update({'figure.autolayout': False})\n",
    "\n",
    "# Seaborn style settings\n",
    "sns.set_style({'axes.axisbelow': True,\n",
    "               'axes.edgecolor': '.8',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': True,\n",
    "               'axes.labelcolor': '.15',\n",
    "               'axes.spines.bottom': True,\n",
    "               'axes.spines.left': True,\n",
    "               'axes.spines.right': True,\n",
    "               'axes.spines.top': True,\n",
    "               'figure.facecolor': 'white',\n",
    "               'font.family': ['sans-serif'],\n",
    "               'font.sans-serif': ['Arial',\n",
    "                'DejaVu Sans',\n",
    "                'Liberation Sans',\n",
    "                'Bitstream Vera Sans',\n",
    "                'sans-serif'],\n",
    "               'grid.color': '.8',\n",
    "               'grid.linestyle': '--',\n",
    "               'image.cmap': 'rocket',\n",
    "               'lines.solid_capstyle': 'round',\n",
    "               'patch.edgecolor': 'w',\n",
    "               'patch.force_edgecolor': True,\n",
    "               'text.color': '.15',\n",
    "               'xtick.bottom': True,\n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'in',\n",
    "               'xtick.top': True,\n",
    "               'ytick.color': '.15',\n",
    "               'ytick.direction': 'in',\n",
    "               'ytick.left': True,\n",
    "               'ytick.right': True})\n",
    "\n",
    "# Colorpalettes, colormaps, etc.\n",
    "sns.set_palette(palette='rocket')\n",
    "rocket_cmap = sns.color_palette('rocket', as_cmap=True)\n",
    "\n",
    "target_colors = np.array([cm.magma(0.5), cm.magma(0.75), cm.magma(0.93)])\n",
    "feature_colors = np.array([rocket_cmap(0.17), cm.magma(0.45), cm.magma(0.60), cm.magma(0.75)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare dataset\n",
    " - load the diabetes_data_upload.csv dataset\n",
    " - search for missing values and if needed, handle them!\n",
    " - encode the non numeric variables into numeric ones! For the binary features simply encode them as (0/1), do not create two separate columns for them!1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data + 'diabetes_data_upload.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./b. Check for and handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values fortunately in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./c. Convert non-numeric entries to numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n = df.copy()\n",
    "\n",
    "# 1, Convert Female/Male to 1/0\n",
    "c = 'Gender'\n",
    "df_n[c] = df[c].map({'Female' : 1, 'Male' : 0})\n",
    "\n",
    "# 2. Convert Yes/No to 1/0\n",
    "for c in df_n.columns[2:-1]:\n",
    "    df_n[c] = df[c].map({'Yes' : 1, 'No' : 0})\n",
    "    \n",
    "# 3. Convert Positive/Negative to 1/0\n",
    "c = 'class'\n",
    "df_n[c] = df[c].map({'Positive' : 1, 'Negative' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_n.head())\n",
    "display(df_n.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train & visualize decision tree classifier\n",
    " - train a decision tree classifier using the sklearn API\n",
    " - use its default parameters\n",
    " - for training use all the data, this is only an exploratory task now\n",
    " - visualize the decision tree (the `plot_tree` function in sklearn will be helpful)\n",
    " - manually check for two cases if the returned Gini impurities are correct\n",
    " - in a few sentences discuss the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./a. Create train and target datasets and fit DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_n.columns[:-1]\n",
    "target = df_n.columns[-1]\n",
    "X = df_n[features]\n",
    "y = df_n[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DTC modell\n",
    "dtc = DecisionTreeClassifier()\n",
    "# Fit modell\n",
    "dtc.fit(X, y);\n",
    "# Get useful fitted parameteres\n",
    "feature_imp = pd.Series(dtc.feature_importances_,\n",
    "                        index=features) # Feature importance\n",
    "gini_imp = dtc.tree_.impurity           # Gini impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fimp, best_f = zip(*sorted(zip(feature_imp, features), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16,11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes.bar(best_f[:10], best_fimp[:10], width=0.9, label='Best 10',\n",
    "         color=cm.magma(0.93), edgecolor='black')\n",
    "axes.bar(best_f[10:], best_fimp[10:], width=0.9, label='Remaining',\n",
    "         color=cm.magma(0.25), edgecolor='black')\n",
    "\n",
    "axes.set_xlabel('Feature names', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('Feature value', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='x', which='major', colors='white',\n",
    "                 labelsize=axisticksize, labelrotation=75)\n",
    "axes.tick_params(axis='y', which='major', colors='white',\n",
    "                 labelsize=axisticksize)\n",
    "\n",
    "axes.legend(loc='upper right', fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 1. Features importance given by a DecisionTreeClassifier.',\n",
    "             fontsize=axistitlesize+5, y=-0.14, color='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./b. Visualize decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to graphviz\n",
    "dot_data = tree.export_graphviz(dtc, out_file=None,\n",
    "                                feature_names=features,\n",
    "                                class_names=['Negative', 'Positive'], # [0, 1]\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True,\n",
    "                                precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to svg file\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(filename='DTC_tree',\n",
    "             directory=out,\n",
    "             view=False,\n",
    "             format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to plot\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./c. Check Gini impurities for binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities of occureces\n",
    "p1 = df_n[features[1:]].sum() / len(df_n)\n",
    "p0 = 1 - p1\n",
    "\n",
    "# Calculate Gini impurity\n",
    "g = 2 * p0 * p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the graph above, we can find the `gini` values for every node in the decision tree. These values however do not match with my calculations. Eg. the Gini value for `Polyuria` is $0.473$ on the graph for $520$ samples, while I got practically $0.5$. Also eg. `Irritablity` sits on a solid $0.14$ for $171$ samples on the graph, while my calculations shows $0.367$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Random forest feature importance\n",
    " - train a random forest classifier on all the data using the sklearn API\n",
    " - use default values again, but fix the random_state to 42!\n",
    " - plot the 10 most important features' importances\n",
    "    - create a bar plot where the height of the bar is the feature importance\n",
    "    - show the 10 features where the feature importance is the highest\n",
    "    - `feature_importance` attribute is helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RFC modell\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "# Fit modell\n",
    "rfc.fit(X, y);\n",
    "# Get useful fitted parameteres\n",
    "feature_imp = pd.Series(rfc.feature_importances_,\n",
    "                        index=features) # Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fimp, best_f = zip(*sorted(zip(feature_imp, features), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16,11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes.bar(best_f[:10], best_fimp[:10], width=0.9, label='Best 10',\n",
    "         color=cm.magma(0.93), edgecolor='black')\n",
    "axes.bar(best_f[10:], best_fimp[10:], width=0.9, label='Remaining',\n",
    "         color=cm.magma(0.25), edgecolor='black')\n",
    "\n",
    "axes.set_xlabel('Feature names', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('Feature value', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='x', which='major', colors='white',\n",
    "                 labelsize=axisticksize, labelrotation=75)\n",
    "axes.tick_params(axis='y', which='major', colors='white',\n",
    "                 labelsize=axisticksize)\n",
    "\n",
    "axes.legend(loc='upper right', fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 2. Features importance given by a RandomForestClassifier.',\n",
    "             fontsize=axistitlesize+5, y=-0.14, color='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    " - generate prediction probabilities with a decision tree and with a random forest model\n",
    " - use 5 fold cross validation for both time (so you should get 520 predictions)\n",
    " - use default parameters for both models\n",
    " - compare the two models with ROC curves\n",
    "   - why does the decision tree's ROC curve looks different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./prev. Get 5-fold CV scores for both model to get some insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_n.columns[:-1]\n",
    "target = df_n.columns[-1]\n",
    "X = df_n[features]\n",
    "y = df_n[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DecisionTreeClassifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds\n",
    "folds = 5\n",
    "# Invoke the KFold class from sklearn for CV tests\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# The model we use is linear regression\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test R^2 score\n",
    "# Refrence: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "scores = cross_val_score(model, X, y, scoring='r2', cv=cv)\n",
    "\n",
    "print('KFOLD SCORES:\\n' +\n",
    "      '----------------')\n",
    "print(scores)\n",
    "print('Mean of scores : {0:.4f}'.format(np.mean(scores)))\n",
    "print('Std of scores : {0:.4f}'.format(np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `RandomForestClassifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds\n",
    "folds = 5\n",
    "# Invoke the KFold class from sklearn for CV tests\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# The model we use is linear regression\n",
    "model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test R^2 score\n",
    "# Refrence: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "scores = cross_val_score(model, X, y, scoring='r2', cv=cv)\n",
    "\n",
    "print('KFOLD SCORES:\\n' +\n",
    "      '----------------')\n",
    "print(scores)\n",
    "print('Mean of scores : {0:.4f}'.format(np.mean(scores)))\n",
    "print('Std of scores : {0:.4f}'.format(np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./a. Generate predictions for 5-fold CV and compare ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of model prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test : array-like of shape (N, )\n",
    "        Original labels of the test dataset.\n",
    "    \n",
    "    y_pred : array-like of shape (N, )\n",
    "        Predicted labels of the test dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Accuracy of model in reference of the true test labels.\n",
    "    \"\"\"\n",
    "    # Binarize labels\n",
    "    y_test = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    y_pred = label_binarize(y_pred, classes=np.unique(y_pred))\n",
    "\n",
    "    correct = 0\n",
    "    for (t, p) in zip(y_test, y_pred):\n",
    "        if all(t == p):\n",
    "            correct += 1\n",
    "    return correct / len(y_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_mat, y, labels=None, title=None):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(figsize=(10,10))\n",
    "    axes.set_aspect('equal')\n",
    "\n",
    "    im = axes.imshow(conf_mat)\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for X in range(conf_mat.shape[0]):\n",
    "        for Y in range(conf_mat.shape[1]):\n",
    "            axes.text(Y, X, conf_mat[X, Y], fontsize=30,\n",
    "                      ha='center', va='center', color='white', fontweight='bold', \n",
    "                      bbox=dict(color=np.array((0,0,0,0.2)), lw=0)\n",
    "                     )\n",
    "\n",
    "    # Set axis tick locations and labels\n",
    "    ticks = [i for i in range(len(set(y)))]\n",
    "    if labels is None:\n",
    "        ticklabels = [i+1 for i in range(len(set(y)))]\n",
    "    else:\n",
    "        ticklabels = list(labels)\n",
    "\n",
    "    axes.set_xticks(ticks)\n",
    "    axes.set_xticklabels(ticklabels)\n",
    "    axes.set_yticks(ticks)\n",
    "    axes.set_yticklabels(ticklabels)\n",
    "\n",
    "    axes.set_xlabel('Predicted labels', fontsize=axislabelsize+5, fontweight='bold')\n",
    "    axes.set_ylabel('True labels', fontsize=axislabelsize+5, fontweight='bold')\n",
    "    axes.tick_params(axis='both', which='major', labelsize=axisticksize+5)\n",
    "    axes.xaxis.tick_top()\n",
    "    axes.xaxis.set_label_position('top') \n",
    "\n",
    "    axes.grid(False)\n",
    "\n",
    "    # Create an axis on the right side of `axes`. The width of `cax` will be 5%\n",
    "    # of `axes` and the padding between `cax` and axes will be fixed at 0.1 inch\n",
    "    divider = make_axes_locatable(axes)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.1)\n",
    "    cbar = plt.colorbar(mappable=im, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=axiscbarfontsize, colors='black')\n",
    "    cbar.set_label('Number of occurences', fontsize=axiscbarfontsize+10, labelpad=15, rotation=90)\n",
    "\n",
    "    plt.suptitle(title,\n",
    "                 fontsize=axistitlesize+5, y=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc(estimator, X, y):\n",
    "    \"\"\"\n",
    "    Creates the ROC curve and computes AUC values an input X-y data-target set.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    estimator : estimator instance\n",
    "        Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
    "        in which the last estimator is a classifier.\n",
    "    \n",
    "    X : {array-like, sparse matrix} of shape (n_test, n_features)\n",
    "        Input values.\n",
    "    \n",
    "    y : array-like of shape (n_train,)\n",
    "        Target values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fpr : ndarray\n",
    "        False positive rates.\n",
    "    tpr : ndarray\n",
    "        True positive rates.\n",
    "    roc_auc : float\n",
    "        Area under ROC curves.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import _plot\n",
    "    \n",
    "    # Calculate scores for ROC curve\n",
    "    # Small hack because I want to automate it. Also I have no trust in S.O. currently.\n",
    "    # Using this: https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/metrics/_plot/roc_curve.py#L114\n",
    "    c_method = _plot.base._check_classifer_response_method(estimator,\n",
    "                                                           response_method='auto')\n",
    "    y_score = c_method(X)\n",
    "\n",
    "    # Calculate ROC and AUC values\n",
    "    pos_label = estimator.classes_[1]\n",
    "    if y_score.ndim != 1:\n",
    "        y_score = y_score[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y, y_score, pos_label=pos_label)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DecisionTreeClassifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds\n",
    "folds = 5\n",
    "# Invoke the KFold class from sklearn for CV tests\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# The model we use is linear regression\n",
    "model = DecisionTreeClassifier()\n",
    "# Fit model\n",
    "model.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(model, X, y, cv=cv,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y, y_pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y, labels=[0,1],\n",
    "                      title=('Fig. 3. Confusion matrix of the predictions\\n' +\n",
    "                             'on the test set of sklearn\\'s DTC model,\\n' +\n",
    "                             'with 5-fold cross-validation.\\n'\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, roc_auc = compute_roc(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(11, 11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes.plot([0, 1], [0, 1], color=rocket_cmap(0.25), lw=5, linestyle='--')\n",
    "\n",
    "# My model\n",
    "axes.plot(fpr, tpr,\n",
    "          label='ROC curve \\n(AUC = %0.3f)' % roc_auc,\n",
    "          color=cm.magma(0.75), lw=4, alpha=0.8)\n",
    "\n",
    "axes.set_xlim([-0.02, 1.02])\n",
    "axes.set_ylim([-0.02, 1.02])\n",
    "\n",
    "axes.set_xlabel('False Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('True Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='lower right', fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 4. ROC curve of the DecisionTreeClassifier,\\n' +\n",
    "             'with 5-fold cross-validation',\n",
    "             fontsize=axistitlesize+5, y=0.04, color='white')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `RandomForestClassifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds\n",
    "folds = 5\n",
    "# Invoke the KFold class from sklearn for CV tests\n",
    "cv = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "# The model we use is linear regression\n",
    "model = RandomForestClassifier()\n",
    "# Fit for train\n",
    "model.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(model, X, y, cv=cv,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y, y_pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y, labels=[0,1],\n",
    "                      title=('Fig. 5. Confusion matrix of the predictions\\n' +\n",
    "                             'on the test set of sklearn\\'s RFC model,\\n' +\n",
    "                             'with 5-fold cross-validation.\\n'\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, roc_auc = compute_roc(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(11, 11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes.plot([0, 1], [0, 1], color=rocket_cmap(0.25), lw=5, linestyle='--')\n",
    "\n",
    "# My model\n",
    "axes.plot(fpr, tpr,\n",
    "          label='ROC curve \\n(AUC = %0.3f)' % roc_auc,\n",
    "          color=cm.magma(0.75), lw=4, alpha=0.8)\n",
    "\n",
    "axes.set_xlim([-0.02, 1.02])\n",
    "axes.set_ylim([-0.02, 1.02])\n",
    "\n",
    "axes.set_xlabel('False Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('True Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='lower right', fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 6. ROC curve of the RandomForestClassifier,\\n' +\n",
    "             'with 5-fold cross-validation.',\n",
    "             fontsize=axistitlesize+5, y=0.04, color='white')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./c. \"Why does the Decision tree's ROC curves looks different?\"\n",
    "\n",
    "Looking at my ROC curves, I'm scared to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tuning model\n",
    "  - using 80/20% train/test split generate predictions for a random forest model\n",
    "  - plot the AUC vs number of trees in the forest for both the traing and the test data\n",
    "  - do we experience overfitting if we use too many trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./a. Fit an RFC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_n.columns[:-1]\n",
    "target = df_n.columns[-1]\n",
    "X = df_n[features]\n",
    "y = df_n[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RFC model\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# Fit model\n",
    "rfc.fit(X_train, y_train);\n",
    "# Create predictions\n",
    "y_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./b. Analize results for test set\n",
    "\n",
    "- Confusion matrix\n",
    "- AUC, ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y, labels=[0,1],\n",
    "                      title=('Fig. 7. Confusion matrix of the predictions\\n' +\n",
    "                             'on the test set of sklearn\\'s RFC model.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_tr, tpr_tr, roc_auc_tr = compute_roc(rfc, X_train, y_train)\n",
    "fpr_ts, tpr_ts, roc_auc_ts = compute_roc(rfc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(11, 11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes.plot([0, 1], [0, 1], color=rocket_cmap(0.25), lw=5, linestyle='--')\n",
    "\n",
    "# My model\n",
    "axes.plot(fpr_tr, tpr_tr,\n",
    "          label='ROC curve [train] \\n(AUC = %0.3f)' % roc_auc_tr,\n",
    "          color=cm.magma(0.75), lw=4, alpha=0.8)\n",
    "axes.plot(fpr_ts, tpr_ts,\n",
    "          label='ROC curve [test] \\n(AUC = %0.3f)' % roc_auc_ts,\n",
    "          color=cm.magma(0.93), lw=4, alpha=0.8)\n",
    "\n",
    "axes.set_xlim([-0.02, 1.02])\n",
    "axes.set_ylim([-0.02, 1.02])\n",
    "\n",
    "axes.set_xlabel('False Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('True Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='lower right', fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 8. ROC curve of the RandomForestClassifier,\\n' +\n",
    "             'both for train and test values.',\n",
    "             fontsize=axistitlesize+5, y=0.04, color='white')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./c. Does it overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_n.columns[:-1]\n",
    "target = df_n.columns[-1]\n",
    "X = df_n[features]\n",
    "y = df_n[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RFC model\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# Fit model\n",
    "rfc.fit(X_train, y_train);\n",
    "# Create predictions\n",
    "y_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test, y_pred=y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y, labels=[0,1],\n",
    "                      title=('Fig. 9. Confusion matrix of the predictions\\n' +\n",
    "                             'on the test set of the RandomForestClassifier,\\n' +\n",
    "                             'to test overfit with 95% - 5% train-test ratio.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_tr, tpr_tr, roc_auc_tr = compute_roc(rfc, X_train, y_train)\n",
    "fpr_ts, tpr_ts, roc_auc_ts = compute_roc(rfc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(11, 11),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "axes.plot([0, 1], [0, 1], color=rocket_cmap(0.25), lw=5, linestyle='--')\n",
    "\n",
    "# My model\n",
    "axes.plot(fpr_tr, tpr_tr,\n",
    "          label='ROC curve [train] \\n(AUC = %0.3f)' % roc_auc_tr,\n",
    "          color=cm.magma(0.75), lw=4, alpha=0.8)\n",
    "axes.plot(fpr_ts, tpr_ts,\n",
    "          label='ROC curve [test] \\n(AUC = %0.3f)' % roc_auc_ts,\n",
    "          color=cm.magma(0.93), lw=4, alpha=0.8)\n",
    "\n",
    "axes.set_xlim([-0.02, 1.02])\n",
    "axes.set_ylim([-0.02, 1.02])\n",
    "\n",
    "axes.set_xlabel('False Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.set_ylabel('True Positive Rate', fontsize=axislabelsize, fontweight='bold', color='white')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize, colors='white')\n",
    "\n",
    "axes.legend(loc='lower right', fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 10. ROC curve of the RandomForestClassifier,\\n' +\n",
    "             'both for train and test values to test overfit with\\n' +\n",
    "             '95% - 5% train-test ratio.',\n",
    "             fontsize=axistitlesize+5, y=0.04, color='white')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion for overfitting\n",
    "\n",
    "No, it doesn't seem so from these graphs..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints:\n",
    " - On total you can get 10 points for fully completing all tasks.\n",
    " - Decorate your notebook with, questions, explanation etc, make it self contained and understandable!\n",
    " - Comments you code when necessary\n",
    " - Write functions for repetitive tasks!\n",
    " - Use the pandas package for data loading and handling\n",
    " - Use matplotlib and seaborn for plotting or bokeh and plotly for interactive investigation\n",
    " - Use the scikit learn package for almost everything\n",
    " - Use for loops only if it is really necessary!\n",
    " - Code sharing is not allowed between student! Sharing code will result in zero points.\n",
    " - If you use code found on web, it is OK, but, make its source clear! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
