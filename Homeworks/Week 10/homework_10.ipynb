{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Convolutional Neural Networks\n",
    "\n",
    "Please work on Google Colab. (No.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw_utils import *\n",
    "from hw_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import tensorflow.keras.callbacks as kc\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/'\n",
    "out = './out/'\n",
    "\n",
    "# Bold print for Jupyter Notebook\n",
    "b1 = '\\033[1m'\n",
    "b0 = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some matplotlib and seaborn parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axistitlesize = 20\n",
    "axisticksize = 17\n",
    "axislabelsize = 26\n",
    "axislegendsize = 23\n",
    "axistextsize = 20\n",
    "axiscbarfontsize = 15\n",
    "\n",
    "# Set axtick dimensions\n",
    "major_size = 6\n",
    "major_width = 1.2\n",
    "minor_size = 3\n",
    "minor_width = 1\n",
    "mpl.rcParams['xtick.major.size'] = major_size\n",
    "mpl.rcParams['xtick.major.width'] = major_width\n",
    "mpl.rcParams['xtick.minor.size'] = minor_size\n",
    "mpl.rcParams['xtick.minor.width'] = minor_width\n",
    "mpl.rcParams['ytick.major.size'] = major_size\n",
    "mpl.rcParams['ytick.major.width'] = major_width\n",
    "mpl.rcParams['ytick.minor.size'] = minor_size\n",
    "mpl.rcParams['ytick.minor.width'] = minor_width\n",
    "\n",
    "mpl.rcParams.update({'figure.autolayout': False})\n",
    "\n",
    "# Seaborn style settings\n",
    "sns.set_style({'axes.axisbelow': True,\n",
    "               'axes.edgecolor': '.8',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': True,\n",
    "               'axes.labelcolor': '.15',\n",
    "               'axes.spines.bottom': True,\n",
    "               'axes.spines.left': True,\n",
    "               'axes.spines.right': True,\n",
    "               'axes.spines.top': True,\n",
    "               'figure.facecolor': 'white',\n",
    "               'font.family': ['sans-serif'],\n",
    "               'font.sans-serif': ['Arial',\n",
    "                'DejaVu Sans',\n",
    "                'Liberation Sans',\n",
    "                'Bitstream Vera Sans',\n",
    "                'sans-serif'],\n",
    "               'grid.color': '.8',\n",
    "               'grid.linestyle': '--',\n",
    "               'image.cmap': 'rocket',\n",
    "               'lines.solid_capstyle': 'round',\n",
    "               'patch.edgecolor': 'w',\n",
    "               'patch.force_edgecolor': True,\n",
    "               'text.color': '.15',\n",
    "               'xtick.bottom': True,\n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'in',\n",
    "               'xtick.top': True,\n",
    "               'ytick.color': '.15',\n",
    "               'ytick.direction': 'in',\n",
    "               'ytick.left': True,\n",
    "               'ytick.right': True})\n",
    "\n",
    "# Colorpalettes, colormaps, etc.\n",
    "sns.set_palette(palette='rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the MNIST dataset and create a CNN model\n",
    "\n",
    "- load the MNIST dataset from the tensorflow/keras built-in dataset (just like last time)\n",
    "- use the original train/test split!\n",
    "- divide each pixel's value by 255 and now do not reshape, leave it as is (2D matrix (28x28) )\n",
    "- eg for the test set you will have a (10000, 28, 28) shaped vector\n",
    "- train the following network on the training set and generate prediction for the 10.000 test images:\n",
    "\n",
    "        input (28, 28)\n",
    "        conv2D, 16 kernels, kernel size = 3, valid padding, relu activation\n",
    "        conv2D, 16 kernels, kernel size = 3, valid padding, relu activation\n",
    "        maxpooling kernel size = 2*2\n",
    "        conv2D, 32 kernels, kernel size = 3, valid padding, relu activation\n",
    "        conv2D, 32 kernels, kernel size = 3, valid padding, relu activation\n",
    "        maxpooling kernel size = 2*2\n",
    "        flatten\n",
    "        dense, 10 neurons, softmax activation\n",
    "    * pay attention to channel format, you will need to expand dims!\n",
    "    * how many parameters do we have for each layer?\n",
    "    * use Adam optimizer with default parameters\n",
    "    * use categorical crossentropy as loss function\n",
    "    * compile the model\n",
    "    * print out a summary of the model\n",
    "    * train the CNN on the training data for 5 epochs with batch size of 32\n",
    "    * use the test data as validation data\n",
    "        \n",
    "- calculate the categorical cross-entropy loss and the accuracy! Hint: you should get at least ~98% accuracy\n",
    "- show the confusion matrix of the predictions (predicted values vs actual labels)\n",
    "- where does the model make mistakes? Where does it improve compared to fully connected nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./a. Load and preprocess the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape :', X_train.shape)\n",
    "print('y_train shape :', y_train.shape)\n",
    "print('X_test shape :', X_test.shape)\n",
    "print('y_test shape :', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at some random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 8\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*2, nrows*2),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "rand_idx = np.random.randint(0, len(X_train), size=nrows*ncols)\n",
    "images = X_train[rand_idx]\n",
    "labels = y_train[rand_idx]\n",
    "\n",
    "for i, ax in enumerate(axes.reshape(-1)):\n",
    "    ax.imshow(images[i], cmap='Greys_r')\n",
    "    ax.set_title('Label : {0}'.format(labels[i]), fontweight='bold',\n",
    "                 color='white', pad=0)\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.suptitle('Fig. 1. Sample data along with their labels of the MNIST dataset.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.1)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert labels to one-hot encoded arrays and scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# Reshape data\n",
    "X_train = X_train.reshape((-1, 28,28, 1))\n",
    "X_test = X_test.reshape((-1, 28,28, 1))\n",
    "# Convert labels to one-hot encoded arrays\n",
    "y_train = label_binarize(y_train, classes=np.unique(y_train))\n",
    "y_test = label_binarize(y_test, classes=np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape :', X_train.shape)\n",
    "print('y_train shape :', y_train.shape)\n",
    "print('X_test shape :', X_test.shape)\n",
    "print('y_test shape :', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./b. Define model for task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    # Create checkpoint file to save best model into\n",
    "    best_model_ex1 = kc.ModelCheckpoint('./models/best_model_ex1.hdf5', save_best_only=True, verbose=1)\n",
    "    \n",
    "    # Define the model\n",
    "    cnn_model_ex1 = cnn_model(imsize=28, stride=1, kernelsize=3,\n",
    "                              n_channels=1, num_of_filters=16, reg=5e-5,\n",
    "                              padding='valid', activation='relu', n_class=10,\n",
    "                              model_name='model_ex1')\n",
    "    \n",
    "    # Configure the model\n",
    "    cnn_model_ex1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "                          loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_ex1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./c. Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_ex1 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    history_ex1 = cnn_model_ex1.fit(x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    batch_size=32,\n",
    "                                    epochs=epochs_ex1,\n",
    "                                    verbose=1,\n",
    "                                    validation_split=0.2,\n",
    "                                    shuffle=True,\n",
    "                                    callbacks=[best_model_ex1],\n",
    "                                    initial_epoch=0,\n",
    "                                    steps_per_epoch=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./d. Evaluate loss and accuracy history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*12, nrows*8),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "# LOSS GRAPH\n",
    "ax = axes[0]\n",
    "ax.plot(history_ex1.epoch, history_ex1.history['loss'], label='Training loss',\n",
    "        c=cm.magma(0.75), lw=5)\n",
    "ax.plot(history_ex1.epoch, history_ex1.history['val_loss'], label='Validation loss',\n",
    "        c=cm.magma(0.93), lw=5)\n",
    "ax.set_ylabel('Loss', fontsize=axislabelsize, fontweight='bold',\n",
    "              color='white')\n",
    "\n",
    "# ACCURACY GRAPH\n",
    "ax = axes[1]\n",
    "ax.plot(history_ex1.epoch, history_ex1.history['accuracy'], label='Training accuracy',\n",
    "        c=cm.magma(0.75), lw=5)\n",
    "ax.plot(history_ex1.epoch, history_ex1.history['val_accuracy'], label='Validation accuracy',\n",
    "        c=cm.magma(0.93), lw=5)\n",
    "ax.set_ylabel('Accuracy', fontsize=axislabelsize, fontweight='bold',\n",
    "              color='white')\n",
    "\n",
    "for ax in axes.reshape(-1):\n",
    "    ax.set_xlabel('Epochs', fontsize=axislabelsize, fontweight='bold',\n",
    "                  color='white')\n",
    "    ax.xaxis.set_major_locator(plticker.MultipleLocator(base=1.0))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize,\n",
    "                   colors='white')\n",
    "\n",
    "    ax.legend(fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 2. The loss and accuracy history of the CNN model trained on MNIST dataset. The loss history can be\\n' +\n",
    "             'seen on the left side, while the training accuracy on the right.',\n",
    "             color='white',\n",
    "             fontsize=axistitlesize+5, y=-0.02)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1./e. Make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn_model_ex1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded preds and tests to normal arrays\n",
    "y_test_b = y_test.argmax(axis=-1)\n",
    "y_pred_b = y_pred.argmax(axis=-1)\n",
    "\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test_b, y_pred=y_pred_b)\n",
    "conf_mat = confusion_matrix(y_test_b, y_pred_b, labels=[i for i in range(0,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y_test_b, labels=[i for i in range(0,10)],\n",
    "                      title=('Fig. 3. Confusion matrix of the predictions\\n' +\n",
    "                             'on the test set of my CNN model.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 8\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*2, nrows*2),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "rand_idx = np.random.randint(0, len(X_test), size=nrows*ncols)\n",
    "images = X_test[rand_idx]\n",
    "labels_gr = y_test_b[rand_idx]\n",
    "labels_tr = y_pred_b[rand_idx]\n",
    "\n",
    "for i, ax in enumerate(axes.reshape(-1)):\n",
    "    ax.imshow(images[i].reshape((28,28)), cmap='Greys_r')\n",
    "    ax.set_title('True label : {0}\\nPred label : {1}'.format(labels_gr[i],\n",
    "                                                             labels_tr[i]),\n",
    "                 fontweight='bold', color='white', pad=0)\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.suptitle('Fig. 4. Predictions of the basic CNN model on the test set of the MNIST dataset\\n' +\n",
    "             'by noting groundtruth and predicted labels along with the\\n' +\n",
    "             'corresponding images.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.1)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download the Street View House Numbers (SVHN) Dataset\n",
    "\n",
    "- source: http://ufldl.stanford.edu/housenumbers/\n",
    "- use the cropped dataset!\n",
    "- to get the dataset use eg. wget and keep the original splitting, so download train and test matrix files\n",
    "- preprocess the downloaded data to be able to use it for training and testing, so shapes should be same (except image sizes) as it was in ex 1.\n",
    "- how many classes do we have in the dataset? how many train and test examples do we have?\n",
    "- what is the dimension of the images?\n",
    "- show 5 images from the dataset\n",
    "- make one-hot encoding for the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./a. Load and preprocess the SVHN dataset\n",
    "\n",
    "The dataset at http://ufldl.stanford.edu/housenumbers/ isn't reachable for me at the moment I'm solving this assignment [at 2020-11-22T15:52:43.164052]. So I'm using another method to obtain and load the SVHN dataset.\n",
    "\n",
    "Sources:\n",
    "- Loading data:\n",
    "    - https://www.machinecurve.com/index.php/2020/01/10/making-more-datasets-available-for-keras/\n",
    "    - https://github.com/christianversloot/extra_keras_datasets/blob/master/extra_keras_datasets/svhn.py\n",
    "\n",
    "This can reach the Stanford dataset (somehow...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra_keras_datasets import svhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = svhn.load_data(type='normal')\n",
    "# Define SVHN classes in order\n",
    "svhn_classes = [1,2,3,4,5,6,7,8,9,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape :', X_train.shape)\n",
    "print('y_train shape :', y_train.shape)\n",
    "print('X_test shape :', X_test.shape)\n",
    "print('y_test shape :', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 8\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*2, nrows*2),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "rand_idx = np.random.randint(0, len(X_train), size=nrows*ncols)\n",
    "images = X_train[rand_idx]\n",
    "labels = y_train[rand_idx]\n",
    "\n",
    "for i, ax in enumerate(axes.reshape(-1)):\n",
    "    ax.imshow(images[i], cmap='Greys_r')\n",
    "    ax.set_title('Label : {0}'.format(svhn_classes[labels[i]-1]), fontweight='bold',\n",
    "                 color='white', pad=0)\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.suptitle('Fig. 5. Sample data along with their labels of the SVHN dataset.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.1)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2./c. Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# Reshape data\n",
    "X_train = X_train.reshape((-1, 32,32, 3))\n",
    "X_test = X_test.reshape((-1, 32,32, 3))\n",
    "# Convert labels to one-hot encoded arrays\n",
    "y_train = label_binarize(y_train, classes=np.unique(y_train))\n",
    "y_test = label_binarize(y_test, classes=np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape :', X_train.shape)\n",
    "print('y_train shape :', y_train.shape)\n",
    "print('X_test shape :', X_test.shape)\n",
    "print('y_test shape :', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the CNN model seen in the 1st exercise for this dataset\n",
    "* create a convolutional neural network\n",
    "* the network should have the following layers:\n",
    "        \n",
    "        input (32, 32, 3)\n",
    "        conv2D, 16 kernels, kernel size = 3, valid padding, relu actvation\n",
    "        conv2D, 16 kernels, kernel size = 3, valid padding, relu actvation\n",
    "        maxpooling kernel size = 2*2\n",
    "        conv2D, 32 kernels, kernel size = 3, valid padding, relu actvation\n",
    "        conv2D, 32 kernels, kernel size = 3, valid padding, relu actvation\n",
    "        maxpooling kernel size = 2*2\n",
    "        flatten\n",
    "        dense, 10 neurons, softmax activation\n",
    "        how many parameters do we have for each layer?\n",
    "\n",
    "    * use Adam optimizer with default parameters\n",
    "    * use categorical crossentropy as loss function\n",
    "    * compile the model\n",
    "    * print out a summary of the model\n",
    "    * train the CNN on the training data for 15 epochs with batch size of 32\n",
    "    * use the test data as validation data\n",
    "- calculate the categorical cross-entropy loss and the accuracy! Hint: you should get at least ~80-90% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3./a. Define model for task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    # Create checkpoint file to save best model into\n",
    "    best_model_ex3 = kc.ModelCheckpoint('./models/best_model_ex3.hdf5', save_best_only=True, verbose=1)\n",
    "    \n",
    "    # Define the model\n",
    "    cnn_model_ex3 = cnn_model(imsize=32, stride=1, kernelsize=3,\n",
    "                              n_channels=3, num_of_filters=16, reg=5e-5,\n",
    "                              padding='valid', activation='relu', n_class=10,\n",
    "                              model_name='model_ex3')\n",
    "    \n",
    "    # Configure the model\n",
    "    cnn_model_ex3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "                          loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_ex3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_ex3 = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    history_ex3 = cnn_model_ex3.fit(x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    batch_size=32,\n",
    "                                    epochs=epochs_ex3,\n",
    "                                    verbose=1,\n",
    "                                    validation_split=0.2,\n",
    "                                    shuffle=True,\n",
    "                                    callbacks=[best_model_ex3],\n",
    "                                    initial_epoch=0,\n",
    "                                    steps_per_epoch=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate performance\n",
    "\n",
    "- plot the training and the validation loss on the same plot!\n",
    "- plot the training and the validation accuracy on the same plot!\n",
    "- do we overfit?\n",
    "- show the confusion matrix of the predictions (predicted values vs actual labels)\n",
    "- where does the model make mistakes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./a. Evaluate loss and accuracy history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*12, nrows*8),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "# LOSS GRAPH\n",
    "ax = axes[0]\n",
    "ax.plot(history_ex3.epoch, history_ex3.history['loss'], label='Training loss',\n",
    "        c=cm.magma(0.75), lw=5)\n",
    "ax.plot(history_ex3.epoch, history_ex3.history['val_loss'], label='Validation loss',\n",
    "        c=cm.magma(0.93), lw=5)\n",
    "ax.set_ylabel('Loss', fontsize=axislabelsize, fontweight='bold',\n",
    "              color='white')\n",
    "\n",
    "# ACCURACY GRAPH\n",
    "ax = axes[1]\n",
    "ax.plot(history_ex3.epoch, history_ex3.history['accuracy'], label='Training accuracy',\n",
    "        c=cm.magma(0.75), lw=5)\n",
    "ax.plot(history_ex3.epoch, history_ex3.history['val_accuracy'], label='Validation accuracy',\n",
    "        c=cm.magma(0.93), lw=5)\n",
    "ax.set_ylabel('Accuracy', fontsize=axislabelsize, fontweight='bold',\n",
    "              color='white')\n",
    "\n",
    "for ax in axes.reshape(-1):\n",
    "    ax.set_xlabel('Epochs', fontsize=axislabelsize, fontweight='bold',\n",
    "                  color='white')\n",
    "    ax.xaxis.set_major_locator(plticker.MultipleLocator(base=1.0))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize,\n",
    "                   colors='white')\n",
    "\n",
    "    ax.legend(fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 6. The loss and accuracy history of the CNN model trained on SVHN dataset. The loss history can be\\n' +\n",
    "             'seen on the left side, while the training accuracy on the right.',\n",
    "             color='white',\n",
    "             fontsize=axistitlesize+5, y=-0.02)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit happens, when the validation loss starting to stagnate (or grow), while the training loss further decreases. That's exactly the case here, as seen on the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4./b. Make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn_model_ex3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded preds and tests to normal arrays\n",
    "y_test_b = y_test.argmax(axis=-1)\n",
    "y_pred_b = y_pred.argmax(axis=-1)\n",
    "\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test_b, y_pred=y_pred_b)\n",
    "conf_mat = confusion_matrix(y_test_b, y_pred_b, labels=svhn_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y_test_b, labels=svhn_classes,\n",
    "                      title=('Fig. 7. Confusion matrix of the predictions\\n' +\n",
    "                             'on the SVHN test set of my CNN model.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 8\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*2, nrows*2),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "rand_idx = np.random.randint(0, len(X_test), size=nrows*ncols)\n",
    "images = X_test[rand_idx]\n",
    "labels_gr = y_test_b[rand_idx]\n",
    "labels_tr = y_pred_b[rand_idx]\n",
    "\n",
    "for i, ax in enumerate(axes.reshape(-1)):\n",
    "    ax.imshow(images[i])\n",
    "    ax.set_title('True label : {0}\\nPred label : {1}'.format(svhn_classes[labels_gr[i]],\n",
    "                                                             svhn_classes[labels_tr[i]]),\n",
    "                 fontweight='bold', color='white', pad=0)\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.suptitle('Fig. 8. Predictions of the basic CNN model on the test set of the SVHN dataset\\n' +\n",
    "             'by noting groundtruth and predicted labels along with the corresponding images.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.1)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train an other CNN\n",
    "- as we can see the previous architecture can be further improved\n",
    "- come up with an architecture that can achieve more than 91% accuracy on the test set\n",
    "- print out the summary for this model!\n",
    "- plot the loss and accuracy curves for this model too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./a. Create and fit better model\n",
    "\n",
    "- Added another convolutional block to the network\n",
    "- Changed initial number of filters from `16` to `32`\n",
    "- Changed learning rate from `0.005` to `0.004`\n",
    "- Changed padding from `valid` to `same`\n",
    "- Added early stopping with `patience=10` epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    # Create checkpoint file to save best model into\n",
    "    best_model_ex5 = kc.ModelCheckpoint('./models/best_model_ex5.hdf5', save_best_only=True, verbose=1)\n",
    "    # Configure early stopping with N epochs of patience\n",
    "    es_ex5 = kc.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    \n",
    "    # Define the model\n",
    "    cnn_model_ex5 = better_cnn_model(imsize=32, stride=1, kernelsize=3,\n",
    "                                     n_channels=3, num_of_filters=32,\n",
    "                                     padding='same', activation='relu', n_class=10,\n",
    "                                     model_name='model_ex5')\n",
    "    \n",
    "    # Configure the model\n",
    "    cnn_model_ex5.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.004),\n",
    "                          loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_ex5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_ex5 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    history_ex5 = cnn_model_ex5.fit(x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    batch_size=32,\n",
    "                                    epochs=epochs_ex5,\n",
    "                                    verbose=1,\n",
    "                                    validation_split=0.2,\n",
    "                                    shuffle=True,\n",
    "                                    callbacks=[es_ex5, best_model_ex5],\n",
    "                                    initial_epoch=0,\n",
    "                                    steps_per_epoch=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./b. Evaluate loss and accuracy history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*12, nrows*8),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "\n",
    "# LOSS GRAPH\n",
    "ax = axes[0]\n",
    "ax.plot(history_ex5.epoch, history_ex5.history['loss'], label='Training loss',\n",
    "        c=cm.magma(0.75), lw=5)\n",
    "ax.plot(history_ex5.epoch, history_ex5.history['val_loss'], label='Validation loss',\n",
    "        c=cm.magma(0.93), lw=5)\n",
    "ax.set_ylabel('Loss', fontsize=axislabelsize, fontweight='bold',\n",
    "              color='white')\n",
    "\n",
    "# ACCURACY GRAPH\n",
    "ax = axes[1]\n",
    "ax.plot(history_ex5.epoch, history_ex5.history['accuracy'], label='Training accuracy',\n",
    "        c=cm.magma(0.75), lw=5)\n",
    "ax.plot(history_ex5.epoch, history_ex5.history['val_accuracy'], label='Validation accuracy',\n",
    "        c=cm.magma(0.93), lw=5)\n",
    "ax.set_ylabel('Accuracy', fontsize=axislabelsize, fontweight='bold',\n",
    "              color='white')\n",
    "\n",
    "for ax in axes.reshape(-1):\n",
    "    if epochs_ex5 > history_ex5.epoch[-1] : ax.axvline(x=history_ex5.epoch[-1], label='Early stopping',\n",
    "                                                       color=cm.magma(0.5), ls='--', lw=4)\n",
    "    ax.set_xlabel('Epochs', fontsize=axislabelsize, fontweight='bold',\n",
    "                  color='white')\n",
    "    #ax.xaxis.set_major_locator(plticker.MultipleLocator(base=1.0))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize,\n",
    "                   colors='white')\n",
    "\n",
    "axes[0].legend(loc='upper right', fontsize=axislegendsize)\n",
    "axes[1].legend(loc='lower right', fontsize=axislegendsize)\n",
    "\n",
    "plt.suptitle('Fig. 9. The loss and accuracy history of the BETTER CNN model trained on SVHN dataset. The loss history can be\\n' +\n",
    "             'seen on the left side, while the training accuracy on the right.',\n",
    "             color='white',\n",
    "             fontsize=axistitlesize+5, y=-0.02)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5./c. Make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn_model_ex5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded preds and tests to normal arrays\n",
    "y_test_b = y_test.argmax(axis=-1)\n",
    "y_pred_b = y_pred.argmax(axis=-1)\n",
    "\n",
    "# Calculate and accuracy metric and the confusion matrix\n",
    "accuracy = accuracy_metric(y_test=y_test_b, y_pred=y_pred_b)\n",
    "conf_mat = confusion_matrix(y_test_b, y_pred_b, labels=svhn_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, y_test_b, labels=svhn_classes,\n",
    "                      title=('Fig. 10. Confusion matrix of the predictions\\n' +\n",
    "                             'on the SVHN test set of my BETTER CNN model.\\n' +\n",
    "                             'Accuracy of model is {0:.3f}%'.format(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 8\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*2, nrows*2),\n",
    "                         facecolor='black', subplot_kw={'facecolor' : 'black'})\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "rand_idx = np.random.randint(0, len(X_test), size=nrows*ncols)\n",
    "images = X_test[rand_idx]\n",
    "labels_gr = y_test_b[rand_idx]\n",
    "labels_tr = y_pred_b[rand_idx]\n",
    "\n",
    "for i, ax in enumerate(axes.reshape(-1)):\n",
    "    ax.imshow(images[i])\n",
    "    ax.set_title('True label : {0}\\nPred label : {1}'.format(svhn_classes[labels_gr[i]],\n",
    "                                                             svhn_classes[labels_tr[i]]),\n",
    "                 fontweight='bold', color='white', pad=0)\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.suptitle('Fig. 11. Predictions of the BETTER CNN model on the test set of the SVHN dataset\\n' +\n",
    "             'by noting groundtruth and predicted labels along with the\\n' +\n",
    "             'corresponding images.',\n",
    "             color='white', fontsize=axistitlesize+5, y=0.1)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
